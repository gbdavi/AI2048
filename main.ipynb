{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 00:09:03.732772: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-28 00:09:03.732815: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   406/20000: episode: 1, duration: 0.388s, episode steps: 406, steps per second: 1048, episode reward: -14842.000, mean reward: -36.557 [-50.000, 84.000], mean action: 2.069 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "   915/20000: episode: 2, duration: 0.329s, episode steps: 509, steps per second: 1547, episode reward: -18660.000, mean reward: -36.660 [-50.000, 128.000], mean action: 2.102 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1488/20000: episode: 3, duration: 2.814s, episode steps: 573, steps per second: 204, episode reward: -17998.000, mean reward: -31.410 [-50.000, 260.000], mean action: 0.990 [0.000, 3.000],  loss: 564.431475, mae: 26.227176, mean_q: -20.476298\n",
      "  1679/20000: episode: 4, duration: 0.879s, episode steps: 191, steps per second: 217, episode reward: -5678.000, mean reward: -29.728 [-50.000, 80.000], mean action: 0.649 [0.000, 3.000],  loss: 374.835358, mae: 63.950764, mean_q: -73.039307\n",
      "  2049/20000: episode: 5, duration: 1.609s, episode steps: 370, steps per second: 230, episode reward: -11810.000, mean reward: -31.919 [-50.000, 140.000], mean action: 2.297 [0.000, 3.000],  loss: 408.101562, mae: 99.414070, mean_q: -120.218475\n",
      "  2286/20000: episode: 6, duration: 1.038s, episode steps: 237, steps per second: 228, episode reward: -4356.000, mean reward: -18.380 [-50.000, 136.000], mean action: 1.713 [0.000, 3.000],  loss: 486.465546, mae: 144.610794, mean_q: -179.926620\n",
      "  2387/20000: episode: 7, duration: 0.438s, episode steps: 101, steps per second: 231, episode reward: -1182.000, mean reward: -11.703 [-50.000, 64.000], mean action: 1.772 [0.000, 3.000],  loss: 676.745972, mae: 162.384933, mean_q: -202.436111\n",
      "  2593/20000: episode: 8, duration: 0.865s, episode steps: 206, steps per second: 238, episode reward: -2498.000, mean reward: -12.126 [-50.000, 140.000], mean action: 2.218 [0.000, 3.000],  loss: 574.879333, mae: 181.838898, mean_q: -228.358856\n",
      "  2684/20000: episode: 9, duration: 0.393s, episode steps:  91, steps per second: 231, episode reward: -800.000, mean reward: -8.791 [-50.000, 36.000], mean action: 1.593 [0.000, 3.000],  loss: 732.334534, mae: 199.377579, mean_q: -251.163803\n",
      "  2935/20000: episode: 10, duration: 1.074s, episode steps: 251, steps per second: 234, episode reward: -4376.000, mean reward: -17.434 [-50.000, 128.000], mean action: 1.486 [0.000, 3.000],  loss: 734.527222, mae: 217.494705, mean_q: -274.720062\n",
      "  3066/20000: episode: 11, duration: 0.560s, episode steps: 131, steps per second: 234, episode reward: 438.000, mean reward:  3.344 [-50.000, 152.000], mean action: 1.672 [0.000, 3.000],  loss: 817.544617, mae: 236.391693, mean_q: -299.495117\n",
      "  3128/20000: episode: 12, duration: 0.270s, episode steps:  62, steps per second: 230, episode reward: 224.000, mean reward:  3.613 [-50.000, 48.000], mean action: 1.548 [0.000, 3.000],  loss: 932.282959, mae: 245.245834, mean_q: -310.178284\n",
      "  3279/20000: episode: 13, duration: 0.649s, episode steps: 151, steps per second: 233, episode reward: -988.000, mean reward: -6.543 [-50.000, 64.000], mean action: 1.364 [0.000, 3.000],  loss: 841.189148, mae: 249.651718, mean_q: -316.192841\n",
      "  3451/20000: episode: 14, duration: 0.732s, episode steps: 172, steps per second: 235, episode reward: -2200.000, mean reward: -12.791 [-50.000, 136.000], mean action: 2.064 [0.000, 3.000],  loss: 1168.019287, mae: 255.742737, mean_q: -323.167328\n",
      "  3542/20000: episode: 15, duration: 0.424s, episode steps:  91, steps per second: 215, episode reward: -1674.000, mean reward: -18.396 [-50.000, 40.000], mean action: 0.868 [0.000, 3.000],  loss: 1257.909302, mae: 259.281586, mean_q: -326.658600\n",
      "  3639/20000: episode: 16, duration: 0.420s, episode steps:  97, steps per second: 231, episode reward: 160.000, mean reward:  1.649 [-50.000, 80.000], mean action: 1.485 [0.000, 3.000],  loss: 1048.683472, mae: 261.950684, mean_q: -331.000885\n",
      "  3734/20000: episode: 17, duration: 0.416s, episode steps:  95, steps per second: 228, episode reward: -2040.000, mean reward: -21.474 [-50.000, 52.000], mean action: 1.126 [0.000, 3.000],  loss: 1085.860840, mae: 265.144562, mean_q: -334.041656\n",
      "  3820/20000: episode: 18, duration: 0.392s, episode steps:  86, steps per second: 219, episode reward: 500.000, mean reward:  5.814 [-50.000, 72.000], mean action: 1.291 [0.000, 3.000],  loss: 1183.790039, mae: 268.343567, mean_q: -337.847656\n",
      "  4119/20000: episode: 19, duration: 1.296s, episode steps: 299, steps per second: 231, episode reward: -6432.000, mean reward: -21.512 [-50.000, 148.000], mean action: 1.064 [0.000, 3.000],  loss: 1256.195435, mae: 271.430847, mean_q: -341.919312\n",
      "  4283/20000: episode: 20, duration: 0.713s, episode steps: 164, steps per second: 230, episode reward: -1816.000, mean reward: -11.073 [-50.000, 144.000], mean action: 1.762 [0.000, 3.000],  loss: 1240.844360, mae: 283.552582, mean_q: -357.770294\n",
      "  4389/20000: episode: 21, duration: 0.469s, episode steps: 106, steps per second: 226, episode reward: -2826.000, mean reward: -26.660 [-50.000, 48.000], mean action: 2.519 [0.000, 3.000],  loss: 1714.414307, mae: 291.073395, mean_q: -366.713898\n",
      "  4497/20000: episode: 22, duration: 0.470s, episode steps: 108, steps per second: 230, episode reward: -1278.000, mean reward: -11.833 [-50.000, 40.000], mean action: 1.120 [0.000, 3.000],  loss: 1236.648193, mae: 292.474152, mean_q: -369.459442\n",
      "  4645/20000: episode: 23, duration: 0.645s, episode steps: 148, steps per second: 230, episode reward: -1042.000, mean reward: -7.041 [-50.000, 180.000], mean action: 1.642 [0.000, 3.000],  loss: 1616.258911, mae: 295.908844, mean_q: -373.136993\n",
      "  4761/20000: episode: 24, duration: 0.510s, episode steps: 116, steps per second: 228, episode reward: -72.000, mean reward: -0.621 [-50.000, 140.000], mean action: 1.086 [0.000, 3.000],  loss: 1777.908325, mae: 301.673492, mean_q: -379.819122\n",
      "  4861/20000: episode: 25, duration: 0.433s, episode steps: 100, steps per second: 231, episode reward: -490.000, mean reward: -4.900 [-50.000, 64.000], mean action: 2.140 [0.000, 3.000],  loss: 1373.633301, mae: 301.116425, mean_q: -379.243561\n",
      "  4960/20000: episode: 26, duration: 0.424s, episode steps:  99, steps per second: 233, episode reward: -748.000, mean reward: -7.556 [-50.000, 64.000], mean action: 1.657 [0.000, 3.000],  loss: 1576.184814, mae: 299.430603, mean_q: -376.741425\n",
      "  5073/20000: episode: 27, duration: 0.486s, episode steps: 113, steps per second: 232, episode reward: -946.000, mean reward: -8.372 [-50.000, 64.000], mean action: 1.805 [0.000, 3.000],  loss: 1538.362549, mae: 301.620850, mean_q: -379.650879\n",
      "  5171/20000: episode: 28, duration: 0.434s, episode steps:  98, steps per second: 226, episode reward: 666.000, mean reward:  6.796 [-50.000, 80.000], mean action: 1.449 [0.000, 3.000],  loss: 2037.093140, mae: 301.804291, mean_q: -378.916962\n",
      "  5533/20000: episode: 29, duration: 1.527s, episode steps: 362, steps per second: 237, episode reward: -9744.000, mean reward: -26.917 [-50.000, 132.000], mean action: 1.917 [0.000, 3.000],  loss: 1755.504150, mae: 313.305328, mean_q: -394.042297\n",
      "  5715/20000: episode: 30, duration: 0.789s, episode steps: 182, steps per second: 231, episode reward: -2816.000, mean reward: -15.473 [-50.000, 128.000], mean action: 1.225 [0.000, 3.000],  loss: 1692.017334, mae: 332.263306, mean_q: -418.076416\n",
      "  5844/20000: episode: 31, duration: 0.559s, episode steps: 129, steps per second: 231, episode reward: -1870.000, mean reward: -14.496 [-50.000, 116.000], mean action: 1.845 [0.000, 3.000],  loss: 1570.445068, mae: 343.368561, mean_q: -432.528229\n",
      "  6052/20000: episode: 32, duration: 0.891s, episode steps: 208, steps per second: 233, episode reward: -5260.000, mean reward: -25.288 [-50.000, 64.000], mean action: 1.303 [0.000, 3.000],  loss: 1763.517090, mae: 346.794891, mean_q: -437.217407\n",
      "  6141/20000: episode: 33, duration: 0.394s, episode steps:  89, steps per second: 226, episode reward: 622.000, mean reward:  6.989 [-50.000, 80.000], mean action: 1.562 [0.000, 3.000],  loss: 1997.466309, mae: 342.416901, mean_q: -430.604187\n",
      "  6267/20000: episode: 34, duration: 0.549s, episode steps: 126, steps per second: 230, episode reward: -2174.000, mean reward: -17.254 [-50.000, 40.000], mean action: 1.230 [0.000, 3.000],  loss: 1660.783081, mae: 342.106720, mean_q: -430.129456\n",
      "  6380/20000: episode: 35, duration: 0.502s, episode steps: 113, steps per second: 225, episode reward: 102.000, mean reward:  0.903 [-50.000, 72.000], mean action: 1.301 [0.000, 3.000],  loss: 1954.255981, mae: 344.440308, mean_q: -432.958008\n",
      "  6462/20000: episode: 36, duration: 0.359s, episode steps:  82, steps per second: 229, episode reward: 140.000, mean reward:  1.707 [-50.000, 76.000], mean action: 1.573 [0.000, 3.000],  loss: 2289.969727, mae: 353.824280, mean_q: -444.225555\n",
      "  6603/20000: episode: 37, duration: 0.619s, episode steps: 141, steps per second: 228, episode reward: 958.000, mean reward:  6.794 [-50.000, 144.000], mean action: 1.468 [0.000, 3.000],  loss: 1776.252197, mae: 351.118530, mean_q: -441.104706\n",
      "  6650/20000: episode: 38, duration: 0.206s, episode steps:  47, steps per second: 228, episode reward: 236.000, mean reward:  5.021 [ 0.000, 60.000], mean action: 1.851 [0.000, 3.000],  loss: 1944.007446, mae: 348.455902, mean_q: -436.794220\n",
      "  6731/20000: episode: 39, duration: 0.359s, episode steps:  81, steps per second: 226, episode reward: 600.000, mean reward:  7.407 [ 0.000, 80.000], mean action: 1.247 [0.000, 3.000],  loss: 2180.553223, mae: 347.279541, mean_q: -435.137573\n",
      "  6931/20000: episode: 40, duration: 0.879s, episode steps: 200, steps per second: 227, episode reward: -2144.000, mean reward: -10.720 [-50.000, 132.000], mean action: 1.145 [0.000, 3.000],  loss: 1776.289062, mae: 353.837097, mean_q: -443.092957\n",
      "  7035/20000: episode: 41, duration: 0.459s, episode steps: 104, steps per second: 227, episode reward: -400.000, mean reward: -3.846 [-50.000, 92.000], mean action: 1.365 [0.000, 3.000],  loss: 2055.340088, mae: 359.717834, mean_q: -449.747437\n",
      "  7157/20000: episode: 42, duration: 0.529s, episode steps: 122, steps per second: 231, episode reward: -862.000, mean reward: -7.066 [-50.000, 76.000], mean action: 1.549 [0.000, 3.000],  loss: 1838.805542, mae: 355.427521, mean_q: -443.402710\n",
      "  7322/20000: episode: 43, duration: 0.708s, episode steps: 165, steps per second: 233, episode reward: -616.000, mean reward: -3.733 [-50.000, 144.000], mean action: 2.018 [0.000, 3.000],  loss: 1773.781860, mae: 358.881378, mean_q: -446.997742\n",
      "  7505/20000: episode: 44, duration: 0.786s, episode steps: 183, steps per second: 233, episode reward: -1730.000, mean reward: -9.454 [-50.000, 136.000], mean action: 1.672 [0.000, 3.000],  loss: 2074.323486, mae: 353.548828, mean_q: -438.991028\n",
      "  7661/20000: episode: 45, duration: 0.664s, episode steps: 156, steps per second: 235, episode reward: -4614.000, mean reward: -29.577 [-50.000, 44.000], mean action: 1.006 [0.000, 3.000],  loss: 2144.598633, mae: 349.160309, mean_q: -432.202118\n",
      "  7820/20000: episode: 46, duration: 0.678s, episode steps: 159, steps per second: 234, episode reward: -1028.000, mean reward: -6.465 [-50.000, 136.000], mean action: 2.006 [0.000, 3.000],  loss: 1916.378906, mae: 345.542511, mean_q: -427.064178\n",
      "  7941/20000: episode: 47, duration: 0.517s, episode steps: 121, steps per second: 234, episode reward: -480.000, mean reward: -3.967 [-50.000, 72.000], mean action: 1.843 [0.000, 3.000],  loss: 2384.860352, mae: 346.245178, mean_q: -427.170685\n",
      "  7996/20000: episode: 48, duration: 0.265s, episode steps:  55, steps per second: 208, episode reward: -536.000, mean reward: -9.745 [-50.000, 16.000], mean action: 1.200 [0.000, 3.000],  loss: 2258.236084, mae: 340.530548, mean_q: -419.621124\n",
      "  8089/20000: episode: 49, duration: 0.402s, episode steps:  93, steps per second: 231, episode reward: -830.000, mean reward: -8.925 [-50.000, 64.000], mean action: 1.108 [0.000, 3.000],  loss: 2204.170654, mae: 339.792999, mean_q: -417.851654\n",
      "  8140/20000: episode: 50, duration: 0.223s, episode steps:  51, steps per second: 229, episode reward: 256.000, mean reward:  5.020 [ 0.000, 40.000], mean action: 1.314 [0.000, 3.000],  loss: 2294.584229, mae: 336.682861, mean_q: -413.301025\n",
      "  8365/20000: episode: 51, duration: 0.961s, episode steps: 225, steps per second: 234, episode reward: -2998.000, mean reward: -13.324 [-50.000, 132.000], mean action: 1.693 [0.000, 3.000],  loss: 2162.068848, mae: 336.510132, mean_q: -413.007996\n",
      "  8518/20000: episode: 52, duration: 0.656s, episode steps: 153, steps per second: 233, episode reward: -2606.000, mean reward: -17.033 [-50.000, 72.000], mean action: 1.346 [0.000, 3.000],  loss: 2285.025391, mae: 339.992340, mean_q: -417.289764\n",
      "  8610/20000: episode: 53, duration: 0.407s, episode steps:  92, steps per second: 226, episode reward: 24.000, mean reward:  0.261 [-50.000, 64.000], mean action: 1.098 [0.000, 3.000],  loss: 2468.636475, mae: 341.576965, mean_q: -419.375580\n",
      "  8700/20000: episode: 54, duration: 0.396s, episode steps:  90, steps per second: 227, episode reward: 618.000, mean reward:  6.867 [-50.000, 76.000], mean action: 1.300 [0.000, 3.000],  loss: 1852.046997, mae: 343.494263, mean_q: -423.183685\n",
      "  8953/20000: episode: 55, duration: 1.082s, episode steps: 253, steps per second: 234, episode reward: -4708.000, mean reward: -18.609 [-50.000, 132.000], mean action: 1.308 [0.000, 3.000],  loss: 1877.106201, mae: 344.949768, mean_q: -424.744202\n",
      "  9087/20000: episode: 56, duration: 0.574s, episode steps: 134, steps per second: 233, episode reward: -2740.000, mean reward: -20.448 [-50.000, 40.000], mean action: 0.575 [0.000, 3.000],  loss: 1766.088379, mae: 350.696106, mean_q: -433.272156\n",
      "  9223/20000: episode: 57, duration: 0.593s, episode steps: 136, steps per second: 229, episode reward: -1462.000, mean reward: -10.750 [-50.000, 80.000], mean action: 1.779 [0.000, 3.000],  loss: 1923.854370, mae: 357.142212, mean_q: -441.744385\n",
      "  9351/20000: episode: 58, duration: 0.554s, episode steps: 128, steps per second: 231, episode reward: 880.000, mean reward:  6.875 [-50.000, 140.000], mean action: 1.383 [0.000, 3.000],  loss: 1886.391357, mae: 356.874115, mean_q: -441.493500\n",
      "  9581/20000: episode: 59, duration: 0.982s, episode steps: 230, steps per second: 234, episode reward: -4430.000, mean reward: -19.261 [-50.000, 148.000], mean action: 1.552 [0.000, 3.000],  loss: 2177.469727, mae: 355.848175, mean_q: -438.715088\n",
      "  9802/20000: episode: 60, duration: 0.924s, episode steps: 221, steps per second: 239, episode reward: -4186.000, mean reward: -18.941 [-50.000, 136.000], mean action: 1.606 [0.000, 3.000],  loss: 2235.513184, mae: 364.650940, mean_q: -450.561920\n",
      "  9879/20000: episode: 61, duration: 0.332s, episode steps:  77, steps per second: 232, episode reward: 540.000, mean reward:  7.013 [ 0.000, 64.000], mean action: 1.481 [0.000, 3.000],  loss: 2681.708984, mae: 368.823212, mean_q: -456.256287\n",
      " 10020/20000: episode: 62, duration: 0.610s, episode steps: 141, steps per second: 231, episode reward: -352.000, mean reward: -2.496 [-50.000, 72.000], mean action: 1.135 [0.000, 3.000],  loss: 2704.334229, mae: 373.622803, mean_q: -462.017883\n",
      " 10119/20000: episode: 63, duration: 0.425s, episode steps:  99, steps per second: 233, episode reward: -602.000, mean reward: -6.081 [-50.000, 68.000], mean action: 0.909 [0.000, 3.000],  loss: 2149.716553, mae: 368.196411, mean_q: -455.589966\n",
      " 10184/20000: episode: 64, duration: 0.282s, episode steps:  65, steps per second: 231, episode reward: -992.000, mean reward: -15.262 [-50.000, 32.000], mean action: 0.708 [0.000, 3.000],  loss: 1748.970459, mae: 380.335052, mean_q: -472.535492\n",
      " 10288/20000: episode: 65, duration: 0.446s, episode steps: 104, steps per second: 233, episode reward: -1510.000, mean reward: -14.519 [-50.000, 68.000], mean action: 2.144 [0.000, 3.000],  loss: 3055.222656, mae: 373.936676, mean_q: -462.465668\n",
      " 10370/20000: episode: 66, duration: 0.357s, episode steps:  82, steps per second: 229, episode reward: 604.000, mean reward:  7.366 [ 0.000, 64.000], mean action: 1.537 [0.000, 3.000],  loss: 2765.243408, mae: 372.112091, mean_q: -460.883728\n",
      " 10513/20000: episode: 67, duration: 0.622s, episode steps: 143, steps per second: 230, episode reward: -3728.000, mean reward: -26.070 [-50.000, 52.000], mean action: 0.881 [0.000, 3.000],  loss: 1671.942749, mae: 370.670227, mean_q: -460.606812\n",
      " 10716/20000: episode: 68, duration: 0.867s, episode steps: 203, steps per second: 234, episode reward: -3120.000, mean reward: -15.369 [-50.000, 128.000], mean action: 1.079 [0.000, 3.000],  loss: 2692.828613, mae: 373.418427, mean_q: -462.966064\n",
      " 11115/20000: episode: 69, duration: 1.702s, episode steps: 399, steps per second: 234, episode reward: -10562.000, mean reward: -26.471 [-50.000, 132.000], mean action: 1.669 [0.000, 3.000],  loss: 2239.311768, mae: 385.016174, mean_q: -480.007355\n",
      " 11187/20000: episode: 70, duration: 0.313s, episode steps:  72, steps per second: 230, episode reward: -148.000, mean reward: -2.056 [-50.000, 40.000], mean action: 1.833 [0.000, 3.000],  loss: 2019.417358, mae: 405.202393, mean_q: -507.241577\n",
      " 11328/20000: episode: 71, duration: 0.600s, episode steps: 141, steps per second: 235, episode reward: -1310.000, mean reward: -9.291 [-50.000, 64.000], mean action: 1.638 [0.000, 3.000],  loss: 1949.169800, mae: 403.762238, mean_q: -504.970642\n",
      " 11499/20000: episode: 72, duration: 0.754s, episode steps: 171, steps per second: 227, episode reward: -2498.000, mean reward: -14.608 [-50.000, 64.000], mean action: 1.626 [0.000, 3.000],  loss: 2029.351196, mae: 412.380585, mean_q: -516.797607\n",
      " 11568/20000: episode: 73, duration: 0.301s, episode steps:  69, steps per second: 229, episode reward: 412.000, mean reward:  5.971 [ 0.000, 36.000], mean action: 1.348 [0.000, 3.000],  loss: 2340.548340, mae: 405.274231, mean_q: -506.266418\n",
      " 11645/20000: episode: 74, duration: 0.336s, episode steps:  77, steps per second: 229, episode reward: 390.000, mean reward:  5.065 [-50.000, 72.000], mean action: 1.688 [0.000, 3.000],  loss: 2240.518555, mae: 410.382751, mean_q: -513.652588\n",
      " 11821/20000: episode: 75, duration: 0.772s, episode steps: 176, steps per second: 228, episode reward: -1668.000, mean reward: -9.477 [-50.000, 136.000], mean action: 1.301 [0.000, 3.000],  loss: 2110.724609, mae: 403.879089, mean_q: -502.998749\n",
      " 11932/20000: episode: 76, duration: 0.484s, episode steps: 111, steps per second: 229, episode reward: 114.000, mean reward:  1.027 [-50.000, 64.000], mean action: 1.622 [0.000, 3.000],  loss: 3514.107666, mae: 410.680328, mean_q: -509.771820\n",
      " 12171/20000: episode: 77, duration: 1.017s, episode steps: 239, steps per second: 235, episode reward: -3486.000, mean reward: -14.586 [-50.000, 128.000], mean action: 1.632 [0.000, 3.000],  loss: 2746.950928, mae: 411.192535, mean_q: -511.942169\n",
      " 12312/20000: episode: 78, duration: 0.614s, episode steps: 141, steps per second: 229, episode reward: -2126.000, mean reward: -15.078 [-50.000, 64.000], mean action: 1.397 [0.000, 3.000],  loss: 2113.521240, mae: 424.114960, mean_q: -529.397522\n",
      " 12713/20000: episode: 79, duration: 1.663s, episode steps: 401, steps per second: 241, episode reward: -7524.000, mean reward: -18.763 [-50.000, 264.000], mean action: 1.935 [0.000, 3.000],  loss: 2553.610596, mae: 438.849640, mean_q: -547.395325\n",
      " 12823/20000: episode: 80, duration: 0.483s, episode steps: 110, steps per second: 228, episode reward: -204.000, mean reward: -1.855 [-50.000, 72.000], mean action: 1.300 [0.000, 3.000],  loss: 2530.563721, mae: 455.111328, mean_q: -568.789307\n",
      " 13145/20000: episode: 81, duration: 1.359s, episode steps: 322, steps per second: 237, episode reward: -7984.000, mean reward: -24.795 [-50.000, 128.000], mean action: 1.401 [0.000, 3.000],  loss: 2362.514648, mae: 442.284607, mean_q: -551.068787\n",
      " 13292/20000: episode: 82, duration: 0.647s, episode steps: 147, steps per second: 227, episode reward: -2388.000, mean reward: -16.245 [-50.000, 72.000], mean action: 0.864 [0.000, 3.000],  loss: 2528.589111, mae: 439.216919, mean_q: -546.774170\n",
      " 13366/20000: episode: 83, duration: 0.320s, episode steps:  74, steps per second: 232, episode reward: 540.000, mean reward:  7.297 [ 0.000, 68.000], mean action: 1.378 [0.000, 3.000],  loss: 3272.910400, mae: 447.152863, mean_q: -557.199951\n",
      " 13461/20000: episode: 84, duration: 0.407s, episode steps:  95, steps per second: 233, episode reward: 492.000, mean reward:  5.179 [-50.000, 68.000], mean action: 1.642 [0.000, 3.000],  loss: 2764.706543, mae: 446.861877, mean_q: -557.754456\n",
      " 13536/20000: episode: 85, duration: 0.325s, episode steps:  75, steps per second: 231, episode reward: 516.000, mean reward:  6.880 [ 0.000, 88.000], mean action: 1.587 [0.000, 3.000],  loss: 2026.473511, mae: 438.433228, mean_q: -546.462585\n",
      " 13694/20000: episode: 86, duration: 0.677s, episode steps: 158, steps per second: 233, episode reward: -2176.000, mean reward: -13.772 [-50.000, 72.000], mean action: 1.108 [0.000, 3.000],  loss: 2482.000000, mae: 440.759430, mean_q: -548.448792\n",
      " 13941/20000: episode: 87, duration: 1.075s, episode steps: 247, steps per second: 230, episode reward: -1810.000, mean reward: -7.328 [-50.000, 264.000], mean action: 1.972 [0.000, 3.000],  loss: 2427.887695, mae: 450.353485, mean_q: -561.599548\n",
      " 14357/20000: episode: 88, duration: 1.772s, episode steps: 416, steps per second: 235, episode reward: -10360.000, mean reward: -24.904 [-50.000, 132.000], mean action: 1.529 [0.000, 3.000],  loss: 2451.913086, mae: 464.266663, mean_q: -577.881714\n",
      " 14511/20000: episode: 89, duration: 0.658s, episode steps: 154, steps per second: 234, episode reward: -3018.000, mean reward: -19.597 [-50.000, 64.000], mean action: 1.078 [0.000, 3.000],  loss: 2861.788330, mae: 478.941467, mean_q: -596.870544\n",
      " 14726/20000: episode: 90, duration: 0.919s, episode steps: 215, steps per second: 234, episode reward: -2522.000, mean reward: -11.730 [-50.000, 144.000], mean action: 1.028 [0.000, 3.000],  loss: 2851.288086, mae: 469.442017, mean_q: -583.538574\n",
      " 15013/20000: episode: 91, duration: 1.206s, episode steps: 287, steps per second: 238, episode reward: -3520.000, mean reward: -12.265 [-50.000, 136.000], mean action: 1.753 [0.000, 3.000],  loss: 2391.837158, mae: 468.443176, mean_q: -579.845642\n",
      " 15249/20000: episode: 92, duration: 0.995s, episode steps: 236, steps per second: 237, episode reward: -5346.000, mean reward: -22.653 [-50.000, 76.000], mean action: 1.665 [0.000, 3.000],  loss: 3008.127930, mae: 479.816376, mean_q: -593.175842\n",
      " 15437/20000: episode: 93, duration: 0.796s, episode steps: 188, steps per second: 236, episode reward: -4762.000, mean reward: -25.330 [-50.000, 76.000], mean action: 1.085 [0.000, 3.000],  loss: 2309.355713, mae: 494.574921, mean_q: -614.393799\n",
      " 15548/20000: episode: 94, duration: 0.476s, episode steps: 111, steps per second: 233, episode reward: -300.000, mean reward: -2.703 [-50.000, 72.000], mean action: 1.360 [0.000, 3.000],  loss: 2344.948975, mae: 508.772522, mean_q: -633.538757\n",
      " 15640/20000: episode: 95, duration: 0.407s, episode steps:  92, steps per second: 226, episode reward: 626.000, mean reward:  6.804 [-50.000, 72.000], mean action: 1.457 [0.000, 3.000],  loss: 2251.507568, mae: 513.354431, mean_q: -639.282104\n",
      " 15810/20000: episode: 96, duration: 0.746s, episode steps: 170, steps per second: 228, episode reward: -1768.000, mean reward: -10.400 [-50.000, 132.000], mean action: 1.100 [0.000, 3.000],  loss: 2868.088867, mae: 509.251190, mean_q: -631.925903\n",
      " 15924/20000: episode: 97, duration: 0.497s, episode steps: 114, steps per second: 229, episode reward: 356.000, mean reward:  3.123 [-50.000, 84.000], mean action: 1.658 [0.000, 3.000],  loss: 2940.680176, mae: 515.626770, mean_q: -640.468689\n",
      " 16066/20000: episode: 98, duration: 0.608s, episode steps: 142, steps per second: 234, episode reward: -1338.000, mean reward: -9.423 [-50.000, 84.000], mean action: 2.275 [0.000, 3.000],  loss: 2836.769775, mae: 509.871033, mean_q: -630.748474\n",
      " 16331/20000: episode: 99, duration: 1.131s, episode steps: 265, steps per second: 234, episode reward: -4122.000, mean reward: -15.555 [-50.000, 132.000], mean action: 1.928 [0.000, 3.000],  loss: 2702.523682, mae: 504.459381, mean_q: -624.295349\n",
      " 16415/20000: episode: 100, duration: 0.364s, episode steps:  84, steps per second: 230, episode reward: 688.000, mean reward:  8.190 [ 0.000, 76.000], mean action: 1.655 [0.000, 3.000],  loss: 2320.203613, mae: 507.984985, mean_q: -631.005737\n",
      " 16568/20000: episode: 101, duration: 0.656s, episode steps: 153, steps per second: 233, episode reward: -1308.000, mean reward: -8.549 [-50.000, 64.000], mean action: 1.824 [0.000, 3.000],  loss: 2981.207520, mae: 502.805450, mean_q: -621.947144\n",
      " 16902/20000: episode: 102, duration: 1.421s, episode steps: 334, steps per second: 235, episode reward: -4506.000, mean reward: -13.491 [-50.000, 276.000], mean action: 1.581 [0.000, 3.000],  loss: 3458.960693, mae: 510.261139, mean_q: -632.182800\n",
      " 17067/20000: episode: 103, duration: 0.703s, episode steps: 165, steps per second: 235, episode reward: -1328.000, mean reward: -8.048 [-50.000, 148.000], mean action: 1.915 [0.000, 3.000],  loss: 3878.620361, mae: 514.639465, mean_q: -638.323975\n",
      " 17141/20000: episode: 104, duration: 0.333s, episode steps:  74, steps per second: 222, episode reward:  0.000, mean reward:  0.000 [-50.000, 40.000], mean action: 1.365 [0.000, 3.000],  loss: 3330.406982, mae: 513.816162, mean_q: -637.204590\n",
      " 17463/20000: episode: 105, duration: 1.355s, episode steps: 322, steps per second: 238, episode reward: -8298.000, mean reward: -25.770 [-50.000, 136.000], mean action: 0.994 [0.000, 3.000],  loss: 3645.615723, mae: 497.209045, mean_q: -614.298157\n",
      " 17717/20000: episode: 106, duration: 1.119s, episode steps: 254, steps per second: 227, episode reward: -2156.000, mean reward: -8.488 [-50.000, 256.000], mean action: 1.449 [0.000, 3.000],  loss: 3478.913330, mae: 517.430664, mean_q: -642.474182\n",
      " 17823/20000: episode: 107, duration: 0.452s, episode steps: 106, steps per second: 235, episode reward: -774.000, mean reward: -7.302 [-50.000, 68.000], mean action: 1.094 [0.000, 3.000],  loss: 3511.499512, mae: 522.257690, mean_q: -649.269836\n",
      " 17996/20000: episode: 108, duration: 0.753s, episode steps: 173, steps per second: 230, episode reward: -1086.000, mean reward: -6.277 [-50.000, 128.000], mean action: 1.711 [0.000, 3.000],  loss: 3552.223877, mae: 516.186951, mean_q: -640.065247\n",
      " 18230/20000: episode: 109, duration: 1.009s, episode steps: 234, steps per second: 232, episode reward: -1288.000, mean reward: -5.504 [-50.000, 128.000], mean action: 1.372 [0.000, 3.000],  loss: 3050.147461, mae: 524.364868, mean_q: -650.489136\n",
      " 18315/20000: episode: 110, duration: 0.373s, episode steps:  85, steps per second: 228, episode reward: 504.000, mean reward:  5.929 [-50.000, 72.000], mean action: 1.647 [0.000, 3.000],  loss: 3267.928711, mae: 516.062561, mean_q: -636.543884\n",
      " 18563/20000: episode: 111, duration: 1.063s, episode steps: 248, steps per second: 233, episode reward: -5424.000, mean reward: -21.871 [-50.000, 136.000], mean action: 2.286 [0.000, 3.000],  loss: 3329.119629, mae: 513.710327, mean_q: -634.390259\n",
      " 18629/20000: episode: 112, duration: 0.287s, episode steps:  66, steps per second: 230, episode reward: 138.000, mean reward:  2.091 [-50.000, 36.000], mean action: 1.455 [0.000, 3.000],  loss: 2190.990967, mae: 514.486694, mean_q: -638.781128\n",
      " 18955/20000: episode: 113, duration: 1.385s, episode steps: 326, steps per second: 235, episode reward: -8242.000, mean reward: -25.282 [-50.000, 128.000], mean action: 1.227 [0.000, 3.000],  loss: 3252.731934, mae: 505.097198, mean_q: -624.727844\n",
      " 19219/20000: episode: 114, duration: 1.127s, episode steps: 264, steps per second: 234, episode reward: -3224.000, mean reward: -12.212 [-50.000, 132.000], mean action: 1.803 [0.000, 3.000],  loss: 2982.260010, mae: 521.934448, mean_q: -648.104980\n",
      " 19380/20000: episode: 115, duration: 0.688s, episode steps: 161, steps per second: 234, episode reward: 1488.000, mean reward:  9.242 [-50.000, 156.000], mean action: 1.503 [0.000, 3.000],  loss: 3602.866455, mae: 529.996216, mean_q: -656.379639\n",
      " 19552/20000: episode: 116, duration: 0.738s, episode steps: 172, steps per second: 233, episode reward: -140.000, mean reward: -0.814 [-50.000, 148.000], mean action: 1.395 [0.000, 3.000],  loss: 3427.935303, mae: 513.132812, mean_q: -630.813110\n",
      " 19660/20000: episode: 117, duration: 0.463s, episode steps: 108, steps per second: 233, episode reward: -1110.000, mean reward: -10.278 [-50.000, 64.000], mean action: 1.056 [0.000, 3.000],  loss: 3833.843750, mae: 509.024048, mean_q: -624.209106\n",
      " 19895/20000: episode: 118, duration: 0.995s, episode steps: 235, steps per second: 236, episode reward: -4794.000, mean reward: -20.400 [-50.000, 128.000], mean action: 1.494 [0.000, 3.000],  loss: 4202.178223, mae: 504.099792, mean_q: -617.722351\n",
      "done, took 83.371 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e43b4670>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "# hiddenFeatures = 16\n",
    "hiddenFeatures = 32\n",
    "\n",
    "activation=\"relu\"\n",
    "# activation=\"leaky_relu\"\n",
    "\n",
    "# Higher temporal window confuses the model\n",
    "# temporalWindow = 5\n",
    "temporalWindow = 1\n",
    "\n",
    "\n",
    "\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "\n",
    "# Result look alike\n",
    "model.add(Activation(activation=activation))\n",
    "# model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "\n",
    "# Worst result\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "\n",
    "# Additional layers\n",
    "# model.add(Dense(hiddenFeatures))\n",
    "# model.add(Activation(activation=activation))\n",
    "\n",
    "# model.add(Dense(hiddenFeatures))\n",
    "# model.add(Activation(activation=activation))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Less invalid moves with more limit\n",
    "replayMemory = SequentialMemory(limit=2000, window_length=temporalWindow)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=1000,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy,\n",
    ")\n",
    "# dqn.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "# dqn.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mse\"])\n",
    "dqn.compile(optimizer=RMSprop(learning_rate=0.001, epsilon=0.0000001), metrics=[\"mae\"])\n",
    "# dqn.compile(optimizer=RMSprop(learning_rate=0.001, epsilon=0.0000001), metrics=[\"mse\"])\n",
    "dqn.fit(env=env, nb_steps=20000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10 is over.\n",
      "Game 20 is over.\n",
      "Game 30 is over.\n",
      "Game 40 is over.\n",
      "Game 50 is over.\n",
      "Game 60 is over.\n",
      "Game 70 is over.\n",
      "Game 80 is over.\n",
      "Game 90 is over.\n",
      "Game 100 is over.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1130.440000</td>\n",
       "      <td>117.120000</td>\n",
       "      <td>0.262594</td>\n",
       "      <td>176.280000</td>\n",
       "      <td>41.710000</td>\n",
       "      <td>39.330000</td>\n",
       "      <td>54.070000</td>\n",
       "      <td>41.170000</td>\n",
       "      <td>1.508777</td>\n",
       "      <td>1.339330</td>\n",
       "      <td>1.660872</td>\n",
       "      <td>1.329625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>579.360817</td>\n",
       "      <td>64.678424</td>\n",
       "      <td>0.185439</td>\n",
       "      <td>85.838359</td>\n",
       "      <td>35.783946</td>\n",
       "      <td>28.395851</td>\n",
       "      <td>44.392022</td>\n",
       "      <td>28.464422</td>\n",
       "      <td>1.100372</td>\n",
       "      <td>0.620959</td>\n",
       "      <td>0.965732</td>\n",
       "      <td>0.738339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>268.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>635.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>1.035406</td>\n",
       "      <td>1.029848</td>\n",
       "      <td>1.027590</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1086.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.101959</td>\n",
       "      <td>1.074176</td>\n",
       "      <td>1.178409</td>\n",
       "      <td>1.061862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1393.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.418411</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.285256</td>\n",
       "      <td>1.317726</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.211806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2484.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.683721</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>6.789474</td>\n",
       "      <td>4.378378</td>\n",
       "      <td>5.150000</td>\n",
       "      <td>5.740741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score  higherTile  invalidMove (%)       steps     upMoves  \\\n",
       "count   100.000000  100.000000       100.000000  100.000000  100.000000   \n",
       "mean   1130.440000  117.120000         0.262594  176.280000   41.710000   \n",
       "std     579.360817   64.678424         0.185439   85.838359   35.783946   \n",
       "min     268.000000   32.000000         0.000000   53.000000   12.000000   \n",
       "25%     635.000000   64.000000         0.094694  113.000000   22.000000   \n",
       "50%    1086.000000  128.000000         0.241128  156.000000   29.500000   \n",
       "75%    1393.000000  128.000000         0.418411  212.000000   43.000000   \n",
       "max    2484.000000  256.000000         0.683721  445.000000  224.000000   \n",
       "\n",
       "        downMoves   leftMoves  rightMoves    upSeqAvg  downSeqAvg  leftSeqAvg  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean    39.330000   54.070000   41.170000    1.508777    1.339330    1.660872   \n",
       "std     28.395851   44.392022   28.464422    1.100372    0.620959    0.965732   \n",
       "min      9.000000   13.000000   12.000000    1.000000    1.000000    1.000000   \n",
       "25%     22.750000   28.000000   23.750000    1.035406    1.029848    1.027590   \n",
       "50%     30.000000   40.000000   34.000000    1.101959    1.074176    1.178409   \n",
       "75%     43.000000   70.500000   50.000000    1.285256    1.317726    2.000000   \n",
       "max    162.000000  309.000000  177.000000    6.789474    4.378378    5.150000   \n",
       "\n",
       "       rightSeqAvg  \n",
       "count   100.000000  \n",
       "mean      1.329625  \n",
       "std       0.738339  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.061862  \n",
       "75%       1.211806  \n",
       "max       5.740741  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "def log2_transform(grid):\n",
    "    transformed = np.log2(np.where(grid > 0, grid, 1))\n",
    "    return transformed\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(100)]\n",
    "metrics = {\n",
    "    \"gameId\": [],\n",
    "    \"score\": [],\n",
    "    \"higherTile\": [],\n",
    "    \"invalidMove (%)\": [],\n",
    "    \"steps\": [],\n",
    "    \"upMoves\": [],\n",
    "    \"downMoves\": [],\n",
    "    \"leftMoves\": [],\n",
    "    \"rightMoves\": [],\n",
    "    \"upSeqAvg\": [],\n",
    "    \"downSeqAvg\": [],\n",
    "    \"leftSeqAvg\": [],\n",
    "    \"rightSeqAvg\": [],\n",
    "}\n",
    "for i, game in enumerate(games):\n",
    "    # Metrics\n",
    "    steps = 0\n",
    "    invalidMove = 0\n",
    "    upMoves = 0\n",
    "    downMoves = 0\n",
    "    leftMoves = 0\n",
    "    rightMoves = 0\n",
    "    upSeqAvg = 0\n",
    "    downSeqAvg = 0\n",
    "    leftSeqAvg = 0\n",
    "    rightSeqAvg = 0\n",
    "\n",
    "    sequences = {\n",
    "        \"up\": [],\n",
    "        \"down\": [],\n",
    "        \"left\": [],\n",
    "        \"right\": []\n",
    "    }\n",
    "    lastMove = None\n",
    "\n",
    "    # print(f\"Game {i + 1} initial state:\")\n",
    "    # game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        observation = log2_transform(observation)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        # print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        if not game.move(actionMapping[predictedAction]):\n",
    "            invalidMove += 1\n",
    "        \n",
    "        # print(f\"Game {i + 1} currrent state:\")\n",
    "        # game.render()\n",
    "\n",
    "        # input(\"\\nPress any key to continue...\")\n",
    "\n",
    "        steps += 1\n",
    "        if actionMapping[predictedAction] == \"up\":\n",
    "            upMoves += 1\n",
    "            if lastMove == \"up\":\n",
    "                sequences[\"up\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"up\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"down\":\n",
    "            downMoves += 1\n",
    "            if lastMove == \"down\":\n",
    "                sequences[\"down\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"down\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"left\":\n",
    "            leftMoves += 1\n",
    "            if lastMove == \"left\":\n",
    "                sequences[\"left\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"left\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"right\":\n",
    "            rightMoves += 1\n",
    "            if lastMove == \"right\":\n",
    "                sequences[\"right\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"right\"].append(1)\n",
    "        \n",
    "        lastMove = actionMapping[predictedAction]\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "    metrics[\"gameId\"].append(i)\n",
    "    metrics[\"score\"].append(game.score)\n",
    "    metrics[\"higherTile\"].append(np.max(game.grid))\n",
    "    metrics[\"invalidMove (%)\"].append(invalidMove/steps)\n",
    "    metrics[\"steps\"].append(steps)\n",
    "    metrics[\"upMoves\"].append(upMoves)\n",
    "    metrics[\"downMoves\"].append(downMoves)\n",
    "    metrics[\"leftMoves\"].append(leftMoves)\n",
    "    metrics[\"rightMoves\"].append(rightMoves)\n",
    "    metrics[\"upSeqAvg\"].append(np.mean(np.array(sequences[\"up\"])))\n",
    "    metrics[\"downSeqAvg\"].append(np.mean(np.array(sequences[\"down\"])))\n",
    "    metrics[\"leftSeqAvg\"].append(np.mean(np.array(sequences[\"left\"])))\n",
    "    metrics[\"rightSeqAvg\"].append(np.mean(np.array(sequences[\"right\"])))\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics).set_index(\"gameId\")\n",
    "# display(metrics)\n",
    "display(metrics.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [\"up\", \"left\", \"right\", \"down\"]\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
