{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500/20000: episode: 1, duration: 0.798s, episode steps: 500, steps per second: 627, episode reward: -19310.000, mean reward: -38.620 [-50.000, 88.000], mean action: 1.860 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1089/20000: episode: 2, duration: 2.343s, episode steps: 589, steps per second: 251, episode reward: -22420.000, mean reward: -38.065 [-50.000, 128.000], mean action: 1.438 [0.000, 3.000],  loss: 684.166446, mae: 19.674812, mean_q: 7.476380\n",
      "  1533/20000: episode: 3, duration: 2.261s, episode steps: 444, steps per second: 196, episode reward: -13736.000, mean reward: -30.937 [-50.000, 128.000], mean action: 1.628 [0.000, 3.000],  loss: 418.702454, mae: 34.614090, mean_q: -26.742210\n",
      "  1868/20000: episode: 4, duration: 1.710s, episode steps: 335, steps per second: 196, episode reward: -8270.000, mean reward: -24.687 [-50.000, 128.000], mean action: 1.072 [0.000, 3.000],  loss: 445.915771, mae: 57.482182, mean_q: -56.674091\n",
      "  1965/20000: episode: 5, duration: 0.486s, episode steps:  97, steps per second: 200, episode reward: -1310.000, mean reward: -13.505 [-50.000, 76.000], mean action: 2.361 [0.000, 3.000],  loss: 362.823425, mae: 58.967587, mean_q: -58.463760\n",
      "  2382/20000: episode: 6, duration: 2.092s, episode steps: 417, steps per second: 199, episode reward: -12548.000, mean reward: -30.091 [-50.000, 132.000], mean action: 1.400 [0.000, 3.000],  loss: 439.080048, mae: 103.453644, mean_q: -116.856194\n",
      "  2601/20000: episode: 7, duration: 1.103s, episode steps: 219, steps per second: 199, episode reward: -4236.000, mean reward: -19.342 [-50.000, 96.000], mean action: 1.215 [0.000, 3.000],  loss: 474.705688, mae: 127.784172, mean_q: -150.953949\n",
      "  2864/20000: episode: 8, duration: 1.328s, episode steps: 263, steps per second: 198, episode reward: -7844.000, mean reward: -29.825 [-50.000, 72.000], mean action: 1.840 [0.000, 3.000],  loss: 413.885529, mae: 108.692093, mean_q: -125.264450\n",
      "  2950/20000: episode: 9, duration: 0.433s, episode steps:  86, steps per second: 199, episode reward: -1320.000, mean reward: -15.349 [-50.000, 52.000], mean action: 0.640 [0.000, 3.000],  loss: 430.684814, mae: 121.010551, mean_q: -140.981003\n",
      "  3128/20000: episode: 10, duration: 0.888s, episode steps: 178, steps per second: 200, episode reward: -3712.000, mean reward: -20.854 [-50.000, 92.000], mean action: 1.545 [0.000, 3.000],  loss: 385.276764, mae: 114.371552, mean_q: -132.245728\n",
      "  3453/20000: episode: 11, duration: 1.647s, episode steps: 325, steps per second: 197, episode reward: -7770.000, mean reward: -23.908 [-50.000, 144.000], mean action: 1.126 [0.000, 3.000],  loss: 486.307129, mae: 186.011871, mean_q: -223.118729\n",
      "  3662/20000: episode: 12, duration: 1.036s, episode steps: 209, steps per second: 202, episode reward: -4698.000, mean reward: -22.478 [-50.000, 68.000], mean action: 1.794 [0.000, 3.000],  loss: 655.026306, mae: 169.060440, mean_q: -204.337708\n",
      "  4000/20000: episode: 13, duration: 1.745s, episode steps: 338, steps per second: 194, episode reward: -7520.000, mean reward: -22.249 [-50.000, 136.000], mean action: 1.790 [0.000, 3.000],  loss: 525.385010, mae: 215.354614, mean_q: -259.881836\n",
      "  4268/20000: episode: 14, duration: 1.358s, episode steps: 268, steps per second: 197, episode reward: -4332.000, mean reward: -16.164 [-50.000, 132.000], mean action: 1.257 [0.000, 3.000],  loss: 923.319519, mae: 235.882751, mean_q: -279.753662\n",
      "  4482/20000: episode: 15, duration: 1.105s, episode steps: 214, steps per second: 194, episode reward: -3820.000, mean reward: -17.850 [-50.000, 128.000], mean action: 1.678 [0.000, 3.000],  loss: 1114.009766, mae: 197.848770, mean_q: -225.849960\n",
      "  4599/20000: episode: 16, duration: 0.614s, episode steps: 117, steps per second: 191, episode reward: -914.000, mean reward: -7.812 [-50.000, 72.000], mean action: 1.291 [0.000, 3.000],  loss: 785.751282, mae: 163.640518, mean_q: -193.765610\n",
      "  4771/20000: episode: 17, duration: 0.860s, episode steps: 172, steps per second: 200, episode reward: -3126.000, mean reward: -18.174 [-50.000, 64.000], mean action: 1.209 [0.000, 3.000],  loss: 475.070007, mae: 137.841171, mean_q: -159.475357\n",
      "  5080/20000: episode: 18, duration: 1.566s, episode steps: 309, steps per second: 197, episode reward: -7190.000, mean reward: -23.269 [-50.000, 144.000], mean action: 0.955 [0.000, 3.000],  loss: 514.832947, mae: 190.074265, mean_q: -224.262589\n",
      "  5335/20000: episode: 19, duration: 1.285s, episode steps: 255, steps per second: 199, episode reward: -7400.000, mean reward: -29.020 [-50.000, 68.000], mean action: 1.345 [0.000, 3.000],  loss: 799.146973, mae: 184.950897, mean_q: -216.390457\n",
      "  5521/20000: episode: 20, duration: 0.939s, episode steps: 186, steps per second: 198, episode reward: -4002.000, mean reward: -21.516 [-50.000, 64.000], mean action: 1.452 [0.000, 3.000],  loss: 555.354187, mae: 182.101242, mean_q: -218.458160\n",
      "  5623/20000: episode: 21, duration: 0.533s, episode steps: 102, steps per second: 192, episode reward: -1760.000, mean reward: -17.255 [-50.000, 32.000], mean action: 1.745 [0.000, 3.000],  loss: 852.416748, mae: 165.844177, mean_q: -186.027390\n",
      "  5810/20000: episode: 22, duration: 0.937s, episode steps: 187, steps per second: 200, episode reward: -3988.000, mean reward: -21.326 [-50.000, 68.000], mean action: 0.893 [0.000, 3.000],  loss: 499.644226, mae: 151.756851, mean_q: -168.811432\n",
      "  5926/20000: episode: 23, duration: 0.579s, episode steps: 116, steps per second: 200, episode reward: -1278.000, mean reward: -11.017 [-50.000, 64.000], mean action: 2.095 [0.000, 3.000],  loss: 677.123840, mae: 155.858948, mean_q: -171.555023\n",
      "  6056/20000: episode: 24, duration: 0.645s, episode steps: 130, steps per second: 202, episode reward: -2480.000, mean reward: -19.077 [-50.000, 80.000], mean action: 2.169 [0.000, 3.000],  loss: 625.188171, mae: 150.857513, mean_q: -175.547440\n",
      "  6314/20000: episode: 25, duration: 1.307s, episode steps: 258, steps per second: 197, episode reward: -5302.000, mean reward: -20.550 [-50.000, 132.000], mean action: 0.810 [0.000, 3.000],  loss: 463.154907, mae: 204.717056, mean_q: -245.112885\n",
      "  6452/20000: episode: 26, duration: 0.678s, episode steps: 138, steps per second: 203, episode reward: -2088.000, mean reward: -15.130 [-50.000, 80.000], mean action: 1.688 [0.000, 3.000],  loss: 977.829956, mae: 181.888687, mean_q: -209.089386\n",
      "  6660/20000: episode: 27, duration: 1.054s, episode steps: 208, steps per second: 197, episode reward: -5104.000, mean reward: -24.538 [-50.000, 84.000], mean action: 1.510 [0.000, 3.000],  loss: 491.300079, mae: 189.281967, mean_q: -222.065353\n",
      "  6893/20000: episode: 28, duration: 1.180s, episode steps: 233, steps per second: 197, episode reward: -6656.000, mean reward: -28.567 [-50.000, 84.000], mean action: 1.914 [0.000, 3.000],  loss: 544.190735, mae: 202.424820, mean_q: -228.731461\n",
      "  7060/20000: episode: 29, duration: 0.848s, episode steps: 167, steps per second: 197, episode reward: -3148.000, mean reward: -18.850 [-50.000, 72.000], mean action: 1.251 [0.000, 3.000],  loss: 549.170532, mae: 191.426819, mean_q: -221.943024\n",
      "  7188/20000: episode: 30, duration: 0.642s, episode steps: 128, steps per second: 200, episode reward: -1522.000, mean reward: -11.891 [-50.000, 72.000], mean action: 1.766 [0.000, 3.000],  loss: 691.445068, mae: 183.030151, mean_q: -218.163086\n",
      "  7281/20000: episode: 31, duration: 0.460s, episode steps:  93, steps per second: 202, episode reward: -1066.000, mean reward: -11.462 [-50.000, 36.000], mean action: 1.720 [0.000, 3.000],  loss: 832.034973, mae: 167.804947, mean_q: -199.452515\n",
      "  7393/20000: episode: 32, duration: 0.570s, episode steps: 112, steps per second: 197, episode reward: -3038.000, mean reward: -27.125 [-50.000, 28.000], mean action: 0.652 [0.000, 3.000],  loss: 623.952698, mae: 130.464722, mean_q: -151.860931\n",
      "  7632/20000: episode: 33, duration: 1.235s, episode steps: 239, steps per second: 193, episode reward: -5306.000, mean reward: -22.201 [-50.000, 68.000], mean action: 1.682 [0.000, 3.000],  loss: 396.335846, mae: 196.017242, mean_q: -232.785294\n",
      "  7802/20000: episode: 34, duration: 0.850s, episode steps: 170, steps per second: 200, episode reward: -2534.000, mean reward: -14.906 [-50.000, 84.000], mean action: 2.347 [0.000, 3.000],  loss: 959.854492, mae: 212.086578, mean_q: -233.645996\n",
      "  7964/20000: episode: 35, duration: 0.823s, episode steps: 162, steps per second: 197, episode reward: -2280.000, mean reward: -14.074 [-50.000, 136.000], mean action: 1.241 [0.000, 3.000],  loss: 817.187622, mae: 184.338440, mean_q: -202.105148\n",
      "  8123/20000: episode: 36, duration: 0.791s, episode steps: 159, steps per second: 201, episode reward: -1910.000, mean reward: -12.013 [-50.000, 132.000], mean action: 1.031 [0.000, 3.000],  loss: 919.334229, mae: 170.504532, mean_q: -184.750320\n",
      "  8282/20000: episode: 37, duration: 0.812s, episode steps: 159, steps per second: 196, episode reward: -3096.000, mean reward: -19.472 [-50.000, 80.000], mean action: 1.333 [0.000, 3.000],  loss: 863.807678, mae: 164.997467, mean_q: -190.691193\n",
      "  8434/20000: episode: 38, duration: 0.773s, episode steps: 152, steps per second: 197, episode reward: -3904.000, mean reward: -25.684 [-50.000, 56.000], mean action: 1.678 [0.000, 3.000],  loss: 723.953857, mae: 169.342361, mean_q: -204.349106\n",
      "  8705/20000: episode: 39, duration: 1.346s, episode steps: 271, steps per second: 201, episode reward: -6194.000, mean reward: -22.856 [-50.000, 132.000], mean action: 1.498 [0.000, 3.000],  loss: 607.800049, mae: 230.916199, mean_q: -278.426300\n",
      "  8801/20000: episode: 40, duration: 0.481s, episode steps:  96, steps per second: 200, episode reward: -810.000, mean reward: -8.438 [-50.000, 48.000], mean action: 1.552 [0.000, 3.000],  loss: 1540.046509, mae: 240.208145, mean_q: -287.413910\n",
      "  8874/20000: episode: 41, duration: 0.372s, episode steps:  73, steps per second: 196, episode reward: -392.000, mean reward: -5.370 [-50.000, 44.000], mean action: 1.370 [0.000, 3.000],  loss: 751.171753, mae: 155.142776, mean_q: -175.963943\n",
      "  8999/20000: episode: 42, duration: 0.615s, episode steps: 125, steps per second: 203, episode reward: -2150.000, mean reward: -17.200 [-50.000, 68.000], mean action: 1.640 [0.000, 3.000],  loss: 541.350952, mae: 137.104477, mean_q: -158.568298\n",
      "  9251/20000: episode: 43, duration: 1.279s, episode steps: 252, steps per second: 197, episode reward: -6744.000, mean reward: -26.762 [-50.000, 72.000], mean action: 1.472 [0.000, 3.000],  loss: 560.920410, mae: 239.624786, mean_q: -280.351562\n",
      "  9361/20000: episode: 44, duration: 0.570s, episode steps: 110, steps per second: 193, episode reward: -1686.000, mean reward: -15.327 [-50.000, 68.000], mean action: 1.145 [0.000, 3.000],  loss: 1023.379822, mae: 223.913116, mean_q: -258.046082\n",
      "  9510/20000: episode: 45, duration: 0.753s, episode steps: 149, steps per second: 198, episode reward: -3808.000, mean reward: -25.557 [-50.000, 60.000], mean action: 1.302 [0.000, 3.000],  loss: 600.749084, mae: 162.197128, mean_q: -193.957153\n",
      "  9594/20000: episode: 46, duration: 0.434s, episode steps:  84, steps per second: 193, episode reward: -1042.000, mean reward: -12.405 [-50.000, 64.000], mean action: 1.214 [0.000, 3.000],  loss: 762.770752, mae: 189.029190, mean_q: -220.070709\n",
      "  9656/20000: episode: 47, duration: 0.322s, episode steps:  62, steps per second: 192, episode reward: -542.000, mean reward: -8.742 [-50.000, 28.000], mean action: 1.242 [0.000, 3.000],  loss: 614.734436, mae: 136.118073, mean_q: -155.527878\n",
      "  9758/20000: episode: 48, duration: 0.509s, episode steps: 102, steps per second: 200, episode reward: -1470.000, mean reward: -14.412 [-50.000, 48.000], mean action: 2.294 [0.000, 3.000],  loss: 645.002808, mae: 129.357361, mean_q: -143.544632\n",
      " 10199/20000: episode: 49, duration: 2.196s, episode steps: 441, steps per second: 201, episode reward: -12346.000, mean reward: -27.995 [-50.000, 136.000], mean action: 1.211 [0.000, 3.000],  loss: 479.929779, mae: 330.573853, mean_q: -407.276489\n",
      " 10310/20000: episode: 50, duration: 0.562s, episode steps: 111, steps per second: 198, episode reward: -514.000, mean reward: -4.631 [-50.000, 64.000], mean action: 1.766 [0.000, 3.000],  loss: 2355.423096, mae: 282.867798, mean_q: -341.740540\n",
      " 10451/20000: episode: 51, duration: 0.730s, episode steps: 141, steps per second: 193, episode reward: -2752.000, mean reward: -19.518 [-50.000, 68.000], mean action: 1.837 [0.000, 3.000],  loss: 811.271362, mae: 172.374283, mean_q: -193.148911\n",
      " 10736/20000: episode: 52, duration: 1.444s, episode steps: 285, steps per second: 197, episode reward: -8250.000, mean reward: -28.947 [-50.000, 88.000], mean action: 1.267 [0.000, 3.000],  loss: 537.399353, mae: 226.501175, mean_q: -269.133881\n",
      " 11020/20000: episode: 53, duration: 1.437s, episode steps: 284, steps per second: 198, episode reward: -5616.000, mean reward: -19.775 [-50.000, 180.000], mean action: 1.799 [0.000, 3.000],  loss: 1152.259521, mae: 278.742401, mean_q: -332.405121\n",
      " 11117/20000: episode: 54, duration: 0.532s, episode steps:  97, steps per second: 182, episode reward: -518.000, mean reward: -5.340 [-50.000, 92.000], mean action: 1.897 [0.000, 3.000],  loss: 2544.923584, mae: 265.246246, mean_q: -328.462982\n",
      " 11199/20000: episode: 55, duration: 0.435s, episode steps:  82, steps per second: 189, episode reward: -1128.000, mean reward: -13.756 [-50.000, 36.000], mean action: 1.073 [0.000, 3.000],  loss: 822.063232, mae: 154.237061, mean_q: -175.561554\n",
      " 11466/20000: episode: 56, duration: 1.352s, episode steps: 267, steps per second: 198, episode reward: -8114.000, mean reward: -30.390 [-50.000, 76.000], mean action: 1.693 [0.000, 3.000],  loss: 477.187561, mae: 208.580139, mean_q: -246.647964\n",
      " 11639/20000: episode: 57, duration: 0.870s, episode steps: 173, steps per second: 199, episode reward: -3416.000, mean reward: -19.746 [-50.000, 68.000], mean action: 1.775 [0.000, 3.000],  loss: 745.541565, mae: 237.827667, mean_q: -284.818237\n",
      " 11877/20000: episode: 58, duration: 1.213s, episode steps: 238, steps per second: 196, episode reward: -3618.000, mean reward: -15.202 [-50.000, 128.000], mean action: 1.697 [0.000, 3.000],  loss: 824.025879, mae: 277.189484, mean_q: -333.884857\n",
      " 12152/20000: episode: 59, duration: 1.571s, episode steps: 275, steps per second: 175, episode reward: -8072.000, mean reward: -29.353 [-50.000, 72.000], mean action: 0.996 [0.000, 3.000],  loss: 1035.738892, mae: 283.046906, mean_q: -334.548004\n",
      " 12514/20000: episode: 60, duration: 1.876s, episode steps: 362, steps per second: 193, episode reward: -10926.000, mean reward: -30.182 [-50.000, 128.000], mean action: 1.376 [0.000, 3.000],  loss: 646.490051, mae: 245.593063, mean_q: -304.303040\n",
      " 12645/20000: episode: 61, duration: 0.680s, episode steps: 131, steps per second: 193, episode reward: -2418.000, mean reward: -18.458 [-50.000, 64.000], mean action: 1.992 [0.000, 3.000],  loss: 1380.300049, mae: 233.828857, mean_q: -266.034790\n",
      " 12814/20000: episode: 62, duration: 0.837s, episode steps: 169, steps per second: 202, episode reward: -2540.000, mean reward: -15.030 [-50.000, 68.000], mean action: 1.172 [0.000, 3.000],  loss: 621.760376, mae: 198.622635, mean_q: -234.509796\n",
      " 12995/20000: episode: 63, duration: 0.901s, episode steps: 181, steps per second: 201, episode reward: -3758.000, mean reward: -20.762 [-50.000, 76.000], mean action: 1.856 [0.000, 3.000],  loss: 884.789734, mae: 240.985321, mean_q: -284.666870\n",
      " 13159/20000: episode: 64, duration: 0.810s, episode steps: 164, steps per second: 202, episode reward: -4086.000, mean reward: -24.915 [-50.000, 36.000], mean action: 1.433 [0.000, 3.000],  loss: 1207.947021, mae: 239.500595, mean_q: -283.632141\n",
      " 13288/20000: episode: 65, duration: 0.660s, episode steps: 129, steps per second: 195, episode reward: -1924.000, mean reward: -14.915 [-50.000, 80.000], mean action: 1.333 [0.000, 3.000],  loss: 945.963135, mae: 225.603195, mean_q: -271.150208\n",
      " 13442/20000: episode: 66, duration: 0.759s, episode steps: 154, steps per second: 203, episode reward: -3206.000, mean reward: -20.818 [-50.000, 80.000], mean action: 1.981 [0.000, 3.000],  loss: 767.877808, mae: 214.034805, mean_q: -256.857849\n",
      " 13608/20000: episode: 67, duration: 0.820s, episode steps: 166, steps per second: 202, episode reward: -4034.000, mean reward: -24.301 [-50.000, 72.000], mean action: 0.753 [0.000, 3.000],  loss: 794.189575, mae: 245.815018, mean_q: -286.991058\n",
      " 13761/20000: episode: 68, duration: 0.769s, episode steps: 153, steps per second: 199, episode reward: -3070.000, mean reward: -20.065 [-50.000, 64.000], mean action: 1.915 [0.000, 3.000],  loss: 904.572510, mae: 216.132965, mean_q: -256.371124\n",
      " 13941/20000: episode: 69, duration: 0.904s, episode steps: 180, steps per second: 199, episode reward: -3376.000, mean reward: -18.756 [-50.000, 68.000], mean action: 2.011 [0.000, 3.000],  loss: 747.298462, mae: 253.383240, mean_q: -296.971252\n",
      " 14227/20000: episode: 70, duration: 1.403s, episode steps: 286, steps per second: 204, episode reward: -9252.000, mean reward: -32.350 [-50.000, 84.000], mean action: 1.248 [0.000, 3.000],  loss: 712.748169, mae: 275.458466, mean_q: -320.733459\n",
      " 14393/20000: episode: 71, duration: 0.812s, episode steps: 166, steps per second: 204, episode reward: -3828.000, mean reward: -23.060 [-50.000, 100.000], mean action: 1.898 [0.000, 3.000],  loss: 802.373108, mae: 259.138092, mean_q: -315.779755\n",
      " 14562/20000: episode: 72, duration: 0.842s, episode steps: 169, steps per second: 201, episode reward: -3178.000, mean reward: -18.805 [-50.000, 96.000], mean action: 1.219 [0.000, 3.000],  loss: 827.608887, mae: 271.354156, mean_q: -315.892700\n",
      " 14707/20000: episode: 73, duration: 0.721s, episode steps: 145, steps per second: 201, episode reward: -2214.000, mean reward: -15.269 [-50.000, 64.000], mean action: 1.469 [0.000, 3.000],  loss: 1144.680298, mae: 266.188995, mean_q: -322.027954\n",
      " 14785/20000: episode: 74, duration: 0.398s, episode steps:  78, steps per second: 196, episode reward: 308.000, mean reward:  3.949 [-50.000, 72.000], mean action: 1.295 [0.000, 3.000],  loss: 1863.989624, mae: 253.920517, mean_q: -297.469666\n",
      " 14963/20000: episode: 75, duration: 0.889s, episode steps: 178, steps per second: 200, episode reward: -1970.000, mean reward: -11.067 [-50.000, 128.000], mean action: 1.399 [0.000, 3.000],  loss: 1255.642090, mae: 247.443649, mean_q: -272.928619\n",
      " 15043/20000: episode: 76, duration: 0.406s, episode steps:  80, steps per second: 197, episode reward: -246.000, mean reward: -3.075 [-50.000, 48.000], mean action: 2.025 [0.000, 3.000],  loss: 1400.520264, mae: 290.431488, mean_q: -346.750671\n",
      " 15199/20000: episode: 77, duration: 0.768s, episode steps: 156, steps per second: 203, episode reward: -3480.000, mean reward: -22.308 [-50.000, 68.000], mean action: 1.628 [0.000, 3.000],  loss: 687.788452, mae: 176.754593, mean_q: -206.790436\n",
      " 15363/20000: episode: 78, duration: 0.814s, episode steps: 164, steps per second: 202, episode reward: -1766.000, mean reward: -10.768 [-50.000, 72.000], mean action: 1.628 [0.000, 3.000],  loss: 849.482300, mae: 211.215485, mean_q: -246.979538\n",
      " 15616/20000: episode: 79, duration: 1.220s, episode steps: 253, steps per second: 207, episode reward: -7622.000, mean reward: -30.126 [-50.000, 76.000], mean action: 1.051 [0.000, 3.000],  loss: 708.823181, mae: 247.074249, mean_q: -297.223816\n",
      " 15773/20000: episode: 80, duration: 0.789s, episode steps: 157, steps per second: 199, episode reward: -3092.000, mean reward: -19.694 [-50.000, 120.000], mean action: 2.025 [0.000, 3.000],  loss: 864.486877, mae: 237.166656, mean_q: -284.255432\n",
      " 15843/20000: episode: 81, duration: 0.349s, episode steps:  70, steps per second: 201, episode reward: 492.000, mean reward:  7.029 [ 0.000, 72.000], mean action: 1.386 [0.000, 3.000],  loss: 1304.911499, mae: 226.161011, mean_q: -266.764893\n",
      " 16019/20000: episode: 82, duration: 0.882s, episode steps: 176, steps per second: 200, episode reward: -4410.000, mean reward: -25.057 [-50.000, 68.000], mean action: 1.051 [0.000, 3.000],  loss: 688.325745, mae: 206.594620, mean_q: -240.275574\n",
      " 16189/20000: episode: 83, duration: 0.856s, episode steps: 170, steps per second: 199, episode reward: -3544.000, mean reward: -20.847 [-50.000, 64.000], mean action: 1.900 [0.000, 3.000],  loss: 856.357727, mae: 233.074799, mean_q: -271.829224\n",
      " 16319/20000: episode: 84, duration: 0.649s, episode steps: 130, steps per second: 200, episode reward: -3176.000, mean reward: -24.431 [-50.000, 52.000], mean action: 0.885 [0.000, 3.000],  loss: 1171.257080, mae: 218.830826, mean_q: -235.731674\n",
      " 16383/20000: episode: 85, duration: 0.323s, episode steps:  64, steps per second: 198, episode reward: -668.000, mean reward: -10.438 [-50.000, 40.000], mean action: 1.812 [0.000, 3.000],  loss: 813.479126, mae: 178.916077, mean_q: -204.155243\n",
      " 16587/20000: episode: 86, duration: 1.011s, episode steps: 204, steps per second: 202, episode reward: -4452.000, mean reward: -21.824 [-50.000, 76.000], mean action: 1.064 [0.000, 3.000],  loss: 612.667603, mae: 197.547760, mean_q: -228.547623\n",
      " 16649/20000: episode: 87, duration: 0.344s, episode steps:  62, steps per second: 180, episode reward: 170.000, mean reward:  2.742 [-50.000, 32.000], mean action: 0.887 [0.000, 3.000],  loss: 1006.912598, mae: 270.088074, mean_q: -319.004913\n",
      " 16783/20000: episode: 88, duration: 0.698s, episode steps: 134, steps per second: 192, episode reward: -1884.000, mean reward: -14.060 [-50.000, 76.000], mean action: 1.224 [0.000, 3.000],  loss: 902.846069, mae: 176.948044, mean_q: -201.650040\n",
      " 16890/20000: episode: 89, duration: 0.540s, episode steps: 107, steps per second: 198, episode reward: -512.000, mean reward: -4.785 [-50.000, 88.000], mean action: 1.692 [0.000, 3.000],  loss: 909.022827, mae: 213.283539, mean_q: -254.833344\n",
      " 17167/20000: episode: 90, duration: 1.448s, episode steps: 277, steps per second: 191, episode reward: -6932.000, mean reward: -25.025 [-50.000, 148.000], mean action: 1.440 [0.000, 3.000],  loss: 646.132996, mae: 259.766632, mean_q: -314.270355\n",
      " 17359/20000: episode: 91, duration: 0.942s, episode steps: 192, steps per second: 204, episode reward: -3878.000, mean reward: -20.198 [-50.000, 92.000], mean action: 1.630 [0.000, 3.000],  loss: 878.009094, mae: 279.583466, mean_q: -326.963165\n",
      " 17636/20000: episode: 92, duration: 1.381s, episode steps: 277, steps per second: 201, episode reward: -8052.000, mean reward: -29.069 [-50.000, 64.000], mean action: 1.116 [0.000, 3.000],  loss: 738.885254, mae: 286.212524, mean_q: -341.258484\n",
      " 17735/20000: episode: 93, duration: 0.484s, episode steps:  99, steps per second: 205, episode reward: -184.000, mean reward: -1.859 [-50.000, 64.000], mean action: 1.152 [0.000, 3.000],  loss: 1369.792114, mae: 264.678650, mean_q: -317.790527\n",
      " 18021/20000: episode: 94, duration: 1.382s, episode steps: 286, steps per second: 207, episode reward: -5840.000, mean reward: -20.420 [-50.000, 164.000], mean action: 1.234 [0.000, 3.000],  loss: 694.609619, mae: 284.422668, mean_q: -343.065582\n",
      " 18140/20000: episode: 95, duration: 0.583s, episode steps: 119, steps per second: 204, episode reward: -1080.000, mean reward: -9.076 [-50.000, 76.000], mean action: 1.689 [0.000, 3.000],  loss: 1925.037842, mae: 282.897186, mean_q: -341.341095\n",
      " 18274/20000: episode: 96, duration: 0.655s, episode steps: 134, steps per second: 205, episode reward: -1718.000, mean reward: -12.821 [-50.000, 80.000], mean action: 0.978 [0.000, 3.000],  loss: 871.040039, mae: 203.766495, mean_q: -227.431488\n",
      " 18558/20000: episode: 97, duration: 1.372s, episode steps: 284, steps per second: 207, episode reward: -6456.000, mean reward: -22.732 [-50.000, 144.000], mean action: 1.067 [0.000, 3.000],  loss: 587.012756, mae: 235.560791, mean_q: -279.911224\n",
      " 18778/20000: episode: 98, duration: 1.071s, episode steps: 220, steps per second: 205, episode reward: -5968.000, mean reward: -27.127 [-50.000, 88.000], mean action: 1.882 [0.000, 3.000],  loss: 880.997559, mae: 181.093430, mean_q: -203.620377\n",
      " 18919/20000: episode: 99, duration: 0.685s, episode steps: 141, steps per second: 206, episode reward: -2002.000, mean reward: -14.199 [-50.000, 68.000], mean action: 1.652 [0.000, 3.000],  loss: 691.066711, mae: 184.889542, mean_q: -210.190094\n",
      " 19131/20000: episode: 100, duration: 1.036s, episode steps: 212, steps per second: 205, episode reward: -4258.000, mean reward: -20.085 [-50.000, 64.000], mean action: 1.203 [0.000, 3.000],  loss: 871.560486, mae: 224.495651, mean_q: -263.370758\n",
      " 19348/20000: episode: 101, duration: 1.049s, episode steps: 217, steps per second: 207, episode reward: -6274.000, mean reward: -28.912 [-50.000, 80.000], mean action: 1.014 [0.000, 3.000],  loss: 868.787964, mae: 220.638702, mean_q: -253.671295\n",
      " 19538/20000: episode: 102, duration: 0.930s, episode steps: 190, steps per second: 204, episode reward: -1600.000, mean reward: -8.421 [-50.000, 136.000], mean action: 1.595 [0.000, 3.000],  loss: 816.528809, mae: 225.151993, mean_q: -246.980118\n",
      " 19653/20000: episode: 103, duration: 0.564s, episode steps: 115, steps per second: 204, episode reward: -1564.000, mean reward: -13.600 [-50.000, 64.000], mean action: 1.548 [0.000, 3.000],  loss: 1339.558105, mae: 220.247391, mean_q: -246.075058\n",
      " 19801/20000: episode: 104, duration: 0.719s, episode steps: 148, steps per second: 206, episode reward: -1566.000, mean reward: -10.581 [-50.000, 68.000], mean action: 1.980 [0.000, 3.000],  loss: 962.396667, mae: 188.340103, mean_q: -217.011765\n",
      " 19939/20000: episode: 105, duration: 0.684s, episode steps: 138, steps per second: 202, episode reward: -638.000, mean reward: -4.623 [-50.000, 132.000], mean action: 1.188 [0.000, 3.000],  loss: 914.109619, mae: 189.474182, mean_q: -208.047668\n",
      "done, took 98.405 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29dd77b2e0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "# temporalWindow = 5\n",
    "temporalWindow = 1\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "replayMemory = SequentialMemory(limit=100, window_length=temporalWindow)\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=1000,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=20000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1002/845857735.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  transformed = np.where(grid > 0, np.log2(grid), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 is over.\n",
      "Game 2 is over.\n",
      "Game 3 is over.\n",
      "Game 4 is over.\n",
      "Game 5 is over.\n",
      "Game 6 is over.\n",
      "Game 7 is over.\n",
      "Game 8 is over.\n",
      "Game 9 is over.\n",
      "Game 10 is over.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gameId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>688</td>\n",
       "      <td>64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>182</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>122</td>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.297297</td>\n",
       "      <td>1.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1008</td>\n",
       "      <td>128</td>\n",
       "      <td>0.430851</td>\n",
       "      <td>188</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>122</td>\n",
       "      <td>36</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.711111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1252</td>\n",
       "      <td>128</td>\n",
       "      <td>0.519435</td>\n",
       "      <td>283</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>182</td>\n",
       "      <td>63</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.370370</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>32</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>1.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>708</td>\n",
       "      <td>64</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>290</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>196</td>\n",
       "      <td>67</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>2.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>756</td>\n",
       "      <td>64</td>\n",
       "      <td>0.672026</td>\n",
       "      <td>311</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>247</td>\n",
       "      <td>34</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>6.024390</td>\n",
       "      <td>1.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>700</td>\n",
       "      <td>64</td>\n",
       "      <td>0.449102</td>\n",
       "      <td>167</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>106</td>\n",
       "      <td>29</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.212121</td>\n",
       "      <td>1.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>392</td>\n",
       "      <td>32</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>140</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>336</td>\n",
       "      <td>32</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.588235</td>\n",
       "      <td>1.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1160</td>\n",
       "      <td>128</td>\n",
       "      <td>0.336898</td>\n",
       "      <td>187</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>75</td>\n",
       "      <td>59</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.630435</td>\n",
       "      <td>1.404762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  higherTile  invalidMove (%)  steps  upMoves  downMoves  \\\n",
       "gameId                                                                  \n",
       "0         688          64         0.500000    182        8          8   \n",
       "1        1008         128         0.430851    188       16         14   \n",
       "2        1252         128         0.519435    283       21         17   \n",
       "3         272          32         0.242857     70        8          5   \n",
       "4         708          64         0.675862    290       15         12   \n",
       "5         756          64         0.672026    311       19         11   \n",
       "6         700          64         0.449102    167       20         12   \n",
       "7         392          32         0.535714    140       55         12   \n",
       "8         336          32         0.132353     68       16         11   \n",
       "9        1160         128         0.336898    187       40         13   \n",
       "\n",
       "        leftMoves  rightMoves  upSeqAvg  downSeqAvg  leftSeqAvg  rightSeqAvg  \n",
       "gameId                                                                        \n",
       "0             122          44  1.000000    1.000000    3.297297     1.257143  \n",
       "1             122          36  1.142857    1.000000    2.711111     1.000000  \n",
       "2             182          63  1.105263    1.000000    3.370370     1.312500  \n",
       "3              29          28  1.000000    1.250000    1.318182     1.473684  \n",
       "4             196          67  1.000000    1.000000    5.600000     2.030303  \n",
       "5             247          34  1.187500    1.100000    6.024390     1.030303  \n",
       "6             106          29  1.176471    1.000000    3.212121     1.074074  \n",
       "7              60          13  4.230769    1.090909    3.000000     1.000000  \n",
       "8              27          14  1.142857    1.000000    1.588235     1.272727  \n",
       "9              75          59  2.500000    1.083333    1.630435     1.404762  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>727.200000</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>0.449510</td>\n",
       "      <td>188.600000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>11.50000</td>\n",
       "      <td>116.600000</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>1.548572</td>\n",
       "      <td>1.052424</td>\n",
       "      <td>3.175214</td>\n",
       "      <td>1.285550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>336.263389</td>\n",
       "      <td>40.053298</td>\n",
       "      <td>0.173827</td>\n",
       "      <td>85.414285</td>\n",
       "      <td>14.680297</td>\n",
       "      <td>3.24037</td>\n",
       "      <td>73.327576</td>\n",
       "      <td>19.264533</td>\n",
       "      <td>1.043326</td>\n",
       "      <td>0.081803</td>\n",
       "      <td>1.586444</td>\n",
       "      <td>0.312501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>272.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>466.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.360387</td>\n",
       "      <td>146.750000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.900604</td>\n",
       "      <td>1.041246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>704.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.474551</td>\n",
       "      <td>184.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.106061</td>\n",
       "      <td>1.264935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.531644</td>\n",
       "      <td>259.250000</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>12.75000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>1.184743</td>\n",
       "      <td>1.089015</td>\n",
       "      <td>3.352102</td>\n",
       "      <td>1.381696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1252.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>6.024390</td>\n",
       "      <td>2.030303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score  higherTile  invalidMove (%)       steps    upMoves  \\\n",
       "count    10.000000   10.000000        10.000000   10.000000  10.000000   \n",
       "mean    727.200000   73.600000         0.449510  188.600000  21.800000   \n",
       "std     336.263389   40.053298         0.173827   85.414285  14.680297   \n",
       "min     272.000000   32.000000         0.132353   68.000000   8.000000   \n",
       "25%     466.000000   40.000000         0.360387  146.750000  15.250000   \n",
       "50%     704.000000   64.000000         0.474551  184.500000  17.500000   \n",
       "75%     945.000000  112.000000         0.531644  259.250000  20.750000   \n",
       "max    1252.000000  128.000000         0.675862  311.000000  55.000000   \n",
       "\n",
       "       downMoves   leftMoves  rightMoves   upSeqAvg  downSeqAvg  leftSeqAvg  \\\n",
       "count   10.00000   10.000000   10.000000  10.000000   10.000000   10.000000   \n",
       "mean    11.50000  116.600000   38.700000   1.548572    1.052424    3.175214   \n",
       "std      3.24037   73.327576   19.264533   1.043326    0.081803    1.586444   \n",
       "min      5.00000   27.000000   13.000000   1.000000    1.000000    1.318182   \n",
       "25%     11.00000   63.750000   28.250000   1.026316    1.000000    1.900604   \n",
       "50%     12.00000  114.000000   35.000000   1.142857    1.000000    3.106061   \n",
       "75%     12.75000  167.000000   55.250000   1.184743    1.089015    3.352102   \n",
       "max     17.00000  247.000000   67.000000   4.230769    1.250000    6.024390   \n",
       "\n",
       "       rightSeqAvg  \n",
       "count    10.000000  \n",
       "mean      1.285550  \n",
       "std       0.312501  \n",
       "min       1.000000  \n",
       "25%       1.041246  \n",
       "50%       1.264935  \n",
       "75%       1.381696  \n",
       "max       2.030303  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "def log2_transform(grid):\n",
    "    transformed = np.where(grid > 0, np.log2(grid), 0)\n",
    "    return transformed\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(10)]\n",
    "metrics = {\n",
    "    \"gameId\": [],\n",
    "    \"score\": [],\n",
    "    \"higherTile\": [],\n",
    "    \"invalidMove (%)\": [],\n",
    "    \"steps\": [],\n",
    "    \"upMoves\": [],\n",
    "    \"downMoves\": [],\n",
    "    \"leftMoves\": [],\n",
    "    \"rightMoves\": [],\n",
    "    \"upSeqAvg\": [],\n",
    "    \"downSeqAvg\": [],\n",
    "    \"leftSeqAvg\": [],\n",
    "    \"rightSeqAvg\": [],\n",
    "}\n",
    "for i, game in enumerate(games):\n",
    "    # Metrics\n",
    "    steps = 0\n",
    "    invalidMove = 0\n",
    "    upMoves = 0\n",
    "    downMoves = 0\n",
    "    leftMoves = 0\n",
    "    rightMoves = 0\n",
    "    upSeqAvg = 0\n",
    "    downSeqAvg = 0\n",
    "    leftSeqAvg = 0\n",
    "    rightSeqAvg = 0\n",
    "\n",
    "    sequences = {\n",
    "        \"up\": [],\n",
    "        \"down\": [],\n",
    "        \"left\": [],\n",
    "        \"right\": []\n",
    "    }\n",
    "    lastMove = None\n",
    "\n",
    "    # print(f\"Game {i + 1} initial state:\")\n",
    "    # game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        observation = log2_transform(observation)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        # print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        if not game.move(actionMapping[predictedAction]):\n",
    "            invalidMove += 1\n",
    "        \n",
    "        # print(f\"Game {i + 1} currrent state:\")\n",
    "        # game.render()\n",
    "\n",
    "        # input(\"\\nPress any key to continue...\")\n",
    "\n",
    "        steps += 1\n",
    "        if actionMapping[predictedAction] == \"up\":\n",
    "            upMoves += 1\n",
    "            if lastMove == \"up\":\n",
    "                sequences[\"up\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"up\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"down\":\n",
    "            downMoves += 1\n",
    "            if lastMove == \"down\":\n",
    "                sequences[\"down\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"down\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"left\":\n",
    "            leftMoves += 1\n",
    "            if lastMove == \"left\":\n",
    "                sequences[\"left\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"left\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"right\":\n",
    "            rightMoves += 1\n",
    "            if lastMove == \"right\":\n",
    "                sequences[\"right\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"right\"].append(1)\n",
    "        \n",
    "        lastMove = actionMapping[predictedAction]\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "    metrics[\"gameId\"].append(i)\n",
    "    metrics[\"score\"].append(game.score)\n",
    "    metrics[\"higherTile\"].append(np.max(game.grid))\n",
    "    metrics[\"invalidMove (%)\"].append(invalidMove/steps)\n",
    "    metrics[\"steps\"].append(steps)\n",
    "    metrics[\"upMoves\"].append(upMoves)\n",
    "    metrics[\"downMoves\"].append(downMoves)\n",
    "    metrics[\"leftMoves\"].append(leftMoves)\n",
    "    metrics[\"rightMoves\"].append(rightMoves)\n",
    "    metrics[\"upSeqAvg\"].append(np.mean(np.array(sequences[\"up\"])))\n",
    "    metrics[\"downSeqAvg\"].append(np.mean(np.array(sequences[\"down\"])))\n",
    "    metrics[\"leftSeqAvg\"].append(np.mean(np.array(sequences[\"left\"])))\n",
    "    metrics[\"rightSeqAvg\"].append(np.mean(np.array(sequences[\"right\"])))\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics).set_index(\"gameId\")\n",
    "display(metrics)\n",
    "display(metrics.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [\"up\", \"left\", \"right\", \"down\"]\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
