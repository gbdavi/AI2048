{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 20:44:17.069210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-25 20:44:17.069253: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   686/20000: episode: 1, duration: 1.145s, episode steps: 686, steps per second: 599, episode reward: -27436.000, mean reward: -39.994 [-50.000, 160.000], mean action: 1.067 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1058/20000: episode: 2, duration: 2.520s, episode steps: 372, steps per second: 148, episode reward: -13456.000, mean reward: -36.172 [-50.000, 76.000], mean action: 1.110 [0.000, 3.000],  loss: 938.493001, mae: 11.008136, mean_q: 0.757472\n",
      "  1473/20000: episode: 3, duration: 2.504s, episode steps: 415, steps per second: 166, episode reward: -12560.000, mean reward: -30.265 [-50.000, 136.000], mean action: 1.610 [0.000, 3.000],  loss: 339.907043, mae: 27.749821, mean_q: -25.188963\n",
      "  1756/20000: episode: 4, duration: 1.686s, episode steps: 283, steps per second: 168, episode reward: -5934.000, mean reward: -20.968 [-50.000, 80.000], mean action: 1.035 [0.000, 3.000],  loss: 341.804840, mae: 77.952507, mean_q: -92.816711\n",
      "  2045/20000: episode: 5, duration: 1.762s, episode steps: 289, steps per second: 164, episode reward: -7488.000, mean reward: -25.910 [-50.000, 160.000], mean action: 1.913 [0.000, 3.000],  loss: 393.496429, mae: 116.197121, mean_q: -141.725479\n",
      "  2391/20000: episode: 6, duration: 2.077s, episode steps: 346, steps per second: 167, episode reward: -10068.000, mean reward: -29.098 [-50.000, 128.000], mean action: 1.633 [0.000, 3.000],  loss: 484.577209, mae: 161.861267, mean_q: -200.349014\n",
      "  2741/20000: episode: 7, duration: 2.090s, episode steps: 350, steps per second: 167, episode reward: -9012.000, mean reward: -25.749 [-50.000, 136.000], mean action: 1.049 [0.000, 3.000],  loss: 624.311951, mae: 206.219727, mean_q: -257.495789\n",
      "  2928/20000: episode: 8, duration: 1.114s, episode steps: 187, steps per second: 168, episode reward: -3008.000, mean reward: -16.086 [-50.000, 76.000], mean action: 1.358 [0.000, 3.000],  loss: 798.118286, mae: 241.852066, mean_q: -305.271179\n",
      "  2976/20000: episode: 9, duration: 0.293s, episode steps:  48, steps per second: 164, episode reward: 158.000, mean reward:  3.292 [-50.000, 32.000], mean action: 1.688 [0.000, 3.000],  loss: 754.263000, mae: 251.934494, mean_q: -321.237549\n",
      "  3269/20000: episode: 10, duration: 1.747s, episode steps: 293, steps per second: 168, episode reward: -9288.000, mean reward: -31.700 [-50.000, 92.000], mean action: 1.249 [0.000, 3.000],  loss: 861.433350, mae: 251.037537, mean_q: -312.850189\n",
      "  3491/20000: episode: 11, duration: 1.341s, episode steps: 222, steps per second: 166, episode reward: -4592.000, mean reward: -20.685 [-50.000, 72.000], mean action: 1.486 [0.000, 3.000],  loss: 838.610657, mae: 272.322113, mean_q: -345.978943\n",
      "  3572/20000: episode: 12, duration: 0.503s, episode steps:  81, steps per second: 161, episode reward: -1410.000, mean reward: -17.407 [-50.000, 48.000], mean action: 1.296 [0.000, 3.000],  loss: 1262.938843, mae: 287.752380, mean_q: -364.893921\n",
      "  3798/20000: episode: 13, duration: 1.369s, episode steps: 226, steps per second: 165, episode reward: -4312.000, mean reward: -19.080 [-50.000, 148.000], mean action: 1.296 [0.000, 3.000],  loss: 1093.712891, mae: 301.379120, mean_q: -383.763123\n",
      "  3864/20000: episode: 14, duration: 0.404s, episode steps:  66, steps per second: 163, episode reward: 18.000, mean reward:  0.273 [-50.000, 56.000], mean action: 2.379 [0.000, 3.000],  loss: 1026.786743, mae: 321.555847, mean_q: -411.278534\n",
      "  4090/20000: episode: 15, duration: 1.351s, episode steps: 226, steps per second: 167, episode reward: -3052.000, mean reward: -13.504 [-50.000, 168.000], mean action: 1.885 [0.000, 3.000],  loss: 1328.709961, mae: 326.238251, mean_q: -414.461182\n",
      "  4220/20000: episode: 16, duration: 0.768s, episode steps: 130, steps per second: 169, episode reward: -2194.000, mean reward: -16.877 [-50.000, 72.000], mean action: 2.138 [0.000, 3.000],  loss: 1369.225708, mae: 338.390442, mean_q: -430.393280\n",
      "  4292/20000: episode: 17, duration: 0.440s, episode steps:  72, steps per second: 164, episode reward: -124.000, mean reward: -1.722 [-50.000, 44.000], mean action: 1.917 [0.000, 3.000],  loss: 2115.915771, mae: 342.998260, mean_q: -435.488007\n",
      "  4355/20000: episode: 18, duration: 0.382s, episode steps:  63, steps per second: 165, episode reward: 92.000, mean reward:  1.460 [-50.000, 28.000], mean action: 2.143 [0.000, 3.000],  loss: 1494.253296, mae: 340.669312, mean_q: -430.357819\n",
      "  4703/20000: episode: 19, duration: 2.078s, episode steps: 348, steps per second: 167, episode reward: -5660.000, mean reward: -16.264 [-50.000, 260.000], mean action: 1.638 [0.000, 3.000],  loss: 1625.765503, mae: 346.586700, mean_q: -438.372803\n",
      "  4943/20000: episode: 20, duration: 1.433s, episode steps: 240, steps per second: 168, episode reward: -3788.000, mean reward: -15.783 [-50.000, 152.000], mean action: 1.613 [0.000, 3.000],  loss: 1957.491943, mae: 359.218170, mean_q: -453.835510\n",
      "  5120/20000: episode: 21, duration: 1.067s, episode steps: 177, steps per second: 166, episode reward: -2328.000, mean reward: -13.153 [-50.000, 76.000], mean action: 1.531 [0.000, 3.000],  loss: 1898.589844, mae: 380.880066, mean_q: -483.715515\n",
      "  5238/20000: episode: 22, duration: 0.728s, episode steps: 118, steps per second: 162, episode reward: -260.000, mean reward: -2.203 [-50.000, 108.000], mean action: 1.966 [0.000, 3.000],  loss: 1964.630615, mae: 390.708771, mean_q: -495.495209\n",
      "  5338/20000: episode: 23, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: 346.000, mean reward:  3.460 [-50.000, 68.000], mean action: 2.010 [0.000, 3.000],  loss: 2156.479736, mae: 385.741211, mean_q: -489.241638\n",
      "  5416/20000: episode: 24, duration: 0.483s, episode steps:  78, steps per second: 161, episode reward: 22.000, mean reward:  0.282 [-50.000, 64.000], mean action: 1.526 [0.000, 3.000],  loss: 2205.532959, mae: 388.179382, mean_q: -488.330292\n",
      "  5482/20000: episode: 25, duration: 0.408s, episode steps:  66, steps per second: 162, episode reward: 406.000, mean reward:  6.152 [-50.000, 68.000], mean action: 2.030 [0.000, 3.000],  loss: 2410.807373, mae: 379.912811, mean_q: -480.688812\n",
      "  5662/20000: episode: 26, duration: 1.103s, episode steps: 180, steps per second: 163, episode reward: -4196.000, mean reward: -23.311 [-50.000, 88.000], mean action: 0.833 [0.000, 3.000],  loss: 2519.161133, mae: 377.209534, mean_q: -475.741150\n",
      "  5854/20000: episode: 27, duration: 1.143s, episode steps: 192, steps per second: 168, episode reward: -5904.000, mean reward: -30.750 [-50.000, 40.000], mean action: 1.521 [0.000, 3.000],  loss: 2089.853271, mae: 379.642120, mean_q: -480.777924\n",
      "  6162/20000: episode: 28, duration: 1.829s, episode steps: 308, steps per second: 168, episode reward: -7550.000, mean reward: -24.513 [-50.000, 140.000], mean action: 1.571 [0.000, 3.000],  loss: 1901.650757, mae: 399.136353, mean_q: -506.571747\n",
      "  6370/20000: episode: 29, duration: 1.295s, episode steps: 208, steps per second: 161, episode reward: -3206.000, mean reward: -15.413 [-50.000, 128.000], mean action: 1.519 [0.000, 3.000],  loss: 2345.650146, mae: 412.736969, mean_q: -524.033630\n",
      "  6440/20000: episode: 30, duration: 0.426s, episode steps:  70, steps per second: 164, episode reward: -628.000, mean reward: -8.971 [-50.000, 32.000], mean action: 1.514 [0.000, 3.000],  loss: 2480.593506, mae: 422.244476, mean_q: -536.605774\n",
      "  6627/20000: episode: 31, duration: 1.107s, episode steps: 187, steps per second: 169, episode reward: -5538.000, mean reward: -29.615 [-50.000, 36.000], mean action: 1.631 [0.000, 3.000],  loss: 2148.145752, mae: 423.309235, mean_q: -536.307373\n",
      "  6926/20000: episode: 32, duration: 1.801s, episode steps: 299, steps per second: 166, episode reward: -6330.000, mean reward: -21.171 [-50.000, 164.000], mean action: 1.709 [0.000, 3.000],  loss: 2255.403320, mae: 429.812805, mean_q: -545.285522\n",
      "  7076/20000: episode: 33, duration: 0.911s, episode steps: 150, steps per second: 165, episode reward: -1088.000, mean reward: -7.253 [-50.000, 128.000], mean action: 2.073 [0.000, 3.000],  loss: 2646.714844, mae: 441.612396, mean_q: -561.203552\n",
      "  7137/20000: episode: 34, duration: 0.374s, episode steps:  61, steps per second: 163, episode reward: 252.000, mean reward:  4.131 [-50.000, 48.000], mean action: 1.967 [0.000, 3.000],  loss: 2521.228760, mae: 441.641663, mean_q: -561.987366\n",
      "  7206/20000: episode: 35, duration: 0.418s, episode steps:  69, steps per second: 165, episode reward: -198.000, mean reward: -2.870 [-50.000, 36.000], mean action: 2.174 [0.000, 3.000],  loss: 2421.701172, mae: 439.418091, mean_q: -556.027588\n",
      "  7542/20000: episode: 36, duration: 2.018s, episode steps: 336, steps per second: 167, episode reward: -8676.000, mean reward: -25.821 [-50.000, 132.000], mean action: 1.899 [0.000, 3.000],  loss: 2372.671875, mae: 462.307465, mean_q: -588.857544\n",
      "  7715/20000: episode: 37, duration: 1.036s, episode steps: 173, steps per second: 167, episode reward: -994.000, mean reward: -5.746 [-50.000, 136.000], mean action: 1.532 [0.000, 3.000],  loss: 2296.513428, mae: 491.433533, mean_q: -628.278625\n",
      "  7758/20000: episode: 38, duration: 0.263s, episode steps:  43, steps per second: 164, episode reward: 80.000, mean reward:  1.860 [-50.000, 24.000], mean action: 1.860 [0.000, 3.000],  loss: 3003.511719, mae: 502.220825, mean_q: -642.088257\n",
      "  7908/20000: episode: 39, duration: 0.892s, episode steps: 150, steps per second: 168, episode reward: -2634.000, mean reward: -17.560 [-50.000, 72.000], mean action: 1.100 [0.000, 3.000],  loss: 2841.007324, mae: 501.547821, mean_q: -641.147522\n",
      "  8091/20000: episode: 40, duration: 1.177s, episode steps: 183, steps per second: 156, episode reward: -3504.000, mean reward: -19.148 [-50.000, 88.000], mean action: 1.224 [0.000, 3.000],  loss: 2479.552002, mae: 500.736694, mean_q: -638.774353\n",
      "  8317/20000: episode: 41, duration: 1.358s, episode steps: 226, steps per second: 166, episode reward: -3984.000, mean reward: -17.628 [-50.000, 132.000], mean action: 1.549 [0.000, 3.000],  loss: 2587.398682, mae: 508.133698, mean_q: -648.839355\n",
      "  8583/20000: episode: 42, duration: 1.592s, episode steps: 266, steps per second: 167, episode reward: -7154.000, mean reward: -26.895 [-50.000, 72.000], mean action: 1.124 [0.000, 3.000],  loss: 2343.712646, mae: 520.359009, mean_q: -665.238403\n",
      "  8677/20000: episode: 43, duration: 0.567s, episode steps:  94, steps per second: 166, episode reward: -660.000, mean reward: -7.021 [-50.000, 44.000], mean action: 1.862 [0.000, 3.000],  loss: 2860.307129, mae: 532.299561, mean_q: -679.054749\n",
      "  8802/20000: episode: 44, duration: 0.765s, episode steps: 125, steps per second: 163, episode reward: -1082.000, mean reward: -8.656 [-50.000, 96.000], mean action: 1.312 [0.000, 3.000],  loss: 3497.322021, mae: 533.765991, mean_q: -680.538818\n",
      "  8938/20000: episode: 45, duration: 0.825s, episode steps: 136, steps per second: 165, episode reward: -944.000, mean reward: -6.941 [-50.000, 100.000], mean action: 2.228 [0.000, 3.000],  loss: 2699.150391, mae: 522.769409, mean_q: -666.572876\n",
      "  9008/20000: episode: 46, duration: 0.424s, episode steps:  70, steps per second: 165, episode reward: -210.000, mean reward: -3.000 [-50.000, 52.000], mean action: 2.129 [0.000, 3.000],  loss: 2743.446533, mae: 525.801208, mean_q: -672.449158\n",
      "  9162/20000: episode: 47, duration: 0.937s, episode steps: 154, steps per second: 164, episode reward: -3120.000, mean reward: -20.260 [-50.000, 76.000], mean action: 1.753 [0.000, 3.000],  loss: 2957.300293, mae: 525.742798, mean_q: -670.585266\n",
      "  9275/20000: episode: 48, duration: 0.685s, episode steps: 113, steps per second: 165, episode reward: -116.000, mean reward: -1.027 [-50.000, 136.000], mean action: 2.124 [0.000, 3.000],  loss: 2785.757568, mae: 538.594482, mean_q: -688.831238\n",
      "  9457/20000: episode: 49, duration: 1.094s, episode steps: 182, steps per second: 166, episode reward: -598.000, mean reward: -3.286 [-50.000, 156.000], mean action: 2.165 [0.000, 3.000],  loss: 2754.891602, mae: 528.053528, mean_q: -673.459900\n",
      "  9855/20000: episode: 50, duration: 2.340s, episode steps: 398, steps per second: 170, episode reward: -8754.000, mean reward: -21.995 [-50.000, 272.000], mean action: 2.018 [0.000, 3.000],  loss: 3102.851074, mae: 531.422302, mean_q: -677.906433\n",
      "  9923/20000: episode: 51, duration: 0.407s, episode steps:  68, steps per second: 167, episode reward: -868.000, mean reward: -12.765 [-50.000, 32.000], mean action: 2.324 [0.000, 3.000],  loss: 4749.426758, mae: 544.809875, mean_q: -692.453552\n",
      " 10108/20000: episode: 52, duration: 1.120s, episode steps: 185, steps per second: 165, episode reward: -3098.000, mean reward: -16.746 [-50.000, 72.000], mean action: 1.573 [0.000, 3.000],  loss: 3396.926270, mae: 543.355469, mean_q: -694.316406\n",
      " 10254/20000: episode: 53, duration: 0.876s, episode steps: 146, steps per second: 167, episode reward: -2952.000, mean reward: -20.219 [-50.000, 64.000], mean action: 1.575 [0.000, 3.000],  loss: 3285.631348, mae: 550.877441, mean_q: -703.781128\n",
      " 10372/20000: episode: 54, duration: 0.718s, episode steps: 118, steps per second: 164, episode reward: -2522.000, mean reward: -21.373 [-50.000, 40.000], mean action: 1.695 [0.000, 3.000],  loss: 3497.712158, mae: 547.608704, mean_q: -697.180603\n",
      " 10517/20000: episode: 55, duration: 0.882s, episode steps: 145, steps per second: 164, episode reward: -1124.000, mean reward: -7.752 [-50.000, 140.000], mean action: 1.414 [0.000, 3.000],  loss: 2477.050537, mae: 548.272339, mean_q: -703.557678\n",
      " 10653/20000: episode: 56, duration: 0.823s, episode steps: 136, steps per second: 165, episode reward: -3704.000, mean reward: -27.235 [-50.000, 40.000], mean action: 1.081 [0.000, 3.000],  loss: 3406.529053, mae: 537.060791, mean_q: -687.799316\n",
      " 10790/20000: episode: 57, duration: 0.828s, episode steps: 137, steps per second: 166, episode reward: -1890.000, mean reward: -13.796 [-50.000, 88.000], mean action: 1.496 [0.000, 3.000],  loss: 3925.501709, mae: 541.715759, mean_q: -690.564453\n",
      " 11138/20000: episode: 58, duration: 2.083s, episode steps: 348, steps per second: 167, episode reward: -7092.000, mean reward: -20.379 [-50.000, 148.000], mean action: 2.066 [0.000, 3.000],  loss: 3360.228027, mae: 556.969055, mean_q: -713.605469\n",
      " 11289/20000: episode: 59, duration: 0.892s, episode steps: 151, steps per second: 169, episode reward: -1618.000, mean reward: -10.715 [-50.000, 128.000], mean action: 1.768 [0.000, 3.000],  loss: 2727.010010, mae: 582.443115, mean_q: -748.233765\n",
      " 11452/20000: episode: 60, duration: 0.954s, episode steps: 163, steps per second: 171, episode reward: -1174.000, mean reward: -7.202 [-50.000, 164.000], mean action: 1.429 [0.000, 3.000],  loss: 3668.069092, mae: 581.276367, mean_q: -746.230835\n",
      " 11559/20000: episode: 61, duration: 0.640s, episode steps: 107, steps per second: 167, episode reward: -1708.000, mean reward: -15.963 [-50.000, 64.000], mean action: 1.607 [0.000, 3.000],  loss: 4106.434082, mae: 586.924927, mean_q: -756.194153\n",
      " 11787/20000: episode: 62, duration: 1.344s, episode steps: 228, steps per second: 170, episode reward: -2908.000, mean reward: -12.754 [-50.000, 152.000], mean action: 1.610 [0.000, 3.000],  loss: 2809.077881, mae: 577.833801, mean_q: -744.520752\n",
      " 12105/20000: episode: 63, duration: 1.884s, episode steps: 318, steps per second: 169, episode reward: -8394.000, mean reward: -26.396 [-50.000, 144.000], mean action: 1.176 [0.000, 3.000],  loss: 3716.768066, mae: 576.690796, mean_q: -741.508667\n",
      " 12216/20000: episode: 64, duration: 0.674s, episode steps: 111, steps per second: 165, episode reward: -1256.000, mean reward: -11.315 [-50.000, 76.000], mean action: 1.541 [0.000, 3.000],  loss: 4062.198975, mae: 582.503357, mean_q: -749.015259\n",
      " 12419/20000: episode: 65, duration: 1.199s, episode steps: 203, steps per second: 169, episode reward: -2378.000, mean reward: -11.714 [-50.000, 148.000], mean action: 1.611 [0.000, 3.000],  loss: 4076.831055, mae: 581.855591, mean_q: -747.498840\n",
      " 12567/20000: episode: 66, duration: 0.888s, episode steps: 148, steps per second: 167, episode reward: -2858.000, mean reward: -19.311 [-50.000, 68.000], mean action: 1.838 [0.000, 3.000],  loss: 3492.327881, mae: 580.925476, mean_q: -748.006042\n",
      " 12949/20000: episode: 67, duration: 2.260s, episode steps: 382, steps per second: 169, episode reward: -10690.000, mean reward: -27.984 [-50.000, 156.000], mean action: 1.539 [0.000, 3.000],  loss: 3360.170166, mae: 589.272827, mean_q: -758.851624\n",
      " 13103/20000: episode: 68, duration: 0.908s, episode steps: 154, steps per second: 170, episode reward: -568.000, mean reward: -3.688 [-50.000, 136.000], mean action: 1.773 [0.000, 3.000],  loss: 3466.914062, mae: 595.783386, mean_q: -766.914551\n",
      " 13322/20000: episode: 69, duration: 1.292s, episode steps: 219, steps per second: 170, episode reward: -2668.000, mean reward: -12.183 [-50.000, 140.000], mean action: 1.753 [0.000, 3.000],  loss: 2713.660645, mae: 592.022583, mean_q: -761.499573\n",
      " 13566/20000: episode: 70, duration: 1.449s, episode steps: 244, steps per second: 168, episode reward: -4048.000, mean reward: -16.590 [-50.000, 128.000], mean action: 1.578 [0.000, 3.000],  loss: 3398.777100, mae: 593.821838, mean_q: -761.262268\n",
      " 13802/20000: episode: 71, duration: 1.404s, episode steps: 236, steps per second: 168, episode reward: -6086.000, mean reward: -25.788 [-50.000, 84.000], mean action: 1.525 [0.000, 3.000],  loss: 3150.731689, mae: 603.858398, mean_q: -775.446106\n",
      " 14029/20000: episode: 72, duration: 1.354s, episode steps: 227, steps per second: 168, episode reward: -4868.000, mean reward: -21.445 [-50.000, 140.000], mean action: 1.626 [0.000, 3.000],  loss: 2699.534668, mae: 615.336853, mean_q: -793.471313\n",
      " 14315/20000: episode: 73, duration: 1.705s, episode steps: 286, steps per second: 168, episode reward: -5970.000, mean reward: -20.874 [-50.000, 136.000], mean action: 1.388 [0.000, 3.000],  loss: 3139.145264, mae: 626.709595, mean_q: -809.608154\n",
      " 14444/20000: episode: 74, duration: 0.769s, episode steps: 129, steps per second: 168, episode reward: -1940.000, mean reward: -15.039 [-50.000, 72.000], mean action: 1.992 [0.000, 3.000],  loss: 2782.061035, mae: 642.244080, mean_q: -831.398071\n",
      " 14720/20000: episode: 75, duration: 1.626s, episode steps: 276, steps per second: 170, episode reward: -5568.000, mean reward: -20.174 [-50.000, 156.000], mean action: 1.214 [0.000, 3.000],  loss: 3154.866943, mae: 657.227722, mean_q: -849.598511\n",
      " 14832/20000: episode: 76, duration: 0.670s, episode steps: 112, steps per second: 167, episode reward: -1182.000, mean reward: -10.554 [-50.000, 88.000], mean action: 1.634 [0.000, 3.000],  loss: 2896.452637, mae: 662.705933, mean_q: -858.107727\n",
      " 15020/20000: episode: 77, duration: 1.104s, episode steps: 188, steps per second: 170, episode reward: -2670.000, mean reward: -14.202 [-50.000, 144.000], mean action: 1.362 [0.000, 3.000],  loss: 3108.324463, mae: 659.220642, mean_q: -852.537048\n",
      " 15238/20000: episode: 78, duration: 1.293s, episode steps: 218, steps per second: 169, episode reward: -3426.000, mean reward: -15.716 [-50.000, 136.000], mean action: 1.248 [0.000, 3.000],  loss: 2961.056152, mae: 662.167236, mean_q: -855.884033\n",
      " 15328/20000: episode: 79, duration: 0.539s, episode steps:  90, steps per second: 167, episode reward: -1500.000, mean reward: -16.667 [-50.000, 36.000], mean action: 1.856 [0.000, 3.000],  loss: 2509.651611, mae: 668.587646, mean_q: -866.491516\n",
      " 15472/20000: episode: 80, duration: 0.856s, episode steps: 144, steps per second: 168, episode reward: -1908.000, mean reward: -13.250 [-50.000, 68.000], mean action: 1.292 [0.000, 3.000],  loss: 4042.365479, mae: 672.844971, mean_q: -867.528320\n",
      " 15611/20000: episode: 81, duration: 0.841s, episode steps: 139, steps per second: 165, episode reward: -2482.000, mean reward: -17.856 [-50.000, 68.000], mean action: 2.180 [0.000, 3.000],  loss: 4310.019043, mae: 671.010254, mean_q: -864.586609\n",
      " 15694/20000: episode: 82, duration: 0.509s, episode steps:  83, steps per second: 163, episode reward: -1058.000, mean reward: -12.747 [-50.000, 36.000], mean action: 0.940 [0.000, 3.000],  loss: 5077.407227, mae: 662.527954, mean_q: -853.951355\n",
      " 15850/20000: episode: 83, duration: 0.922s, episode steps: 156, steps per second: 169, episode reward: -1222.000, mean reward: -7.833 [-50.000, 84.000], mean action: 1.936 [0.000, 3.000],  loss: 3596.769287, mae: 651.486450, mean_q: -840.992493\n",
      " 16056/20000: episode: 84, duration: 1.216s, episode steps: 206, steps per second: 169, episode reward: -4238.000, mean reward: -20.573 [-50.000, 92.000], mean action: 1.340 [0.000, 3.000],  loss: 4726.483398, mae: 644.621521, mean_q: -828.536255\n",
      " 16203/20000: episode: 85, duration: 0.870s, episode steps: 147, steps per second: 169, episode reward: -2190.000, mean reward: -14.898 [-50.000, 76.000], mean action: 1.463 [0.000, 3.000],  loss: 3724.139404, mae: 645.547852, mean_q: -834.161621\n",
      " 16319/20000: episode: 86, duration: 0.690s, episode steps: 116, steps per second: 168, episode reward: -1014.000, mean reward: -8.741 [-50.000, 72.000], mean action: 1.284 [0.000, 3.000],  loss: 5935.597168, mae: 633.609619, mean_q: -814.197449\n",
      " 16410/20000: episode: 87, duration: 0.587s, episode steps:  91, steps per second: 155, episode reward: -740.000, mean reward: -8.132 [-50.000, 96.000], mean action: 2.066 [0.000, 3.000],  loss: 4593.676758, mae: 630.494446, mean_q: -811.189758\n",
      " 16546/20000: episode: 88, duration: 0.819s, episode steps: 136, steps per second: 166, episode reward: -1752.000, mean reward: -12.882 [-50.000, 72.000], mean action: 1.853 [0.000, 3.000],  loss: 4784.601074, mae: 622.333679, mean_q: -800.379028\n",
      " 16818/20000: episode: 89, duration: 1.620s, episode steps: 272, steps per second: 168, episode reward: -6476.000, mean reward: -23.809 [-50.000, 152.000], mean action: 1.518 [0.000, 3.000],  loss: 4429.899902, mae: 607.947754, mean_q: -781.738037\n",
      " 17025/20000: episode: 90, duration: 1.230s, episode steps: 207, steps per second: 168, episode reward: -3884.000, mean reward: -18.763 [-50.000, 128.000], mean action: 1.585 [0.000, 3.000],  loss: 4230.199219, mae: 614.242920, mean_q: -791.870728\n",
      " 17119/20000: episode: 91, duration: 0.568s, episode steps:  94, steps per second: 166, episode reward: -1770.000, mean reward: -18.830 [-50.000, 44.000], mean action: 1.904 [0.000, 3.000],  loss: 3587.961426, mae: 615.705444, mean_q: -791.883484\n",
      " 17263/20000: episode: 92, duration: 0.870s, episode steps: 144, steps per second: 166, episode reward: -332.000, mean reward: -2.306 [-50.000, 132.000], mean action: 0.826 [0.000, 3.000],  loss: 4967.126953, mae: 604.079834, mean_q: -776.608765\n",
      " 17379/20000: episode: 93, duration: 0.694s, episode steps: 116, steps per second: 167, episode reward: -1840.000, mean reward: -15.862 [-50.000, 40.000], mean action: 1.172 [0.000, 3.000],  loss: 3739.828613, mae: 603.527771, mean_q: -778.355957\n",
      " 17499/20000: episode: 94, duration: 0.716s, episode steps: 120, steps per second: 168, episode reward: -1194.000, mean reward: -9.950 [-50.000, 64.000], mean action: 1.275 [0.000, 3.000],  loss: 5068.415527, mae: 598.721863, mean_q: -770.048157\n",
      " 17726/20000: episode: 95, duration: 1.345s, episode steps: 227, steps per second: 169, episode reward: -3780.000, mean reward: -16.652 [-50.000, 136.000], mean action: 1.722 [0.000, 3.000],  loss: 4361.120117, mae: 601.957214, mean_q: -773.745300\n",
      " 17850/20000: episode: 96, duration: 0.741s, episode steps: 124, steps per second: 167, episode reward: -618.000, mean reward: -4.984 [-50.000, 64.000], mean action: 1.419 [0.000, 3.000],  loss: 5146.417480, mae: 604.070984, mean_q: -775.818359\n",
      " 17971/20000: episode: 97, duration: 0.738s, episode steps: 121, steps per second: 164, episode reward: -592.000, mean reward: -4.893 [-50.000, 68.000], mean action: 1.306 [0.000, 3.000],  loss: 5088.348633, mae: 600.320801, mean_q: -771.301758\n",
      " 18047/20000: episode: 98, duration: 0.455s, episode steps:  76, steps per second: 167, episode reward: -1040.000, mean reward: -13.684 [-50.000, 36.000], mean action: 1.197 [0.000, 3.000],  loss: 3953.703613, mae: 593.915649, mean_q: -765.602661\n",
      " 18270/20000: episode: 99, duration: 1.326s, episode steps: 223, steps per second: 168, episode reward: -5164.000, mean reward: -23.157 [-50.000, 84.000], mean action: 1.314 [0.000, 3.000],  loss: 4047.821777, mae: 588.857361, mean_q: -758.512329\n",
      " 18384/20000: episode: 100, duration: 0.678s, episode steps: 114, steps per second: 168, episode reward: -954.000, mean reward: -8.368 [-50.000, 80.000], mean action: 1.395 [0.000, 3.000],  loss: 4582.854492, mae: 593.612122, mean_q: -765.327515\n",
      " 18636/20000: episode: 101, duration: 1.517s, episode steps: 252, steps per second: 166, episode reward: -2934.000, mean reward: -11.643 [-50.000, 148.000], mean action: 1.865 [0.000, 3.000],  loss: 4301.843750, mae: 603.409973, mean_q: -777.700562\n",
      " 18715/20000: episode: 102, duration: 0.470s, episode steps:  79, steps per second: 168, episode reward: -594.000, mean reward: -7.519 [-50.000, 48.000], mean action: 1.646 [0.000, 3.000],  loss: 4691.651367, mae: 599.886841, mean_q: -769.137756\n",
      " 18852/20000: episode: 103, duration: 0.814s, episode steps: 137, steps per second: 168, episode reward: -2980.000, mean reward: -21.752 [-50.000, 44.000], mean action: 2.343 [0.000, 3.000],  loss: 3857.921875, mae: 584.979187, mean_q: -754.184448\n",
      " 18900/20000: episode: 104, duration: 0.287s, episode steps:  48, steps per second: 167, episode reward: -510.000, mean reward: -10.625 [-50.000, 16.000], mean action: 2.292 [0.000, 3.000],  loss: 3688.052002, mae: 590.918884, mean_q: -760.309082\n",
      " 19043/20000: episode: 105, duration: 0.848s, episode steps: 143, steps per second: 169, episode reward: -436.000, mean reward: -3.049 [-50.000, 64.000], mean action: 1.217 [0.000, 3.000],  loss: 5075.403809, mae: 580.035706, mean_q: -745.697449\n",
      " 19085/20000: episode: 106, duration: 0.253s, episode steps:  42, steps per second: 166, episode reward: -64.000, mean reward: -1.524 [-50.000, 32.000], mean action: 1.738 [0.000, 3.000],  loss: 3500.866455, mae: 575.074829, mean_q: -740.204529\n",
      " 19302/20000: episode: 107, duration: 1.285s, episode steps: 217, steps per second: 169, episode reward: -5204.000, mean reward: -23.982 [-50.000, 68.000], mean action: 1.309 [0.000, 3.000],  loss: 4282.338867, mae: 572.007263, mean_q: -736.438965\n",
      " 19562/20000: episode: 108, duration: 1.534s, episode steps: 260, steps per second: 170, episode reward: -4164.000, mean reward: -16.015 [-50.000, 140.000], mean action: 0.896 [0.000, 3.000],  loss: 4285.409668, mae: 577.225830, mean_q: -741.279236\n",
      " 19790/20000: episode: 109, duration: 1.370s, episode steps: 228, steps per second: 166, episode reward: -2960.000, mean reward: -12.982 [-50.000, 128.000], mean action: 1.588 [0.000, 3.000],  loss: 3755.496094, mae: 579.802002, mean_q: -747.431335\n",
      " 19936/20000: episode: 110, duration: 0.874s, episode steps: 146, steps per second: 167, episode reward: 320.000, mean reward:  2.192 [-50.000, 128.000], mean action: 1.438 [0.000, 3.000],  loss: 4138.065918, mae: 587.767761, mean_q: -756.153137\n",
      "done, took 117.252 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e56a4bb20>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "# hiddenFeatures = 32\n",
    "# activation=\"relu\"\n",
    "activation=\"leaky_relu\"\n",
    "\n",
    "# Higher temporal window confuses the model\n",
    "# temporalWindow = 5\n",
    "temporalWindow = 1\n",
    "\n",
    "\n",
    "\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "\n",
    "# Result look alike\n",
    "model.add(Activation(activation=activation))\n",
    "# model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "\n",
    "# Worst result\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "\n",
    "# Additional layers\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=activation))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=activation))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Less invalid moves with more limit\n",
    "replayMemory = SequentialMemory(limit=2000, window_length=temporalWindow)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=1000,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=20000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5981/802290428.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  transformed = np.where(grid > 0, np.log2(grid), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 is over.\n",
      "Game 2 is over.\n",
      "Game 3 is over.\n",
      "Game 4 is over.\n",
      "Game 5 is over.\n",
      "Game 6 is over.\n",
      "Game 7 is over.\n",
      "Game 8 is over.\n",
      "Game 9 is over.\n",
      "Game 10 is over.\n",
      "Game 11 is over.\n",
      "Game 12 is over.\n",
      "Game 13 is over.\n",
      "Game 14 is over.\n",
      "Game 15 is over.\n",
      "Game 16 is over.\n",
      "Game 17 is over.\n",
      "Game 18 is over.\n",
      "Game 19 is over.\n",
      "Game 20 is over.\n",
      "Game 21 is over.\n",
      "Game 22 is over.\n",
      "Game 23 is over.\n",
      "Game 24 is over.\n",
      "Game 25 is over.\n",
      "Game 26 is over.\n",
      "Game 27 is over.\n",
      "Game 28 is over.\n",
      "Game 29 is over.\n",
      "Game 30 is over.\n",
      "Game 31 is over.\n",
      "Game 32 is over.\n",
      "Game 33 is over.\n",
      "Game 34 is over.\n",
      "Game 35 is over.\n",
      "Game 36 is over.\n",
      "Game 37 is over.\n",
      "Game 38 is over.\n",
      "Game 39 is over.\n",
      "Game 40 is over.\n",
      "Game 41 is over.\n",
      "Game 42 is over.\n",
      "Game 43 is over.\n",
      "Game 44 is over.\n",
      "Game 45 is over.\n",
      "Game 46 is over.\n",
      "Game 47 is over.\n",
      "Game 48 is over.\n",
      "Game 49 is over.\n",
      "Game 50 is over.\n",
      "Game 51 is over.\n",
      "Game 52 is over.\n",
      "Game 53 is over.\n",
      "Game 54 is over.\n",
      "Game 55 is over.\n",
      "Game 56 is over.\n",
      "Game 57 is over.\n",
      "Game 58 is over.\n",
      "Game 59 is over.\n",
      "Game 60 is over.\n",
      "Game 61 is over.\n",
      "Game 62 is over.\n",
      "Game 63 is over.\n",
      "Game 64 is over.\n",
      "Game 65 is over.\n",
      "Game 66 is over.\n",
      "Game 67 is over.\n",
      "Game 68 is over.\n",
      "Game 69 is over.\n",
      "Game 70 is over.\n",
      "Game 71 is over.\n",
      "Game 72 is over.\n",
      "Game 73 is over.\n",
      "Game 74 is over.\n",
      "Game 75 is over.\n",
      "Game 76 is over.\n",
      "Game 77 is over.\n",
      "Game 78 is over.\n",
      "Game 79 is over.\n",
      "Game 80 is over.\n",
      "Game 81 is over.\n",
      "Game 82 is over.\n",
      "Game 83 is over.\n",
      "Game 84 is over.\n",
      "Game 85 is over.\n",
      "Game 86 is over.\n",
      "Game 87 is over.\n",
      "Game 88 is over.\n",
      "Game 89 is over.\n",
      "Game 90 is over.\n",
      "Game 91 is over.\n",
      "Game 92 is over.\n",
      "Game 93 is over.\n",
      "Game 94 is over.\n",
      "Game 95 is over.\n",
      "Game 96 is over.\n",
      "Game 97 is over.\n",
      "Game 98 is over.\n",
      "Game 99 is over.\n",
      "Game 100 is over.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gameId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1432</td>\n",
       "      <td>128</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>275</td>\n",
       "      <td>91</td>\n",
       "      <td>94</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.764706</td>\n",
       "      <td>1.594595</td>\n",
       "      <td>1.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>996</td>\n",
       "      <td>64</td>\n",
       "      <td>0.514403</td>\n",
       "      <td>243</td>\n",
       "      <td>30</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>3.296296</td>\n",
       "      <td>2.638889</td>\n",
       "      <td>1.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>628</td>\n",
       "      <td>64</td>\n",
       "      <td>0.350746</td>\n",
       "      <td>134</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.764706</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>716</td>\n",
       "      <td>64</td>\n",
       "      <td>0.631356</td>\n",
       "      <td>236</td>\n",
       "      <td>18</td>\n",
       "      <td>145</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>6.904762</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>145</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>4.473684</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>141</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>1.129032</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>700</td>\n",
       "      <td>64</td>\n",
       "      <td>0.384106</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.214286</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>784</td>\n",
       "      <td>64</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>220</td>\n",
       "      <td>27</td>\n",
       "      <td>138</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>1.103448</td>\n",
       "      <td>1.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1160</td>\n",
       "      <td>128</td>\n",
       "      <td>0.580537</td>\n",
       "      <td>298</td>\n",
       "      <td>33</td>\n",
       "      <td>201</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>5.153846</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>688</td>\n",
       "      <td>64</td>\n",
       "      <td>0.606695</td>\n",
       "      <td>239</td>\n",
       "      <td>27</td>\n",
       "      <td>172</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.526316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  higherTile  invalidMove (%)  steps  upMoves  downMoves  \\\n",
       "gameId                                                                  \n",
       "0        1432         128         0.461818    275       91         94   \n",
       "1         996          64         0.514403    243       30         89   \n",
       "2         628          64         0.350746    134       17         64   \n",
       "3         716          64         0.631356    236       18        145   \n",
       "4         512          64         0.489655    145       17         85   \n",
       "...       ...         ...              ...    ...      ...        ...   \n",
       "95       1000         128         0.226950    141       24         62   \n",
       "96        700          64         0.384106    151       11         73   \n",
       "97        784          64         0.550000    220       27        138   \n",
       "98       1160         128         0.580537    298       33        201   \n",
       "99        688          64         0.606695    239       27        172   \n",
       "\n",
       "        leftMoves  rightMoves  upSeqAvg  downSeqAvg  leftSeqAvg  rightSeqAvg  \n",
       "gameId                                                                        \n",
       "0              59          31  2.333333    2.764706    1.594595     1.068966  \n",
       "1              95          29  1.034483    3.296296    2.638889     1.035714  \n",
       "2              29          24  1.000000    3.764706    1.074074     1.000000  \n",
       "3              50          23  1.058824    6.904762    1.785714     1.000000  \n",
       "4              23          20  1.062500    4.473684    1.045455     1.000000  \n",
       "...           ...         ...       ...         ...         ...          ...  \n",
       "95             35          20  1.000000    2.066667    1.129032     1.000000  \n",
       "96             37          30  1.000000    5.214286    1.057143     1.071429  \n",
       "97             32          23  1.000000    5.111111    1.103448     1.045455  \n",
       "98             36          28  1.100000    5.153846    1.058824     1.000000  \n",
       "99             23          17  1.000000    4.526316    1.000000     1.062500  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>674.320000</td>\n",
       "      <td>67.520000</td>\n",
       "      <td>0.353450</td>\n",
       "      <td>155.690000</td>\n",
       "      <td>28.910000</td>\n",
       "      <td>69.050000</td>\n",
       "      <td>36.530000</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>1.343617</td>\n",
       "      <td>2.928931</td>\n",
       "      <td>1.443094</td>\n",
       "      <td>1.047233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>305.846165</td>\n",
       "      <td>30.728686</td>\n",
       "      <td>0.199640</td>\n",
       "      <td>80.968343</td>\n",
       "      <td>24.533257</td>\n",
       "      <td>57.679255</td>\n",
       "      <td>25.404388</td>\n",
       "      <td>9.772152</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>1.753957</td>\n",
       "      <td>0.722043</td>\n",
       "      <td>0.167164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>196.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>511.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.238378</td>\n",
       "      <td>96.750000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>1.032576</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>630.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.369817</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>2.527536</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>758.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>203.250000</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>1.157143</td>\n",
       "      <td>3.811538</td>\n",
       "      <td>1.563122</td>\n",
       "      <td>1.045996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1780.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>4.781250</td>\n",
       "      <td>8.026316</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score  higherTile  invalidMove (%)       steps     upMoves  \\\n",
       "count   100.000000  100.000000       100.000000  100.000000  100.000000   \n",
       "mean    674.320000   67.520000         0.353450  155.690000   28.910000   \n",
       "std     305.846165   30.728686         0.199640   80.968343   24.533257   \n",
       "min     196.000000   16.000000         0.000000   45.000000    8.000000   \n",
       "25%     511.000000   64.000000         0.238378   96.750000   16.000000   \n",
       "50%     630.000000   64.000000         0.369817  132.000000   20.000000   \n",
       "75%     758.000000   64.000000         0.499300  203.250000   30.250000   \n",
       "max    1780.000000  128.000000         0.754717  373.000000  153.000000   \n",
       "\n",
       "        downMoves   leftMoves  rightMoves    upSeqAvg  downSeqAvg  leftSeqAvg  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean    69.050000   36.530000   21.200000    1.343617    2.928931    1.443094   \n",
       "std     57.679255   25.404388    9.772152    0.709016    1.753957    0.722043   \n",
       "min      9.000000    8.000000    7.000000    1.000000    1.000000    1.000000   \n",
       "25%     29.750000   21.000000   16.750000    1.000000    1.433333    1.032576   \n",
       "50%     50.000000   28.500000   20.000000    1.055556    2.527536    1.090909   \n",
       "75%     89.000000   38.250000   24.250000    1.157143    3.811538    1.563122   \n",
       "max    305.000000  138.000000   94.000000    4.781250    8.026316    4.125000   \n",
       "\n",
       "       rightSeqAvg  \n",
       "count   100.000000  \n",
       "mean      1.047233  \n",
       "std       0.167164  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.045996  \n",
       "max       2.611111  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "def log2_transform(grid):\n",
    "    transformed = np.where(grid > 0, np.log2(grid), 0)\n",
    "    return transformed\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(100)]\n",
    "metrics = {\n",
    "    \"gameId\": [],\n",
    "    \"score\": [],\n",
    "    \"higherTile\": [],\n",
    "    \"invalidMove (%)\": [],\n",
    "    \"steps\": [],\n",
    "    \"upMoves\": [],\n",
    "    \"downMoves\": [],\n",
    "    \"leftMoves\": [],\n",
    "    \"rightMoves\": [],\n",
    "    \"upSeqAvg\": [],\n",
    "    \"downSeqAvg\": [],\n",
    "    \"leftSeqAvg\": [],\n",
    "    \"rightSeqAvg\": [],\n",
    "}\n",
    "for i, game in enumerate(games):\n",
    "    # Metrics\n",
    "    steps = 0\n",
    "    invalidMove = 0\n",
    "    upMoves = 0\n",
    "    downMoves = 0\n",
    "    leftMoves = 0\n",
    "    rightMoves = 0\n",
    "    upSeqAvg = 0\n",
    "    downSeqAvg = 0\n",
    "    leftSeqAvg = 0\n",
    "    rightSeqAvg = 0\n",
    "\n",
    "    sequences = {\n",
    "        \"up\": [],\n",
    "        \"down\": [],\n",
    "        \"left\": [],\n",
    "        \"right\": []\n",
    "    }\n",
    "    lastMove = None\n",
    "\n",
    "    # print(f\"Game {i + 1} initial state:\")\n",
    "    # game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        observation = log2_transform(observation)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        # print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        if not game.move(actionMapping[predictedAction]):\n",
    "            invalidMove += 1\n",
    "        \n",
    "        # print(f\"Game {i + 1} currrent state:\")\n",
    "        # game.render()\n",
    "\n",
    "        # input(\"\\nPress any key to continue...\")\n",
    "\n",
    "        steps += 1\n",
    "        if actionMapping[predictedAction] == \"up\":\n",
    "            upMoves += 1\n",
    "            if lastMove == \"up\":\n",
    "                sequences[\"up\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"up\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"down\":\n",
    "            downMoves += 1\n",
    "            if lastMove == \"down\":\n",
    "                sequences[\"down\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"down\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"left\":\n",
    "            leftMoves += 1\n",
    "            if lastMove == \"left\":\n",
    "                sequences[\"left\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"left\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"right\":\n",
    "            rightMoves += 1\n",
    "            if lastMove == \"right\":\n",
    "                sequences[\"right\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"right\"].append(1)\n",
    "        \n",
    "        lastMove = actionMapping[predictedAction]\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "    metrics[\"gameId\"].append(i)\n",
    "    metrics[\"score\"].append(game.score)\n",
    "    metrics[\"higherTile\"].append(np.max(game.grid))\n",
    "    metrics[\"invalidMove (%)\"].append(invalidMove/steps)\n",
    "    metrics[\"steps\"].append(steps)\n",
    "    metrics[\"upMoves\"].append(upMoves)\n",
    "    metrics[\"downMoves\"].append(downMoves)\n",
    "    metrics[\"leftMoves\"].append(leftMoves)\n",
    "    metrics[\"rightMoves\"].append(rightMoves)\n",
    "    metrics[\"upSeqAvg\"].append(np.mean(np.array(sequences[\"up\"])))\n",
    "    metrics[\"downSeqAvg\"].append(np.mean(np.array(sequences[\"down\"])))\n",
    "    metrics[\"leftSeqAvg\"].append(np.mean(np.array(sequences[\"left\"])))\n",
    "    metrics[\"rightSeqAvg\"].append(np.mean(np.array(sequences[\"right\"])))\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics).set_index(\"gameId\")\n",
    "display(metrics)\n",
    "display(metrics.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [\"up\", \"left\", \"right\", \"down\"]\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
