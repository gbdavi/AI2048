{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 14:17:43.510004: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-23 14:17:43.510043: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2025-03-23 14:18:02.648091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-03-23 14:18:02.648136: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-23 14:18:02.648152: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a7476f3fb4ff): /proc/driver/nvidia/version does not exist\n",
      "2025-03-23 14:18:02.648357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-23 14:18:02.655218: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   445/20000: episode: 1, duration: 0.301s, episode steps: 445, steps per second: 1477, episode reward: -16184.000, mean reward: -36.369 [-50.000, 68.000], mean action: 1.939 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "   723/20000: episode: 2, duration: 0.170s, episode steps: 278, steps per second: 1634, episode reward: -10238.000, mean reward: -36.827 [-50.000, 32.000], mean action: 1.899 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1050/20000: episode: 3, duration: 0.661s, episode steps: 327, steps per second: 495, episode reward: -10768.000, mean reward: -32.930 [-50.000, 68.000], mean action: 1.783 [0.000, 3.000],  loss: 896.437793, mae: 11.182580, mean_q: 1.850282\n",
      "  1629/20000: episode: 4, duration: 2.245s, episode steps: 579, steps per second: 258, episode reward: -19018.000, mean reward: -32.846 [-50.000, 168.000], mean action: 1.304 [0.000, 3.000],  loss: 483.147461, mae: 18.585255, mean_q: -12.395197\n",
      "  1790/20000: episode: 5, duration: 0.625s, episode steps: 161, steps per second: 258, episode reward: -4408.000, mean reward: -27.379 [-50.000, 36.000], mean action: 2.342 [0.000, 3.000],  loss: 372.735413, mae: 47.114159, mean_q: -51.118969\n",
      "  2254/20000: episode: 6, duration: 1.773s, episode steps: 464, steps per second: 262, episode reward: -16390.000, mean reward: -35.323 [-50.000, 128.000], mean action: 1.472 [0.000, 3.000],  loss: 382.070038, mae: 80.823059, mean_q: -95.400055\n",
      "  2576/20000: episode: 7, duration: 1.223s, episode steps: 322, steps per second: 263, episode reward: -7932.000, mean reward: -24.634 [-50.000, 136.000], mean action: 1.975 [0.000, 3.000],  loss: 446.293091, mae: 128.155685, mean_q: -157.393585\n",
      "  2791/20000: episode: 8, duration: 0.825s, episode steps: 215, steps per second: 261, episode reward: -4048.000, mean reward: -18.828 [-50.000, 76.000], mean action: 1.274 [0.000, 3.000],  loss: 488.115875, mae: 166.055222, mean_q: -207.371964\n",
      "  2852/20000: episode: 9, duration: 0.237s, episode steps:  61, steps per second: 258, episode reward: 316.000, mean reward:  5.180 [-50.000, 80.000], mean action: 2.016 [0.000, 3.000],  loss: 523.392273, mae: 186.524918, mean_q: -234.367157\n",
      "  3085/20000: episode: 10, duration: 0.885s, episode steps: 233, steps per second: 263, episode reward: -5358.000, mean reward: -22.996 [-50.000, 84.000], mean action: 1.820 [0.000, 3.000],  loss: 646.032043, mae: 203.641296, mean_q: -255.759476\n",
      "  3272/20000: episode: 11, duration: 0.715s, episode steps: 187, steps per second: 262, episode reward: -4898.000, mean reward: -26.193 [-50.000, 64.000], mean action: 1.267 [0.000, 3.000],  loss: 763.519714, mae: 233.641663, mean_q: -295.268372\n",
      "  3408/20000: episode: 12, duration: 0.519s, episode steps: 136, steps per second: 262, episode reward: -1640.000, mean reward: -12.059 [-50.000, 72.000], mean action: 1.118 [0.000, 3.000],  loss: 679.158081, mae: 250.452240, mean_q: -317.786255\n",
      "  3729/20000: episode: 13, duration: 1.220s, episode steps: 321, steps per second: 263, episode reward: -9244.000, mean reward: -28.798 [-50.000, 144.000], mean action: 1.106 [0.000, 3.000],  loss: 842.636475, mae: 265.292877, mean_q: -336.750366\n",
      "  3803/20000: episode: 14, duration: 0.288s, episode steps:  74, steps per second: 257, episode reward: -716.000, mean reward: -9.676 [-50.000, 36.000], mean action: 1.486 [0.000, 3.000],  loss: 818.891052, mae: 289.305695, mean_q: -369.412781\n",
      "  3886/20000: episode: 15, duration: 0.328s, episode steps:  83, steps per second: 253, episode reward: 372.000, mean reward:  4.482 [-50.000, 76.000], mean action: 1.687 [0.000, 3.000],  loss: 956.418396, mae: 299.472198, mean_q: -382.392944\n",
      "  3984/20000: episode: 16, duration: 0.410s, episode steps:  98, steps per second: 239, episode reward: -634.000, mean reward: -6.469 [-50.000, 84.000], mean action: 1.082 [0.000, 3.000],  loss: 887.600037, mae: 306.575043, mean_q: -391.936310\n",
      "  4137/20000: episode: 17, duration: 0.586s, episode steps: 153, steps per second: 261, episode reward: -3830.000, mean reward: -25.033 [-50.000, 48.000], mean action: 2.392 [0.000, 3.000],  loss: 1191.602783, mae: 310.943665, mean_q: -396.442902\n",
      "  4485/20000: episode: 18, duration: 1.331s, episode steps: 348, steps per second: 261, episode reward: -7340.000, mean reward: -21.092 [-50.000, 144.000], mean action: 1.799 [0.000, 3.000],  loss: 1347.072144, mae: 326.400177, mean_q: -415.541229\n",
      "  4772/20000: episode: 19, duration: 1.102s, episode steps: 287, steps per second: 260, episode reward: -8898.000, mean reward: -31.003 [-50.000, 72.000], mean action: 1.962 [0.000, 3.000],  loss: 1686.653076, mae: 350.225891, mean_q: -446.937134\n",
      "  4884/20000: episode: 20, duration: 0.440s, episode steps: 112, steps per second: 255, episode reward: -740.000, mean reward: -6.607 [-50.000, 112.000], mean action: 0.991 [0.000, 3.000],  loss: 1253.505249, mae: 370.609711, mean_q: -475.669769\n",
      "  4968/20000: episode: 21, duration: 0.329s, episode steps:  84, steps per second: 255, episode reward: 446.000, mean reward:  5.310 [-50.000, 68.000], mean action: 0.774 [0.000, 3.000],  loss: 1432.062500, mae: 384.365387, mean_q: -492.619781\n",
      "  5507/20000: episode: 22, duration: 2.043s, episode steps: 539, steps per second: 264, episode reward: -17018.000, mean reward: -31.573 [-50.000, 156.000], mean action: 1.004 [0.000, 3.000],  loss: 1684.542847, mae: 400.125000, mean_q: -512.823853\n",
      "  5615/20000: episode: 23, duration: 0.415s, episode steps: 108, steps per second: 260, episode reward: -1122.000, mean reward: -10.389 [-50.000, 64.000], mean action: 1.796 [0.000, 3.000],  loss: 1561.233032, mae: 436.010803, mean_q: -560.912048\n",
      "  5782/20000: episode: 24, duration: 0.638s, episode steps: 167, steps per second: 262, episode reward: -4654.000, mean reward: -27.868 [-50.000, 40.000], mean action: 1.156 [0.000, 3.000],  loss: 1547.107178, mae: 440.603210, mean_q: -567.175232\n",
      "  5905/20000: episode: 25, duration: 0.483s, episode steps: 123, steps per second: 255, episode reward: -558.000, mean reward: -4.537 [-50.000, 84.000], mean action: 1.390 [0.000, 3.000],  loss: 1817.861084, mae: 454.025360, mean_q: -584.555359\n",
      "  6211/20000: episode: 26, duration: 1.159s, episode steps: 306, steps per second: 264, episode reward: -7752.000, mean reward: -25.333 [-50.000, 164.000], mean action: 0.706 [0.000, 3.000],  loss: 1842.473633, mae: 474.586548, mean_q: -611.932251\n",
      "  6345/20000: episode: 27, duration: 0.519s, episode steps: 134, steps per second: 258, episode reward: -1002.000, mean reward: -7.478 [-50.000, 80.000], mean action: 1.948 [0.000, 3.000],  loss: 2437.969971, mae: 489.634918, mean_q: -629.428406\n",
      "  6658/20000: episode: 28, duration: 1.213s, episode steps: 313, steps per second: 258, episode reward: -8586.000, mean reward: -27.431 [-50.000, 72.000], mean action: 1.137 [0.000, 3.000],  loss: 2156.965576, mae: 494.202362, mean_q: -637.225708\n",
      "  6748/20000: episode: 29, duration: 0.350s, episode steps:  90, steps per second: 257, episode reward: -152.000, mean reward: -1.689 [-50.000, 64.000], mean action: 1.733 [0.000, 3.000],  loss: 2186.386230, mae: 501.792877, mean_q: -647.096069\n",
      "  6989/20000: episode: 30, duration: 0.953s, episode steps: 241, steps per second: 253, episode reward: -3250.000, mean reward: -13.485 [-50.000, 132.000], mean action: 2.357 [0.000, 3.000],  loss: 2212.209473, mae: 503.070465, mean_q: -647.171082\n",
      "  7226/20000: episode: 31, duration: 1.007s, episode steps: 237, steps per second: 235, episode reward: -4156.000, mean reward: -17.536 [-50.000, 164.000], mean action: 2.127 [0.000, 3.000],  loss: 2396.806152, mae: 520.006348, mean_q: -670.016418\n",
      "  7380/20000: episode: 32, duration: 0.608s, episode steps: 154, steps per second: 253, episode reward: -3158.000, mean reward: -20.506 [-50.000, 64.000], mean action: 1.948 [0.000, 3.000],  loss: 2308.915771, mae: 517.694214, mean_q: -665.870117\n",
      "  7562/20000: episode: 33, duration: 0.713s, episode steps: 182, steps per second: 255, episode reward: -5856.000, mean reward: -32.176 [-50.000, 32.000], mean action: 1.852 [0.000, 3.000],  loss: 2743.877441, mae: 508.042053, mean_q: -652.189575\n",
      "  7883/20000: episode: 34, duration: 1.218s, episode steps: 321, steps per second: 264, episode reward: -9046.000, mean reward: -28.181 [-50.000, 164.000], mean action: 0.636 [0.000, 3.000],  loss: 2539.482422, mae: 517.553223, mean_q: -664.290466\n",
      "  8130/20000: episode: 35, duration: 0.953s, episode steps: 247, steps per second: 259, episode reward: -6784.000, mean reward: -27.466 [-50.000, 80.000], mean action: 1.101 [0.000, 3.000],  loss: 2530.433594, mae: 537.601196, mean_q: -691.991699\n",
      "  8230/20000: episode: 36, duration: 0.421s, episode steps: 100, steps per second: 237, episode reward: -184.000, mean reward: -1.840 [-50.000, 112.000], mean action: 1.450 [0.000, 3.000],  loss: 1997.478149, mae: 547.324097, mean_q: -706.567444\n",
      "  8334/20000: episode: 37, duration: 0.458s, episode steps: 104, steps per second: 227, episode reward: -1376.000, mean reward: -13.231 [-50.000, 36.000], mean action: 1.038 [0.000, 3.000],  loss: 3409.465820, mae: 549.464905, mean_q: -707.572815\n",
      "  8399/20000: episode: 38, duration: 0.288s, episode steps:  65, steps per second: 226, episode reward: 388.000, mean reward:  5.969 [ 0.000, 48.000], mean action: 1.723 [0.000, 3.000],  loss: 2315.579346, mae: 553.798828, mean_q: -714.396423\n",
      "  8595/20000: episode: 39, duration: 0.834s, episode steps: 196, steps per second: 235, episode reward: -2210.000, mean reward: -11.276 [-50.000, 132.000], mean action: 2.026 [0.000, 3.000],  loss: 2924.022217, mae: 552.468384, mean_q: -710.980530\n",
      "  8734/20000: episode: 40, duration: 0.560s, episode steps: 139, steps per second: 248, episode reward: -140.000, mean reward: -1.007 [-50.000, 132.000], mean action: 1.799 [0.000, 3.000],  loss: 2615.121826, mae: 560.208862, mean_q: -720.553650\n",
      "  8856/20000: episode: 41, duration: 0.487s, episode steps: 122, steps per second: 250, episode reward: 904.000, mean reward:  7.410 [-50.000, 144.000], mean action: 1.820 [0.000, 3.000],  loss: 3285.166260, mae: 565.267395, mean_q: -726.896729\n",
      "  9060/20000: episode: 42, duration: 0.814s, episode steps: 204, steps per second: 251, episode reward: -5568.000, mean reward: -27.294 [-50.000, 40.000], mean action: 1.549 [0.000, 3.000],  loss: 4257.071289, mae: 563.844666, mean_q: -721.731934\n",
      "  9128/20000: episode: 43, duration: 0.274s, episode steps:  68, steps per second: 248, episode reward: -188.000, mean reward: -2.765 [-50.000, 52.000], mean action: 2.015 [0.000, 3.000],  loss: 2702.083008, mae: 565.106995, mean_q: -726.657715\n",
      "  9231/20000: episode: 44, duration: 0.417s, episode steps: 103, steps per second: 247, episode reward: 500.000, mean reward:  4.854 [-50.000, 84.000], mean action: 1.893 [0.000, 3.000],  loss: 4268.203125, mae: 561.124268, mean_q: -718.569946\n",
      "  9312/20000: episode: 45, duration: 0.329s, episode steps:  81, steps per second: 246, episode reward: -1228.000, mean reward: -15.160 [-50.000, 52.000], mean action: 1.667 [0.000, 3.000],  loss: 4879.058594, mae: 558.294800, mean_q: -715.352356\n",
      "  9479/20000: episode: 46, duration: 0.684s, episode steps: 167, steps per second: 244, episode reward: -4546.000, mean reward: -27.222 [-50.000, 44.000], mean action: 2.084 [0.000, 3.000],  loss: 4125.736328, mae: 553.773010, mean_q: -709.623718\n",
      "  9613/20000: episode: 47, duration: 0.537s, episode steps: 134, steps per second: 250, episode reward: -2896.000, mean reward: -21.612 [-50.000, 40.000], mean action: 1.754 [0.000, 3.000],  loss: 4345.939453, mae: 559.820312, mean_q: -716.210571\n",
      "  9709/20000: episode: 48, duration: 0.403s, episode steps:  96, steps per second: 238, episode reward: 614.000, mean reward:  6.396 [-50.000, 68.000], mean action: 1.635 [0.000, 3.000],  loss: 4544.539551, mae: 554.883118, mean_q: -710.129944\n",
      "  9857/20000: episode: 49, duration: 0.589s, episode steps: 148, steps per second: 251, episode reward: -1656.000, mean reward: -11.189 [-50.000, 64.000], mean action: 1.230 [0.000, 3.000],  loss: 4080.457031, mae: 538.710754, mean_q: -688.397278\n",
      "  9975/20000: episode: 50, duration: 0.488s, episode steps: 118, steps per second: 242, episode reward: -986.000, mean reward: -8.356 [-50.000, 72.000], mean action: 1.042 [0.000, 3.000],  loss: 4616.235352, mae: 544.011780, mean_q: -695.393250\n",
      " 10044/20000: episode: 51, duration: 0.283s, episode steps:  69, steps per second: 244, episode reward: 420.000, mean reward:  6.087 [ 0.000, 56.000], mean action: 1.681 [0.000, 3.000],  loss: 4561.296875, mae: 529.238708, mean_q: -675.714905\n",
      " 10300/20000: episode: 52, duration: 0.984s, episode steps: 256, steps per second: 260, episode reward: -6280.000, mean reward: -24.531 [-50.000, 148.000], mean action: 1.211 [0.000, 3.000],  loss: 4363.578613, mae: 518.838257, mean_q: -659.988525\n",
      " 10409/20000: episode: 53, duration: 0.425s, episode steps: 109, steps per second: 256, episode reward: -1202.000, mean reward: -11.028 [-50.000, 68.000], mean action: 1.018 [0.000, 3.000],  loss: 3215.250488, mae: 524.626892, mean_q: -672.301025\n",
      " 10524/20000: episode: 54, duration: 0.468s, episode steps: 115, steps per second: 245, episode reward: -508.000, mean reward: -4.417 [-50.000, 76.000], mean action: 1.287 [0.000, 3.000],  loss: 4349.446289, mae: 537.213745, mean_q: -688.121338\n",
      " 10641/20000: episode: 55, duration: 0.458s, episode steps: 117, steps per second: 256, episode reward: -1654.000, mean reward: -14.137 [-50.000, 48.000], mean action: 1.402 [0.000, 3.000],  loss: 4821.588379, mae: 529.634338, mean_q: -677.382080\n",
      " 10763/20000: episode: 56, duration: 0.472s, episode steps: 122, steps per second: 258, episode reward: -584.000, mean reward: -4.787 [-50.000, 116.000], mean action: 1.730 [0.000, 3.000],  loss: 4311.625977, mae: 524.017029, mean_q: -669.351807\n",
      " 10941/20000: episode: 57, duration: 0.676s, episode steps: 178, steps per second: 263, episode reward: -3470.000, mean reward: -19.494 [-50.000, 68.000], mean action: 1.118 [0.000, 3.000],  loss: 3965.147217, mae: 530.635315, mean_q: -679.808838\n",
      " 11101/20000: episode: 58, duration: 0.617s, episode steps: 160, steps per second: 260, episode reward: -3292.000, mean reward: -20.575 [-50.000, 68.000], mean action: 1.750 [0.000, 3.000],  loss: 4329.817383, mae: 526.032227, mean_q: -673.214539\n",
      " 11268/20000: episode: 59, duration: 0.651s, episode steps: 167, steps per second: 257, episode reward: -2234.000, mean reward: -13.377 [-50.000, 72.000], mean action: 1.551 [0.000, 3.000],  loss: 2850.420898, mae: 530.303162, mean_q: -680.695251\n",
      " 11384/20000: episode: 60, duration: 0.442s, episode steps: 116, steps per second: 263, episode reward: -1128.000, mean reward: -9.724 [-50.000, 76.000], mean action: 1.983 [0.000, 3.000],  loss: 3738.267334, mae: 541.946777, mean_q: -693.435669\n",
      " 11562/20000: episode: 61, duration: 0.725s, episode steps: 178, steps per second: 245, episode reward: -68.000, mean reward: -0.382 [-50.000, 128.000], mean action: 1.354 [0.000, 3.000],  loss: 3482.664062, mae: 538.685242, mean_q: -690.048889\n",
      " 11854/20000: episode: 62, duration: 1.116s, episode steps: 292, steps per second: 262, episode reward: -9498.000, mean reward: -32.527 [-50.000, 68.000], mean action: 2.175 [0.000, 3.000],  loss: 3964.250000, mae: 541.402466, mean_q: -690.784546\n",
      " 12024/20000: episode: 63, duration: 0.666s, episode steps: 170, steps per second: 255, episode reward: -3758.000, mean reward: -22.106 [-50.000, 76.000], mean action: 1.765 [0.000, 3.000],  loss: 2725.880615, mae: 553.973450, mean_q: -711.373352\n",
      " 12145/20000: episode: 64, duration: 0.474s, episode steps: 121, steps per second: 255, episode reward: 788.000, mean reward:  6.512 [-50.000, 164.000], mean action: 1.694 [0.000, 3.000],  loss: 2700.330078, mae: 556.876831, mean_q: -715.241760\n",
      " 12332/20000: episode: 65, duration: 0.716s, episode steps: 187, steps per second: 261, episode reward: -4422.000, mean reward: -23.647 [-50.000, 68.000], mean action: 1.299 [0.000, 3.000],  loss: 3452.962158, mae: 557.494690, mean_q: -714.342224\n",
      " 12438/20000: episode: 66, duration: 0.411s, episode steps: 106, steps per second: 258, episode reward: -608.000, mean reward: -5.736 [-50.000, 64.000], mean action: 0.991 [0.000, 3.000],  loss: 4024.128906, mae: 558.236450, mean_q: -714.557983\n",
      " 12513/20000: episode: 67, duration: 0.293s, episode steps:  75, steps per second: 256, episode reward: 172.000, mean reward:  2.293 [-50.000, 80.000], mean action: 1.253 [0.000, 3.000],  loss: 4118.295898, mae: 547.024231, mean_q: -698.507141\n",
      " 12840/20000: episode: 68, duration: 1.261s, episode steps: 327, steps per second: 259, episode reward: -9726.000, mean reward: -29.743 [-50.000, 76.000], mean action: 1.309 [0.000, 3.000],  loss: 3610.609375, mae: 556.642090, mean_q: -712.234985\n",
      " 12936/20000: episode: 69, duration: 0.372s, episode steps:  96, steps per second: 258, episode reward: -506.000, mean reward: -5.271 [-50.000, 64.000], mean action: 1.948 [0.000, 3.000],  loss: 3998.395264, mae: 569.341125, mean_q: -731.265137\n",
      " 13178/20000: episode: 70, duration: 0.962s, episode steps: 242, steps per second: 252, episode reward: -5700.000, mean reward: -23.554 [-50.000, 80.000], mean action: 2.066 [0.000, 3.000],  loss: 2945.942627, mae: 571.215149, mean_q: -733.751831\n",
      " 13213/20000: episode: 71, duration: 0.138s, episode steps:  35, steps per second: 253, episode reward: 66.000, mean reward:  1.886 [-50.000, 28.000], mean action: 1.257 [0.000, 3.000],  loss: 2331.884766, mae: 577.261597, mean_q: -742.114929\n",
      " 13333/20000: episode: 72, duration: 0.467s, episode steps: 120, steps per second: 257, episode reward: -2170.000, mean reward: -18.083 [-50.000, 68.000], mean action: 1.067 [0.000, 3.000],  loss: 2579.212402, mae: 570.743408, mean_q: -731.740234\n",
      " 13456/20000: episode: 73, duration: 0.479s, episode steps: 123, steps per second: 257, episode reward: -2152.000, mean reward: -17.496 [-50.000, 52.000], mean action: 1.772 [0.000, 3.000],  loss: 3724.329834, mae: 574.695129, mean_q: -736.730591\n",
      " 13520/20000: episode: 74, duration: 0.252s, episode steps:  64, steps per second: 254, episode reward: 260.000, mean reward:  4.062 [-50.000, 48.000], mean action: 1.812 [0.000, 3.000],  loss: 2873.591553, mae: 572.829468, mean_q: -735.400024\n",
      " 13607/20000: episode: 75, duration: 0.338s, episode steps:  87, steps per second: 258, episode reward: 602.000, mean reward:  6.920 [-50.000, 68.000], mean action: 1.483 [0.000, 3.000],  loss: 3293.828613, mae: 571.083801, mean_q: -734.045898\n",
      " 13855/20000: episode: 76, duration: 0.947s, episode steps: 248, steps per second: 262, episode reward: -5330.000, mean reward: -21.492 [-50.000, 72.000], mean action: 1.089 [0.000, 3.000],  loss: 3514.385498, mae: 564.423035, mean_q: -723.612976\n",
      " 13972/20000: episode: 77, duration: 0.453s, episode steps: 117, steps per second: 259, episode reward: -712.000, mean reward: -6.085 [-50.000, 80.000], mean action: 1.556 [0.000, 3.000],  loss: 4254.178711, mae: 566.202271, mean_q: -723.928223\n",
      " 14051/20000: episode: 78, duration: 0.311s, episode steps:  79, steps per second: 254, episode reward: -1202.000, mean reward: -15.215 [-50.000, 44.000], mean action: 2.127 [0.000, 3.000],  loss: 4038.350830, mae: 561.480774, mean_q: -717.568420\n",
      " 14110/20000: episode: 79, duration: 0.232s, episode steps:  59, steps per second: 254, episode reward: 266.000, mean reward:  4.508 [-50.000, 36.000], mean action: 1.254 [0.000, 3.000],  loss: 3224.301025, mae: 554.821289, mean_q: -710.086548\n",
      " 14278/20000: episode: 80, duration: 0.664s, episode steps: 168, steps per second: 253, episode reward: -3600.000, mean reward: -21.429 [-50.000, 64.000], mean action: 1.381 [0.000, 3.000],  loss: 3841.329590, mae: 554.726257, mean_q: -710.113586\n",
      " 14516/20000: episode: 81, duration: 0.910s, episode steps: 238, steps per second: 261, episode reward: -4722.000, mean reward: -19.840 [-50.000, 136.000], mean action: 1.618 [0.000, 3.000],  loss: 4373.605469, mae: 555.602905, mean_q: -709.967224\n",
      " 14720/20000: episode: 82, duration: 0.800s, episode steps: 204, steps per second: 255, episode reward: -4040.000, mean reward: -19.804 [-50.000, 80.000], mean action: 1.034 [0.000, 3.000],  loss: 4509.637695, mae: 562.341125, mean_q: -717.886047\n",
      " 14816/20000: episode: 83, duration: 0.386s, episode steps:  96, steps per second: 249, episode reward: -622.000, mean reward: -6.479 [-50.000, 64.000], mean action: 1.615 [0.000, 3.000],  loss: 3751.427002, mae: 554.555176, mean_q: -708.983948\n",
      " 14914/20000: episode: 84, duration: 0.382s, episode steps:  98, steps per second: 257, episode reward: -154.000, mean reward: -1.571 [-50.000, 76.000], mean action: 1.316 [0.000, 3.000],  loss: 3738.865723, mae: 556.034973, mean_q: -712.238892\n",
      " 15168/20000: episode: 85, duration: 0.983s, episode steps: 254, steps per second: 258, episode reward: -6866.000, mean reward: -27.031 [-50.000, 64.000], mean action: 1.717 [0.000, 3.000],  loss: 4536.219238, mae: 553.568298, mean_q: -707.265320\n",
      " 15224/20000: episode: 86, duration: 0.231s, episode steps:  56, steps per second: 242, episode reward: -40.000, mean reward: -0.714 [-50.000, 36.000], mean action: 1.071 [0.000, 3.000],  loss: 4078.916260, mae: 551.080750, mean_q: -705.688049\n",
      " 15903/20000: episode: 87, duration: 2.678s, episode steps: 679, steps per second: 254, episode reward: -21844.000, mean reward: -32.171 [-50.000, 256.000], mean action: 1.156 [0.000, 3.000],  loss: 3659.255615, mae: 596.815552, mean_q: -768.406921\n",
      " 15972/20000: episode: 88, duration: 0.282s, episode steps:  69, steps per second: 245, episode reward: -972.000, mean reward: -14.087 [-50.000, 52.000], mean action: 1.609 [0.000, 3.000],  loss: 2793.406494, mae: 643.064819, mean_q: -832.764160\n",
      " 16091/20000: episode: 89, duration: 0.467s, episode steps: 119, steps per second: 255, episode reward: -1370.000, mean reward: -11.513 [-50.000, 68.000], mean action: 2.025 [0.000, 3.000],  loss: 3549.867920, mae: 645.122192, mean_q: -833.909058\n",
      " 16231/20000: episode: 90, duration: 0.545s, episode steps: 140, steps per second: 257, episode reward: -1486.000, mean reward: -10.614 [-50.000, 88.000], mean action: 1.386 [0.000, 3.000],  loss: 2508.491699, mae: 657.004272, mean_q: -850.298462\n",
      " 16378/20000: episode: 91, duration: 0.587s, episode steps: 147, steps per second: 250, episode reward: -3832.000, mean reward: -26.068 [-50.000, 44.000], mean action: 0.769 [0.000, 3.000],  loss: 3918.125000, mae: 664.717651, mean_q: -859.734863\n",
      " 16441/20000: episode: 92, duration: 0.246s, episode steps:  63, steps per second: 256, episode reward: 364.000, mean reward:  5.778 [ 0.000, 44.000], mean action: 1.619 [0.000, 3.000],  loss: 3869.910400, mae: 658.740112, mean_q: -852.141174\n",
      " 16514/20000: episode: 93, duration: 0.288s, episode steps:  73, steps per second: 254, episode reward: -708.000, mean reward: -9.699 [-50.000, 52.000], mean action: 2.082 [0.000, 3.000],  loss: 3624.066406, mae: 656.360168, mean_q: -848.462402\n",
      " 16640/20000: episode: 94, duration: 0.490s, episode steps: 126, steps per second: 257, episode reward: -1004.000, mean reward: -7.968 [-50.000, 68.000], mean action: 2.056 [0.000, 3.000],  loss: 4665.528320, mae: 645.167480, mean_q: -830.597351\n",
      " 16849/20000: episode: 95, duration: 0.823s, episode steps: 209, steps per second: 254, episode reward: -5092.000, mean reward: -24.364 [-50.000, 84.000], mean action: 1.455 [0.000, 3.000],  loss: 3919.375000, mae: 641.159851, mean_q: -825.944519\n",
      " 16998/20000: episode: 96, duration: 0.579s, episode steps: 149, steps per second: 257, episode reward: -1972.000, mean reward: -13.235 [-50.000, 84.000], mean action: 2.201 [0.000, 3.000],  loss: 3771.739502, mae: 651.479736, mean_q: -840.344971\n",
      " 17130/20000: episode: 97, duration: 0.512s, episode steps: 132, steps per second: 258, episode reward: -2614.000, mean reward: -19.803 [-50.000, 40.000], mean action: 2.280 [0.000, 3.000],  loss: 3687.669434, mae: 646.405762, mean_q: -831.031006\n",
      " 17239/20000: episode: 98, duration: 0.419s, episode steps: 109, steps per second: 260, episode reward: -1040.000, mean reward: -9.541 [-50.000, 68.000], mean action: 1.853 [0.000, 3.000],  loss: 3574.967041, mae: 638.309937, mean_q: -823.910767\n",
      " 17324/20000: episode: 99, duration: 0.341s, episode steps:  85, steps per second: 249, episode reward: 164.000, mean reward:  1.929 [-50.000, 104.000], mean action: 1.424 [0.000, 3.000],  loss: 3575.708496, mae: 643.371399, mean_q: -831.205811\n",
      " 17472/20000: episode: 100, duration: 0.582s, episode steps: 148, steps per second: 254, episode reward: -3080.000, mean reward: -20.811 [-50.000, 72.000], mean action: 1.399 [0.000, 3.000],  loss: 4141.400879, mae: 643.484192, mean_q: -830.439819\n",
      " 17559/20000: episode: 101, duration: 0.338s, episode steps:  87, steps per second: 257, episode reward: 234.000, mean reward:  2.690 [-50.000, 96.000], mean action: 1.862 [0.000, 3.000],  loss: 4667.481445, mae: 632.069641, mean_q: -813.651733\n",
      " 17803/20000: episode: 102, duration: 0.935s, episode steps: 244, steps per second: 261, episode reward: -5714.000, mean reward: -23.418 [-50.000, 132.000], mean action: 1.213 [0.000, 3.000],  loss: 5102.202637, mae: 602.491272, mean_q: -773.648071\n",
      " 17918/20000: episode: 103, duration: 0.449s, episode steps: 115, steps per second: 256, episode reward: -2104.000, mean reward: -18.296 [-50.000, 48.000], mean action: 0.896 [0.000, 3.000],  loss: 4841.576172, mae: 586.164734, mean_q: -751.024902\n",
      " 17975/20000: episode: 104, duration: 0.223s, episode steps:  57, steps per second: 255, episode reward: 220.000, mean reward:  3.860 [-50.000, 32.000], mean action: 1.789 [0.000, 3.000],  loss: 4496.260254, mae: 582.181946, mean_q: -746.863831\n",
      " 18025/20000: episode: 105, duration: 0.193s, episode steps:  50, steps per second: 259, episode reward: 24.000, mean reward:  0.480 [-50.000, 40.000], mean action: 1.900 [0.000, 3.000],  loss: 6340.381836, mae: 583.291870, mean_q: -746.904846\n",
      " 18280/20000: episode: 106, duration: 0.968s, episode steps: 255, steps per second: 263, episode reward: -6060.000, mean reward: -23.765 [-50.000, 128.000], mean action: 2.224 [0.000, 3.000],  loss: 5613.013672, mae: 581.385437, mean_q: -745.242615\n",
      " 18331/20000: episode: 107, duration: 0.200s, episode steps:  51, steps per second: 254, episode reward: 198.000, mean reward:  3.882 [-50.000, 36.000], mean action: 1.569 [0.000, 3.000],  loss: 3359.139893, mae: 581.304443, mean_q: -746.686951\n",
      " 18414/20000: episode: 108, duration: 0.331s, episode steps:  83, steps per second: 251, episode reward: -876.000, mean reward: -10.554 [-50.000, 36.000], mean action: 1.434 [0.000, 3.000],  loss: 3544.708984, mae: 576.306946, mean_q: -740.378418\n",
      " 18641/20000: episode: 109, duration: 0.859s, episode steps: 227, steps per second: 264, episode reward: -7020.000, mean reward: -30.925 [-50.000, 48.000], mean action: 0.648 [0.000, 3.000],  loss: 4830.277344, mae: 583.913818, mean_q: -746.609436\n",
      " 18861/20000: episode: 110, duration: 0.840s, episode steps: 220, steps per second: 262, episode reward: -6312.000, mean reward: -28.691 [-50.000, 64.000], mean action: 1.414 [0.000, 3.000],  loss: 4174.291504, mae: 587.004944, mean_q: -753.516174\n",
      " 18990/20000: episode: 111, duration: 0.504s, episode steps: 129, steps per second: 256, episode reward: -1266.000, mean reward: -9.814 [-50.000, 84.000], mean action: 1.194 [0.000, 3.000],  loss: 5060.395508, mae: 589.592285, mean_q: -756.766357\n",
      " 19225/20000: episode: 112, duration: 0.884s, episode steps: 235, steps per second: 266, episode reward: -7250.000, mean reward: -30.851 [-50.000, 104.000], mean action: 0.613 [0.000, 3.000],  loss: 5324.772949, mae: 579.767578, mean_q: -740.164429\n",
      " 19371/20000: episode: 113, duration: 0.571s, episode steps: 146, steps per second: 256, episode reward: -2596.000, mean reward: -17.781 [-50.000, 80.000], mean action: 2.308 [0.000, 3.000],  loss: 4082.040283, mae: 580.515747, mean_q: -743.849670\n",
      " 19541/20000: episode: 114, duration: 0.653s, episode steps: 170, steps per second: 260, episode reward: -2112.000, mean reward: -12.424 [-50.000, 128.000], mean action: 1.471 [0.000, 3.000],  loss: 4212.235352, mae: 589.323730, mean_q: -754.271423\n",
      " 19746/20000: episode: 115, duration: 0.782s, episode steps: 205, steps per second: 262, episode reward: -4516.000, mean reward: -22.029 [-50.000, 80.000], mean action: 2.083 [0.000, 3.000],  loss: 4131.405273, mae: 597.946655, mean_q: -764.940308\n",
      " 19841/20000: episode: 116, duration: 0.369s, episode steps:  95, steps per second: 258, episode reward: -828.000, mean reward: -8.716 [-50.000, 84.000], mean action: 1.653 [0.000, 3.000],  loss: 3726.750000, mae: 596.983093, mean_q: -766.317261\n",
      "done, took 75.137 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5022a3220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "\n",
    "# Higher temporal window confuses the model\n",
    "# temporalWindow = 5\n",
    "temporalWindow = 1\n",
    "\n",
    "\n",
    "\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "\n",
    "# Result look alike\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "\n",
    "# Worst result\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Less invalid moves with more limit\n",
    "replayMemory = SequentialMemory(limit=2000, window_length=temporalWindow)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=1000,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=20000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_683/802290428.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  transformed = np.where(grid > 0, np.log2(grid), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 is over.\n",
      "Game 2 is over.\n",
      "Game 3 is over.\n",
      "Game 4 is over.\n",
      "Game 5 is over.\n",
      "Game 6 is over.\n",
      "Game 7 is over.\n",
      "Game 8 is over.\n",
      "Game 9 is over.\n",
      "Game 10 is over.\n",
      "Game 11 is over.\n",
      "Game 12 is over.\n",
      "Game 13 is over.\n",
      "Game 14 is over.\n",
      "Game 15 is over.\n",
      "Game 16 is over.\n",
      "Game 17 is over.\n",
      "Game 18 is over.\n",
      "Game 19 is over.\n",
      "Game 20 is over.\n",
      "Game 21 is over.\n",
      "Game 22 is over.\n",
      "Game 23 is over.\n",
      "Game 24 is over.\n",
      "Game 25 is over.\n",
      "Game 26 is over.\n",
      "Game 27 is over.\n",
      "Game 28 is over.\n",
      "Game 29 is over.\n",
      "Game 30 is over.\n",
      "Game 31 is over.\n",
      "Game 32 is over.\n",
      "Game 33 is over.\n",
      "Game 34 is over.\n",
      "Game 35 is over.\n",
      "Game 36 is over.\n",
      "Game 37 is over.\n",
      "Game 38 is over.\n",
      "Game 39 is over.\n",
      "Game 40 is over.\n",
      "Game 41 is over.\n",
      "Game 42 is over.\n",
      "Game 43 is over.\n",
      "Game 44 is over.\n",
      "Game 45 is over.\n",
      "Game 46 is over.\n",
      "Game 47 is over.\n",
      "Game 48 is over.\n",
      "Game 49 is over.\n",
      "Game 50 is over.\n",
      "Game 51 is over.\n",
      "Game 52 is over.\n",
      "Game 53 is over.\n",
      "Game 54 is over.\n",
      "Game 55 is over.\n",
      "Game 56 is over.\n",
      "Game 57 is over.\n",
      "Game 58 is over.\n",
      "Game 59 is over.\n",
      "Game 60 is over.\n",
      "Game 61 is over.\n",
      "Game 62 is over.\n",
      "Game 63 is over.\n",
      "Game 64 is over.\n",
      "Game 65 is over.\n",
      "Game 66 is over.\n",
      "Game 67 is over.\n",
      "Game 68 is over.\n",
      "Game 69 is over.\n",
      "Game 70 is over.\n",
      "Game 71 is over.\n",
      "Game 72 is over.\n",
      "Game 73 is over.\n",
      "Game 74 is over.\n",
      "Game 75 is over.\n",
      "Game 76 is over.\n",
      "Game 77 is over.\n",
      "Game 78 is over.\n",
      "Game 79 is over.\n",
      "Game 80 is over.\n",
      "Game 81 is over.\n",
      "Game 82 is over.\n",
      "Game 83 is over.\n",
      "Game 84 is over.\n",
      "Game 85 is over.\n",
      "Game 86 is over.\n",
      "Game 87 is over.\n",
      "Game 88 is over.\n",
      "Game 89 is over.\n",
      "Game 90 is over.\n",
      "Game 91 is over.\n",
      "Game 92 is over.\n",
      "Game 93 is over.\n",
      "Game 94 is over.\n",
      "Game 95 is over.\n",
      "Game 96 is over.\n",
      "Game 97 is over.\n",
      "Game 98 is over.\n",
      "Game 99 is over.\n",
      "Game 100 is over.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gameId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>988</td>\n",
       "      <td>128</td>\n",
       "      <td>0.475490</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>126</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>1.096774</td>\n",
       "      <td>3.315789</td>\n",
       "      <td>1.130435</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>620</td>\n",
       "      <td>64</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>106</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1.068966</td>\n",
       "      <td>1.606061</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552</td>\n",
       "      <td>64</td>\n",
       "      <td>0.593583</td>\n",
       "      <td>187</td>\n",
       "      <td>129</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>4.031250</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>944</td>\n",
       "      <td>128</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>245</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>122</td>\n",
       "      <td>24</td>\n",
       "      <td>1.043478</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>4.357143</td>\n",
       "      <td>1.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>828</td>\n",
       "      <td>64</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>217</td>\n",
       "      <td>77</td>\n",
       "      <td>102</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>644</td>\n",
       "      <td>64</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.468750</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>608</td>\n",
       "      <td>64</td>\n",
       "      <td>0.528736</td>\n",
       "      <td>174</td>\n",
       "      <td>25</td>\n",
       "      <td>120</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>1.041667</td>\n",
       "      <td>4.137931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>532</td>\n",
       "      <td>64</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>97</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.030303</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1008</td>\n",
       "      <td>128</td>\n",
       "      <td>0.147287</td>\n",
       "      <td>129</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>1.095238</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>1.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  higherTile  invalidMove (%)  steps  upMoves  downMoves  \\\n",
       "gameId                                                                  \n",
       "0         576          64         0.000000     78       20         23   \n",
       "1         988         128         0.475490    204       34        126   \n",
       "2         620          64         0.198113    106       31         53   \n",
       "3         552          64         0.593583    187      129         40   \n",
       "4         944         128         0.595918    245       24         75   \n",
       "...       ...         ...              ...    ...      ...        ...   \n",
       "95        828          64         0.548387    217       77        102   \n",
       "96        644          64         0.187500    112       36         47   \n",
       "97        608          64         0.528736    174       25        120   \n",
       "98        532          64         0.185567     97       31         34   \n",
       "99       1008         128         0.147287    129       23         24   \n",
       "\n",
       "        leftMoves  rightMoves  upSeqAvg  downSeqAvg  leftSeqAvg  rightSeqAvg  \n",
       "gameId                                                                        \n",
       "0              18          17  1.000000    1.000000    1.000000     1.000000  \n",
       "1              26          18  1.096774    3.315789    1.130435     1.000000  \n",
       "2              13           9  1.068966    1.606061    1.083333     1.000000  \n",
       "3              13           5  4.031250    1.333333    1.083333     1.000000  \n",
       "4             122          24  1.043478    2.777778    4.357143     1.043478  \n",
       "...           ...         ...       ...         ...         ...          ...  \n",
       "95             21          17  2.750000    3.187500    1.050000     1.000000  \n",
       "96             16          13  1.200000    1.468750    1.066667     1.083333  \n",
       "97             16          13  1.041667    4.137931    1.000000     1.083333  \n",
       "98             26           6  1.000000    1.030303    3.250000     1.000000  \n",
       "99             34          48  1.095238    1.090909    1.062500     1.454545  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>774.920000</td>\n",
       "      <td>78.080000</td>\n",
       "      <td>0.328155</td>\n",
       "      <td>161.670000</td>\n",
       "      <td>33.540000</td>\n",
       "      <td>78.010000</td>\n",
       "      <td>29.540000</td>\n",
       "      <td>20.580000</td>\n",
       "      <td>1.254238</td>\n",
       "      <td>2.340723</td>\n",
       "      <td>1.409042</td>\n",
       "      <td>1.139608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>376.748387</td>\n",
       "      <td>42.925277</td>\n",
       "      <td>0.207529</td>\n",
       "      <td>88.269566</td>\n",
       "      <td>23.732695</td>\n",
       "      <td>62.247874</td>\n",
       "      <td>23.652544</td>\n",
       "      <td>13.766582</td>\n",
       "      <td>0.586989</td>\n",
       "      <td>1.308818</td>\n",
       "      <td>0.833240</td>\n",
       "      <td>0.380032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>570.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.152030</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.190616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>712.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.346473</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.043478</td>\n",
       "      <td>2.025641</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>993.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.488353</td>\n",
       "      <td>202.500000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>112.250000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>3.043735</td>\n",
       "      <td>1.188889</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2228.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.720379</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>4.117647</td>\n",
       "      <td>6.052632</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.680000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score  higherTile  invalidMove (%)       steps     upMoves  \\\n",
       "count   100.000000  100.000000       100.000000  100.000000  100.000000   \n",
       "mean    774.920000   78.080000         0.328155  161.670000   33.540000   \n",
       "std     376.748387   42.925277         0.207529   88.269566   23.732695   \n",
       "min     160.000000   16.000000         0.000000   44.000000    7.000000   \n",
       "25%     570.000000   64.000000         0.152030  100.000000   20.000000   \n",
       "50%     712.000000   64.000000         0.346473  139.500000   27.000000   \n",
       "75%     993.000000  128.000000         0.488353  202.500000   36.250000   \n",
       "max    2228.000000  256.000000         0.720379  494.000000  140.000000   \n",
       "\n",
       "        downMoves   leftMoves  rightMoves    upSeqAvg  downSeqAvg  leftSeqAvg  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean    78.010000   29.540000   20.580000    1.254238    2.340723    1.409042   \n",
       "std     62.247874   23.652544   13.766582    0.586989    1.308818    0.833240   \n",
       "min     11.000000    6.000000    3.000000    1.000000    1.000000    1.000000   \n",
       "25%     31.750000   16.000000   13.000000    1.000000    1.190616    1.000000   \n",
       "50%     56.000000   21.500000   18.000000    1.043478    2.025641    1.062500   \n",
       "75%    112.250000   32.000000   24.000000    1.111111    3.043735    1.188889   \n",
       "max    285.000000  160.000000   92.000000    4.117647    6.052632    5.000000   \n",
       "\n",
       "       rightSeqAvg  \n",
       "count   100.000000  \n",
       "mean      1.139608  \n",
       "std       0.380032  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.083333  \n",
       "max       3.680000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "def log2_transform(grid):\n",
    "    transformed = np.where(grid > 0, np.log2(grid), 0)\n",
    "    return transformed\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(100)]\n",
    "metrics = {\n",
    "    \"gameId\": [],\n",
    "    \"score\": [],\n",
    "    \"higherTile\": [],\n",
    "    \"invalidMove (%)\": [],\n",
    "    \"steps\": [],\n",
    "    \"upMoves\": [],\n",
    "    \"downMoves\": [],\n",
    "    \"leftMoves\": [],\n",
    "    \"rightMoves\": [],\n",
    "    \"upSeqAvg\": [],\n",
    "    \"downSeqAvg\": [],\n",
    "    \"leftSeqAvg\": [],\n",
    "    \"rightSeqAvg\": [],\n",
    "}\n",
    "for i, game in enumerate(games):\n",
    "    # Metrics\n",
    "    steps = 0\n",
    "    invalidMove = 0\n",
    "    upMoves = 0\n",
    "    downMoves = 0\n",
    "    leftMoves = 0\n",
    "    rightMoves = 0\n",
    "    upSeqAvg = 0\n",
    "    downSeqAvg = 0\n",
    "    leftSeqAvg = 0\n",
    "    rightSeqAvg = 0\n",
    "\n",
    "    sequences = {\n",
    "        \"up\": [],\n",
    "        \"down\": [],\n",
    "        \"left\": [],\n",
    "        \"right\": []\n",
    "    }\n",
    "    lastMove = None\n",
    "\n",
    "    # print(f\"Game {i + 1} initial state:\")\n",
    "    # game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        observation = log2_transform(observation)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        # print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        if not game.move(actionMapping[predictedAction]):\n",
    "            invalidMove += 1\n",
    "        \n",
    "        # print(f\"Game {i + 1} currrent state:\")\n",
    "        # game.render()\n",
    "\n",
    "        # input(\"\\nPress any key to continue...\")\n",
    "\n",
    "        steps += 1\n",
    "        if actionMapping[predictedAction] == \"up\":\n",
    "            upMoves += 1\n",
    "            if lastMove == \"up\":\n",
    "                sequences[\"up\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"up\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"down\":\n",
    "            downMoves += 1\n",
    "            if lastMove == \"down\":\n",
    "                sequences[\"down\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"down\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"left\":\n",
    "            leftMoves += 1\n",
    "            if lastMove == \"left\":\n",
    "                sequences[\"left\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"left\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"right\":\n",
    "            rightMoves += 1\n",
    "            if lastMove == \"right\":\n",
    "                sequences[\"right\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"right\"].append(1)\n",
    "        \n",
    "        lastMove = actionMapping[predictedAction]\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "    metrics[\"gameId\"].append(i)\n",
    "    metrics[\"score\"].append(game.score)\n",
    "    metrics[\"higherTile\"].append(np.max(game.grid))\n",
    "    metrics[\"invalidMove (%)\"].append(invalidMove/steps)\n",
    "    metrics[\"steps\"].append(steps)\n",
    "    metrics[\"upMoves\"].append(upMoves)\n",
    "    metrics[\"downMoves\"].append(downMoves)\n",
    "    metrics[\"leftMoves\"].append(leftMoves)\n",
    "    metrics[\"rightMoves\"].append(rightMoves)\n",
    "    metrics[\"upSeqAvg\"].append(np.mean(np.array(sequences[\"up\"])))\n",
    "    metrics[\"downSeqAvg\"].append(np.mean(np.array(sequences[\"down\"])))\n",
    "    metrics[\"leftSeqAvg\"].append(np.mean(np.array(sequences[\"left\"])))\n",
    "    metrics[\"rightSeqAvg\"].append(np.mean(np.array(sequences[\"right\"])))\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics).set_index(\"gameId\")\n",
    "display(metrics)\n",
    "display(metrics.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [\"up\", \"left\", \"right\", \"down\"]\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
