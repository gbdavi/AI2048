{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 00:26:59.013688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-04-10 00:26:59.013734: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "\n",
    "# hiddenFeatures = 16\n",
    "hiddenFeatures = 32\n",
    "hiddenLayers = 1\n",
    "# hiddenLayers = 10\n",
    "\n",
    "activation=\"relu\"\n",
    "# activation=\"leaky_relu\"\n",
    "\n",
    "# Higher temporal window confuses the model\n",
    "# temporalWindow = 5\n",
    "temporalWindow = 1\n",
    "\n",
    "\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "for i in range(hiddenLayers):\n",
    "    model.add(Dense(hiddenFeatures))\n",
    "    model.add(Activation(activation=activation))\n",
    "\n",
    "# Result look alike\n",
    "# model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "\n",
    "# Worst result\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Less invalid moves with more limit\n",
    "replayMemory = SequentialMemory(limit=2000, window_length=temporalWindow)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=1000,\n",
    "    # target_model_update=0.01,\n",
    "    target_model_update=500,\n",
    "    policy=policy,\n",
    ")\n",
    "# dqn.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "# dqn.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mse\"])\n",
    "# dqn.compile(optimizer=RMSprop(learning_rate=0.001, epsilon=0.0000001), metrics=[\"mae\"])\n",
    "# dqn.compile(optimizer=RMSprop(learning_rate=0.001, epsilon=0.0000001), metrics=[\"mse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  349/2000: episode: 1, duration: 0.443s, episode steps: 349, steps per second: 789, episode reward: -12220.000, mean reward: -35.014 [-50.000, 68.000], mean action: 1.628 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  652/2000: episode: 2, duration: 0.196s, episode steps: 303, steps per second: 1549, episode reward: -8698.000, mean reward: -28.706 [-50.000, 164.000], mean action: 1.904 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1126/2000: episode: 3, duration: 1.420s, episode steps: 474, steps per second: 334, episode reward: -16024.000, mean reward: -33.806 [-50.000, 140.000], mean action: 1.080 [0.000, 3.000],  loss: 376.771841, mse: 603.668831, mean_q: -11.507554\n",
      " 1262/2000: episode: 4, duration: 0.616s, episode steps: 136, steps per second: 221, episode reward: -458.000, mean reward: -3.368 [-50.000, 128.000], mean action: 1.941 [0.000, 3.000],  loss: 291.103729, mse: 533.470337, mean_q: 3.498658\n",
      " 1414/2000: episode: 5, duration: 0.674s, episode steps: 152, steps per second: 225, episode reward: -2702.000, mean reward: -17.776 [-50.000, 84.000], mean action: 1.836 [0.000, 3.000],  loss: 254.180740, mse: 549.038879, mean_q: 12.747724\n",
      " 1775/2000: episode: 6, duration: 1.575s, episode steps: 361, steps per second: 229, episode reward: -6256.000, mean reward: -17.330 [-50.000, 272.000], mean action: 1.443 [0.000, 3.000],  loss: 318.651001, mse: 557.210693, mean_q: 7.777756\n",
      " 1853/2000: episode: 7, duration: 0.350s, episode steps:  78, steps per second: 223, episode reward: -200.000, mean reward: -2.564 [-50.000, 36.000], mean action: 1.526 [0.000, 3.000],  loss: 298.294983, mse: 586.753235, mean_q: 5.839777\n",
      "done, took 5.917 seconds\n",
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  377/10000: episode: 1, duration: 0.438s, episode steps: 377, steps per second: 860, episode reward: -10510.000, mean reward: -27.878 [-50.000, 164.000], mean action: 1.674 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  749/10000: episode: 2, duration: 0.253s, episode steps: 372, steps per second: 1469, episode reward: -6370.000, mean reward: -17.124 [-50.000, 276.000], mean action: 1.691 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  881/10000: episode: 3, duration: 0.088s, episode steps: 132, steps per second: 1508, episode reward: -2560.000, mean reward: -19.394 [-50.000, 36.000], mean action: 1.356 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1031/10000: episode: 4, duration: 0.946s, episode steps: 150, steps per second: 159, episode reward: -3356.000, mean reward: -22.373 [-50.000, 48.000], mean action: 2.287 [0.000, 3.000],  loss: 605.248158, mse: 907.488090, mean_q: -1.864245\n",
      " 1110/10000: episode: 5, duration: 0.398s, episode steps:  79, steps per second: 199, episode reward: 468.000, mean reward:  5.924 [-50.000, 64.000], mean action: 1.709 [0.000, 3.000],  loss: 511.679596, mse: 1080.131470, mean_q: -7.386292\n",
      " 1197/10000: episode: 6, duration: 0.472s, episode steps:  87, steps per second: 184, episode reward: -1084.000, mean reward: -12.460 [-50.000, 32.000], mean action: 0.701 [0.000, 3.000],  loss: 418.814972, mse: 997.068359, mean_q: -6.291780\n",
      " 1479/10000: episode: 7, duration: 1.294s, episode steps: 282, steps per second: 218, episode reward: -5872.000, mean reward: -20.823 [-50.000, 144.000], mean action: 1.465 [0.000, 3.000],  loss: 482.826904, mse: 1035.965210, mean_q: -6.034113\n",
      " 1675/10000: episode: 8, duration: 0.899s, episode steps: 196, steps per second: 218, episode reward: -5262.000, mean reward: -26.847 [-50.000, 64.000], mean action: 0.648 [0.000, 3.000],  loss: 545.534973, mse: 1632.123901, mean_q: -15.609347\n",
      " 1841/10000: episode: 9, duration: 0.758s, episode steps: 166, steps per second: 219, episode reward: -1528.000, mean reward: -9.205 [-50.000, 76.000], mean action: 0.843 [0.000, 3.000],  loss: 538.510071, mse: 1837.867065, mean_q: -20.385746\n",
      " 1985/10000: episode: 10, duration: 0.651s, episode steps: 144, steps per second: 221, episode reward: -680.000, mean reward: -4.722 [-50.000, 132.000], mean action: 1.562 [0.000, 3.000],  loss: 452.103943, mse: 1823.552124, mean_q: -20.312191\n",
      " 2122/10000: episode: 11, duration: 0.623s, episode steps: 137, steps per second: 220, episode reward: -1922.000, mean reward: -14.029 [-50.000, 68.000], mean action: 1.891 [0.000, 3.000],  loss: 582.655823, mse: 2359.243652, mean_q: -25.398235\n",
      " 2210/10000: episode: 12, duration: 0.403s, episode steps:  88, steps per second: 218, episode reward: 250.000, mean reward:  2.841 [-50.000, 68.000], mean action: 1.250 [0.000, 3.000],  loss: 603.949402, mse: 2543.487305, mean_q: -25.103796\n",
      " 2434/10000: episode: 13, duration: 0.999s, episode steps: 224, steps per second: 224, episode reward: -2578.000, mean reward: -11.509 [-50.000, 168.000], mean action: 1.022 [0.000, 3.000],  loss: 534.456665, mse: 2390.131348, mean_q: -21.354748\n",
      " 2548/10000: episode: 14, duration: 0.524s, episode steps: 114, steps per second: 217, episode reward: -846.000, mean reward: -7.421 [-50.000, 76.000], mean action: 1.114 [0.000, 3.000],  loss: 528.760803, mse: 2659.582031, mean_q: -21.247704\n",
      " 2610/10000: episode: 15, duration: 0.285s, episode steps:  62, steps per second: 217, episode reward: 224.000, mean reward:  3.613 [-50.000, 44.000], mean action: 1.823 [0.000, 3.000],  loss: 612.203552, mse: 3015.701416, mean_q: -24.704708\n",
      " 2700/10000: episode: 16, duration: 0.406s, episode steps:  90, steps per second: 222, episode reward: 518.000, mean reward:  5.756 [-50.000, 64.000], mean action: 1.356 [0.000, 3.000],  loss: 502.825653, mse: 2946.017090, mean_q: -22.614494\n",
      " 2817/10000: episode: 17, duration: 0.527s, episode steps: 117, steps per second: 222, episode reward: 160.000, mean reward:  1.368 [-50.000, 128.000], mean action: 1.521 [0.000, 3.000],  loss: 470.130981, mse: 2802.940186, mean_q: -17.853586\n",
      " 2865/10000: episode: 18, duration: 0.219s, episode steps:  48, steps per second: 219, episode reward: 182.000, mean reward:  3.792 [-50.000, 52.000], mean action: 1.896 [0.000, 3.000],  loss: 503.299957, mse: 2896.654297, mean_q: -18.081171\n",
      " 3101/10000: episode: 19, duration: 1.066s, episode steps: 236, steps per second: 221, episode reward: -7014.000, mean reward: -29.720 [-50.000, 92.000], mean action: 1.987 [0.000, 3.000],  loss: 533.176208, mse: 2561.737793, mean_q: -13.752756\n",
      " 3173/10000: episode: 20, duration: 0.329s, episode steps:  72, steps per second: 219, episode reward: 430.000, mean reward:  5.972 [-50.000, 64.000], mean action: 1.750 [0.000, 3.000],  loss: 519.275391, mse: 2447.204102, mean_q: -15.671762\n",
      " 3246/10000: episode: 21, duration: 0.334s, episode steps:  73, steps per second: 218, episode reward: 218.000, mean reward:  2.986 [-50.000, 76.000], mean action: 1.411 [0.000, 3.000],  loss: 541.021606, mse: 2402.075439, mean_q: -16.412560\n",
      " 3543/10000: episode: 22, duration: 1.461s, episode steps: 297, steps per second: 203, episode reward: -4128.000, mean reward: -13.899 [-50.000, 164.000], mean action: 1.953 [0.000, 3.000],  loss: 536.019287, mse: 2302.699951, mean_q: -14.778688\n",
      " 3607/10000: episode: 23, duration: 0.296s, episode steps:  64, steps per second: 216, episode reward: 360.000, mean reward:  5.625 [ 0.000, 36.000], mean action: 1.469 [0.000, 3.000],  loss: 610.951294, mse: 2593.949951, mean_q: -18.137722\n",
      " 3682/10000: episode: 24, duration: 0.350s, episode steps:  75, steps per second: 214, episode reward: 544.000, mean reward:  7.253 [ 0.000, 68.000], mean action: 1.373 [0.000, 3.000],  loss: 593.388550, mse: 2563.997559, mean_q: -17.069878\n",
      " 4034/10000: episode: 25, duration: 1.590s, episode steps: 352, steps per second: 221, episode reward: -7610.000, mean reward: -21.619 [-50.000, 132.000], mean action: 1.807 [0.000, 3.000],  loss: 616.326416, mse: 2741.095459, mean_q: -19.926537\n",
      " 4257/10000: episode: 26, duration: 0.974s, episode steps: 223, steps per second: 229, episode reward: -2368.000, mean reward: -10.619 [-50.000, 140.000], mean action: 1.798 [0.000, 3.000],  loss: 648.837585, mse: 3650.594238, mean_q: -30.590643\n",
      " 4423/10000: episode: 27, duration: 0.745s, episode steps: 166, steps per second: 223, episode reward: -1520.000, mean reward: -9.157 [-50.000, 128.000], mean action: 1.199 [0.000, 3.000],  loss: 572.192566, mse: 3907.298828, mean_q: -33.243938\n",
      " 4498/10000: episode: 28, duration: 0.346s, episode steps:  75, steps per second: 217, episode reward: 404.000, mean reward:  5.387 [-50.000, 68.000], mean action: 1.133 [0.000, 3.000],  loss: 694.832214, mse: 3830.665039, mean_q: -30.735964\n",
      " 4555/10000: episode: 29, duration: 0.266s, episode steps:  57, steps per second: 215, episode reward: 150.000, mean reward:  2.632 [-50.000, 40.000], mean action: 1.281 [0.000, 3.000],  loss: 727.187866, mse: 4172.266113, mean_q: -33.589905\n",
      " 4765/10000: episode: 30, duration: 0.938s, episode steps: 210, steps per second: 224, episode reward: -1046.000, mean reward: -4.981 [-50.000, 136.000], mean action: 1.476 [0.000, 3.000],  loss: 684.225281, mse: 4344.839844, mean_q: -35.869835\n",
      " 5127/10000: episode: 31, duration: 1.589s, episode steps: 362, steps per second: 228, episode reward: -9926.000, mean reward: -27.420 [-50.000, 128.000], mean action: 1.472 [0.000, 3.000],  loss: 693.424988, mse: 4962.895020, mean_q: -41.317669\n",
      " 5191/10000: episode: 32, duration: 0.291s, episode steps:  64, steps per second: 220, episode reward: -332.000, mean reward: -5.188 [-50.000, 40.000], mean action: 1.484 [0.000, 3.000],  loss: 756.420288, mse: 5722.912109, mean_q: -47.888954\n",
      " 5316/10000: episode: 33, duration: 0.571s, episode steps: 125, steps per second: 219, episode reward: -1504.000, mean reward: -12.032 [-50.000, 72.000], mean action: 1.640 [0.000, 3.000],  loss: 750.674988, mse: 5891.528320, mean_q: -49.311657\n",
      " 5431/10000: episode: 34, duration: 0.515s, episode steps: 115, steps per second: 223, episode reward: -2146.000, mean reward: -18.661 [-50.000, 48.000], mean action: 1.243 [0.000, 3.000],  loss: 724.146729, mse: 5601.052734, mean_q: -46.490837\n",
      " 5508/10000: episode: 35, duration: 0.354s, episode steps:  77, steps per second: 218, episode reward: -202.000, mean reward: -2.623 [-50.000, 72.000], mean action: 1.455 [0.000, 3.000],  loss: 614.750427, mse: 5537.868652, mean_q: -45.699368\n",
      " 5589/10000: episode: 36, duration: 0.386s, episode steps:  81, steps per second: 210, episode reward: -52.000, mean reward: -0.642 [-50.000, 56.000], mean action: 1.630 [0.000, 3.000],  loss: 717.218384, mse: 6323.962891, mean_q: -50.932098\n",
      " 5693/10000: episode: 37, duration: 0.472s, episode steps: 104, steps per second: 221, episode reward: -400.000, mean reward: -3.846 [-50.000, 80.000], mean action: 1.471 [0.000, 3.000],  loss: 727.098938, mse: 6188.792480, mean_q: -49.931133\n",
      " 5921/10000: episode: 38, duration: 1.018s, episode steps: 228, steps per second: 224, episode reward: -6344.000, mean reward: -27.825 [-50.000, 64.000], mean action: 0.978 [0.000, 3.000],  loss: 603.258789, mse: 6480.795898, mean_q: -51.704975\n",
      " 6038/10000: episode: 39, duration: 0.512s, episode steps: 117, steps per second: 228, episode reward: -1696.000, mean reward: -14.496 [-50.000, 76.000], mean action: 1.590 [0.000, 3.000],  loss: 640.632263, mse: 6060.951172, mean_q: -49.226101\n",
      " 6087/10000: episode: 40, duration: 0.227s, episode steps:  49, steps per second: 216, episode reward: 248.000, mean reward:  5.061 [ 0.000, 40.000], mean action: 1.429 [0.000, 3.000],  loss: 684.199829, mse: 6721.219238, mean_q: -52.772976\n",
      " 6189/10000: episode: 41, duration: 0.465s, episode steps: 102, steps per second: 219, episode reward: -1462.000, mean reward: -14.333 [-50.000, 44.000], mean action: 1.392 [0.000, 3.000],  loss: 674.389404, mse: 6854.236328, mean_q: -52.803310\n",
      " 6321/10000: episode: 42, duration: 0.586s, episode steps: 132, steps per second: 225, episode reward: -2624.000, mean reward: -19.879 [-50.000, 84.000], mean action: 1.455 [0.000, 3.000],  loss: 658.142639, mse: 6663.792480, mean_q: -52.252621\n",
      " 6406/10000: episode: 43, duration: 0.383s, episode steps:  85, steps per second: 222, episode reward: -562.000, mean reward: -6.612 [-50.000, 44.000], mean action: 1.376 [0.000, 3.000],  loss: 597.505798, mse: 6929.152344, mean_q: -53.591850\n",
      " 6461/10000: episode: 44, duration: 0.255s, episode steps:  55, steps per second: 215, episode reward:  2.000, mean reward:  0.036 [-50.000, 40.000], mean action: 1.673 [0.000, 3.000],  loss: 598.503357, mse: 6888.521484, mean_q: -54.800106\n",
      " 6632/10000: episode: 45, duration: 0.801s, episode steps: 171, steps per second: 213, episode reward: -1264.000, mean reward: -7.392 [-50.000, 140.000], mean action: 1.292 [0.000, 3.000],  loss: 661.904968, mse: 7496.802734, mean_q: -57.611015\n",
      " 6703/10000: episode: 46, duration: 0.333s, episode steps:  71, steps per second: 213, episode reward: -438.000, mean reward: -6.169 [-50.000, 40.000], mean action: 1.310 [0.000, 3.000],  loss: 664.845093, mse: 7818.689941, mean_q: -57.851398\n",
      " 6837/10000: episode: 47, duration: 0.613s, episode steps: 134, steps per second: 219, episode reward: -3558.000, mean reward: -26.552 [-50.000, 68.000], mean action: 1.328 [0.000, 3.000],  loss: 652.924194, mse: 7846.031738, mean_q: -59.648300\n",
      " 7072/10000: episode: 48, duration: 1.058s, episode steps: 235, steps per second: 222, episode reward: -1090.000, mean reward: -4.638 [-50.000, 276.000], mean action: 1.936 [0.000, 3.000],  loss: 660.976868, mse: 7810.258301, mean_q: -58.079388\n",
      " 7145/10000: episode: 49, duration: 0.339s, episode steps:  73, steps per second: 215, episode reward: 414.000, mean reward:  5.671 [-50.000, 48.000], mean action: 1.205 [0.000, 3.000],  loss: 839.167664, mse: 7967.019043, mean_q: -56.280205\n",
      " 7277/10000: episode: 50, duration: 0.588s, episode steps: 132, steps per second: 224, episode reward: -2452.000, mean reward: -18.576 [-50.000, 80.000], mean action: 2.167 [0.000, 3.000],  loss: 720.900391, mse: 8102.530273, mean_q: -58.806824\n",
      " 7362/10000: episode: 51, duration: 0.383s, episode steps:  85, steps per second: 222, episode reward: -260.000, mean reward: -3.059 [-50.000, 36.000], mean action: 1.435 [0.000, 3.000],  loss: 789.973328, mse: 7827.414062, mean_q: -56.514603\n",
      " 7476/10000: episode: 52, duration: 0.523s, episode steps: 114, steps per second: 218, episode reward: 944.000, mean reward:  8.281 [-50.000, 132.000], mean action: 1.570 [0.000, 3.000],  loss: 743.303101, mse: 7904.651855, mean_q: -58.063629\n",
      " 7752/10000: episode: 53, duration: 1.269s, episode steps: 276, steps per second: 217, episode reward: -8524.000, mean reward: -30.884 [-50.000, 72.000], mean action: 0.884 [0.000, 3.000],  loss: 695.337341, mse: 8391.443359, mean_q: -61.641296\n",
      " 7900/10000: episode: 54, duration: 0.666s, episode steps: 148, steps per second: 222, episode reward: -2324.000, mean reward: -15.703 [-50.000, 64.000], mean action: 2.020 [0.000, 3.000],  loss: 719.363098, mse: 8859.590820, mean_q: -64.334473\n",
      " 8133/10000: episode: 55, duration: 1.073s, episode steps: 233, steps per second: 217, episode reward: -3854.000, mean reward: -16.541 [-50.000, 144.000], mean action: 1.060 [0.000, 3.000],  loss: 843.146301, mse: 8892.963867, mean_q: -63.870129\n",
      " 8233/10000: episode: 56, duration: 0.442s, episode steps: 100, steps per second: 226, episode reward: -420.000, mean reward: -4.200 [-50.000, 104.000], mean action: 1.560 [0.000, 3.000],  loss: 731.696777, mse: 9985.420898, mean_q: -68.644806\n",
      " 8294/10000: episode: 57, duration: 0.278s, episode steps:  61, steps per second: 220, episode reward: 356.000, mean reward:  5.836 [ 0.000, 40.000], mean action: 1.787 [0.000, 3.000],  loss: 856.514282, mse: 9915.550781, mean_q: -66.925964\n",
      " 8388/10000: episode: 58, duration: 0.422s, episode steps:  94, steps per second: 223, episode reward: -8.000, mean reward: -0.085 [-50.000, 68.000], mean action: 2.085 [0.000, 3.000],  loss: 766.267700, mse: 9563.811523, mean_q: -65.100235\n",
      " 8691/10000: episode: 59, duration: 1.342s, episode steps: 303, steps per second: 226, episode reward: -7030.000, mean reward: -23.201 [-50.000, 148.000], mean action: 2.208 [0.000, 3.000],  loss: 847.807434, mse: 10518.833008, mean_q: -73.035645\n",
      " 8845/10000: episode: 60, duration: 0.685s, episode steps: 154, steps per second: 225, episode reward: -2010.000, mean reward: -13.052 [-50.000, 68.000], mean action: 2.039 [0.000, 3.000],  loss: 839.361389, mse: 11678.478516, mean_q: -80.024734\n",
      " 9075/10000: episode: 61, duration: 1.019s, episode steps: 230, steps per second: 226, episode reward: -2302.000, mean reward: -10.009 [-50.000, 132.000], mean action: 1.335 [0.000, 3.000],  loss: 883.376099, mse: 12144.936523, mean_q: -82.373047\n",
      " 9127/10000: episode: 62, duration: 0.237s, episode steps:  52, steps per second: 220, episode reward: -548.000, mean reward: -10.538 [-50.000, 24.000], mean action: 1.346 [0.000, 3.000],  loss: 875.127502, mse: 13364.736328, mean_q: -88.035172\n",
      " 9246/10000: episode: 63, duration: 0.530s, episode steps: 119, steps per second: 225, episode reward: -1224.000, mean reward: -10.286 [-50.000, 76.000], mean action: 1.765 [0.000, 3.000],  loss: 839.869263, mse: 13042.110352, mean_q: -85.353722\n",
      " 9428/10000: episode: 64, duration: 0.812s, episode steps: 182, steps per second: 224, episode reward: -2904.000, mean reward: -15.956 [-50.000, 100.000], mean action: 1.934 [0.000, 3.000],  loss: 849.467346, mse: 14051.878906, mean_q: -90.914619\n",
      " 9537/10000: episode: 65, duration: 0.486s, episode steps: 109, steps per second: 224, episode reward: 668.000, mean reward:  6.128 [-50.000, 140.000], mean action: 1.358 [0.000, 3.000],  loss: 879.719299, mse: 14878.882812, mean_q: -95.234932\n",
      " 9583/10000: episode: 66, duration: 0.207s, episode steps:  46, steps per second: 223, episode reward: 232.000, mean reward:  5.043 [ 0.000, 52.000], mean action: 1.348 [0.000, 3.000],  loss: 882.324829, mse: 15323.089844, mean_q: -95.693420\n",
      " 9674/10000: episode: 67, duration: 0.428s, episode steps:  91, steps per second: 213, episode reward: -530.000, mean reward: -5.824 [-50.000, 68.000], mean action: 1.099 [0.000, 3.000],  loss: 857.320801, mse: 15139.872070, mean_q: -93.157867\n",
      " 9814/10000: episode: 68, duration: 0.618s, episode steps: 140, steps per second: 227, episode reward: -484.000, mean reward: -3.457 [-50.000, 132.000], mean action: 1.057 [0.000, 3.000],  loss: 865.240479, mse: 14136.333984, mean_q: -86.508705\n",
      "done, took 42.542 seconds\n",
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   97/10000: episode: 1, duration: 0.283s, episode steps:  97, steps per second: 342, episode reward: 580.000, mean reward:  5.979 [-50.000, 136.000], mean action: 1.423 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  234/10000: episode: 2, duration: 0.092s, episode steps: 137, steps per second: 1482, episode reward: -94.000, mean reward: -0.686 [-50.000, 128.000], mean action: 1.358 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  333/10000: episode: 3, duration: 0.071s, episode steps:  99, steps per second: 1400, episode reward: -1336.000, mean reward: -13.495 [-50.000, 40.000], mean action: 1.616 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  529/10000: episode: 4, duration: 0.139s, episode steps: 196, steps per second: 1412, episode reward: -2870.000, mean reward: -14.643 [-50.000, 160.000], mean action: 1.821 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  670/10000: episode: 5, duration: 0.098s, episode steps: 141, steps per second: 1437, episode reward: -1306.000, mean reward: -9.262 [-50.000, 76.000], mean action: 1.000 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n",
      "  770/10000: episode: 6, duration: 0.074s, episode steps: 100, steps per second: 1345, episode reward: -1466.000, mean reward: -14.660 [-50.000, 44.000], mean action: 0.920 [0.000, 3.000],  loss: --, mse: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1071/10000: episode: 7, duration: 1.242s, episode steps: 301, steps per second: 242, episode reward: -7022.000, mean reward: -23.329 [-50.000, 144.000], mean action: 0.874 [0.000, 3.000],  loss: 1058.257135, mse: 11347.533936, mean_q: -73.215160\n",
      " 1159/10000: episode: 8, duration: 0.411s, episode steps:  88, steps per second: 214, episode reward: 184.000, mean reward:  2.091 [-50.000, 76.000], mean action: 1.989 [0.000, 3.000],  loss: 1060.338745, mse: 11814.556641, mean_q: -77.195229\n",
      " 1325/10000: episode: 9, duration: 0.765s, episode steps: 166, steps per second: 217, episode reward: -1186.000, mean reward: -7.145 [-50.000, 152.000], mean action: 1.181 [0.000, 3.000],  loss: 999.443237, mse: 11381.389648, mean_q: -74.397835\n",
      " 1512/10000: episode: 10, duration: 0.840s, episode steps: 187, steps per second: 223, episode reward: -1452.000, mean reward: -7.765 [-50.000, 152.000], mean action: 1.963 [0.000, 3.000],  loss: 939.545349, mse: 10579.209961, mean_q: -70.482445\n",
      " 1598/10000: episode: 11, duration: 0.392s, episode steps:  86, steps per second: 219, episode reward: 450.000, mean reward:  5.233 [-50.000, 68.000], mean action: 1.721 [0.000, 3.000],  loss: 1034.804199, mse: 11409.000977, mean_q: -76.189957\n",
      " 1650/10000: episode: 12, duration: 0.243s, episode steps:  52, steps per second: 214, episode reward: 222.000, mean reward:  4.269 [-50.000, 36.000], mean action: 1.942 [0.000, 3.000],  loss: 971.254333, mse: 11350.535156, mean_q: -75.665146\n",
      " 1850/10000: episode: 13, duration: 0.911s, episode steps: 200, steps per second: 219, episode reward: -3260.000, mean reward: -16.300 [-50.000, 76.000], mean action: 1.700 [0.000, 3.000],  loss: 975.197510, mse: 11576.362305, mean_q: -76.325432\n",
      " 2126/10000: episode: 14, duration: 1.224s, episode steps: 276, steps per second: 226, episode reward: -8822.000, mean reward: -31.964 [-50.000, 88.000], mean action: 2.377 [0.000, 3.000],  loss: 997.553650, mse: 12611.733398, mean_q: -82.265213\n",
      " 2200/10000: episode: 15, duration: 0.348s, episode steps:  74, steps per second: 213, episode reward: 162.000, mean reward:  2.189 [-50.000, 36.000], mean action: 1.932 [0.000, 3.000],  loss: 988.498413, mse: 14485.791992, mean_q: -92.283371\n",
      " 2354/10000: episode: 16, duration: 0.687s, episode steps: 154, steps per second: 224, episode reward: -1570.000, mean reward: -10.195 [-50.000, 76.000], mean action: 2.182 [0.000, 3.000],  loss: 954.367676, mse: 14855.752930, mean_q: -95.327881\n",
      " 2473/10000: episode: 17, duration: 0.540s, episode steps: 119, steps per second: 221, episode reward: -628.000, mean reward: -5.277 [-50.000, 100.000], mean action: 1.815 [0.000, 3.000],  loss: 917.276794, mse: 15624.534180, mean_q: -99.428833\n",
      " 2618/10000: episode: 18, duration: 0.684s, episode steps: 145, steps per second: 212, episode reward: -4056.000, mean reward: -27.972 [-50.000, 44.000], mean action: 2.483 [0.000, 3.000],  loss: 951.206238, mse: 16060.693359, mean_q: -101.376869\n",
      " 2680/10000: episode: 19, duration: 0.295s, episode steps:  62, steps per second: 210, episode reward: 306.000, mean reward:  4.935 [-50.000, 40.000], mean action: 1.548 [0.000, 3.000],  loss: 953.965515, mse: 17527.960938, mean_q: -108.959564\n",
      " 2844/10000: episode: 20, duration: 0.755s, episode steps: 164, steps per second: 217, episode reward: 152.000, mean reward:  0.927 [-50.000, 184.000], mean action: 1.555 [0.000, 3.000],  loss: 989.813904, mse: 17571.285156, mean_q: -108.482109\n",
      " 2978/10000: episode: 21, duration: 0.617s, episode steps: 134, steps per second: 217, episode reward: -1996.000, mean reward: -14.896 [-50.000, 76.000], mean action: 1.097 [0.000, 3.000],  loss: 988.038818, mse: 17423.814453, mean_q: -106.656151\n",
      " 3214/10000: episode: 22, duration: 1.077s, episode steps: 236, steps per second: 219, episode reward: -4294.000, mean reward: -18.195 [-50.000, 140.000], mean action: 0.754 [0.000, 3.000],  loss: 1056.481812, mse: 18227.279297, mean_q: -110.112183\n",
      " 3419/10000: episode: 23, duration: 0.918s, episode steps: 205, steps per second: 223, episode reward: -6204.000, mean reward: -30.263 [-50.000, 64.000], mean action: 0.678 [0.000, 3.000],  loss: 1013.423645, mse: 20677.609375, mean_q: -121.792458\n",
      " 3698/10000: episode: 24, duration: 1.251s, episode steps: 279, steps per second: 223, episode reward: -6434.000, mean reward: -23.061 [-50.000, 128.000], mean action: 1.072 [0.000, 3.000],  loss: 990.613098, mse: 22800.994141, mean_q: -132.280518\n",
      " 3747/10000: episode: 25, duration: 0.224s, episode steps:  49, steps per second: 219, episode reward: -54.000, mean reward: -1.102 [-50.000, 20.000], mean action: 1.633 [0.000, 3.000],  loss: 978.486450, mse: 25256.726562, mean_q: -141.783005\n",
      " 3801/10000: episode: 26, duration: 0.262s, episode steps:  54, steps per second: 206, episode reward: 296.000, mean reward:  5.481 [ 0.000, 40.000], mean action: 1.407 [0.000, 3.000],  loss: 899.106689, mse: 26686.070312, mean_q: -148.369324\n",
      " 3911/10000: episode: 27, duration: 0.495s, episode steps: 110, steps per second: 222, episode reward: -70.000, mean reward: -0.636 [-50.000, 92.000], mean action: 1.282 [0.000, 3.000],  loss: 983.896545, mse: 25619.501953, mean_q: -142.889175\n",
      " 3999/10000: episode: 28, duration: 0.403s, episode steps:  88, steps per second: 218, episode reward: -1230.000, mean reward: -13.977 [-50.000, 36.000], mean action: 1.114 [0.000, 3.000],  loss: 1013.271484, mse: 25246.230469, mean_q: -140.789459\n",
      " 4169/10000: episode: 29, duration: 0.777s, episode steps: 170, steps per second: 219, episode reward: -1598.000, mean reward: -9.400 [-50.000, 136.000], mean action: 1.724 [0.000, 3.000],  loss: 1073.906616, mse: 25470.544922, mean_q: -140.348572\n",
      " 4546/10000: episode: 30, duration: 1.694s, episode steps: 377, steps per second: 223, episode reward: -9724.000, mean reward: -25.793 [-50.000, 160.000], mean action: 2.082 [0.000, 3.000],  loss: 921.139282, mse: 28351.369141, mean_q: -151.398621\n",
      " 4832/10000: episode: 31, duration: 1.300s, episode steps: 286, steps per second: 220, episode reward: -6254.000, mean reward: -21.867 [-50.000, 148.000], mean action: 2.346 [0.000, 3.000],  loss: 1004.349243, mse: 33812.785156, mean_q: -168.914124\n",
      " 5080/10000: episode: 32, duration: 1.129s, episode steps: 248, steps per second: 220, episode reward: -4810.000, mean reward: -19.395 [-50.000, 132.000], mean action: 2.452 [0.000, 3.000],  loss: 997.602844, mse: 37508.445312, mean_q: -181.631302\n",
      " 5169/10000: episode: 33, duration: 0.423s, episode steps:  89, steps per second: 210, episode reward: -1446.000, mean reward: -16.247 [-50.000, 40.000], mean action: 2.034 [0.000, 3.000],  loss: 1073.104858, mse: 40358.792969, mean_q: -191.745728\n",
      " 5372/10000: episode: 34, duration: 0.930s, episode steps: 203, steps per second: 218, episode reward: -3244.000, mean reward: -15.980 [-50.000, 76.000], mean action: 2.039 [0.000, 3.000],  loss: 972.033386, mse: 40074.183594, mean_q: -189.727386\n",
      " 5479/10000: episode: 35, duration: 0.494s, episode steps: 107, steps per second: 217, episode reward: -770.000, mean reward: -7.196 [-50.000, 72.000], mean action: 1.813 [0.000, 3.000],  loss: 966.842773, mse: 39883.050781, mean_q: -188.855804\n",
      " 5693/10000: episode: 36, duration: 0.939s, episode steps: 214, steps per second: 228, episode reward: -5360.000, mean reward: -25.047 [-50.000, 76.000], mean action: 0.682 [0.000, 3.000],  loss: 1105.078857, mse: 39614.695312, mean_q: -185.557938\n",
      " 5790/10000: episode: 37, duration: 0.436s, episode steps:  97, steps per second: 222, episode reward: -696.000, mean reward: -7.175 [-50.000, 64.000], mean action: 1.412 [0.000, 3.000],  loss: 1046.967407, mse: 40366.558594, mean_q: -187.517502\n",
      " 5846/10000: episode: 38, duration: 0.267s, episode steps:  56, steps per second: 210, episode reward: 250.000, mean reward:  4.464 [-50.000, 36.000], mean action: 1.571 [0.000, 3.000],  loss: 899.265320, mse: 40867.570312, mean_q: -189.532196\n",
      " 5912/10000: episode: 39, duration: 0.301s, episode steps:  66, steps per second: 219, episode reward: 358.000, mean reward:  5.424 [-50.000, 36.000], mean action: 1.364 [0.000, 3.000],  loss: 891.385132, mse: 42031.066406, mean_q: -192.919571\n",
      " 6033/10000: episode: 40, duration: 0.546s, episode steps: 121, steps per second: 222, episode reward: -1680.000, mean reward: -13.884 [-50.000, 64.000], mean action: 1.802 [0.000, 3.000],  loss: 1047.515137, mse: 41021.527344, mean_q: -188.522537\n",
      " 6101/10000: episode: 41, duration: 0.309s, episode steps:  68, steps per second: 220, episode reward: -222.000, mean reward: -3.265 [-50.000, 40.000], mean action: 0.956 [0.000, 3.000],  loss: 905.531128, mse: 42184.230469, mean_q: -192.088440\n",
      " 6506/10000: episode: 42, duration: 1.809s, episode steps: 405, steps per second: 224, episode reward: -7360.000, mean reward: -18.173 [-50.000, 256.000], mean action: 0.847 [0.000, 3.000],  loss: 1045.195312, mse: 43184.816406, mean_q: -193.338928\n",
      " 6791/10000: episode: 43, duration: 1.283s, episode steps: 285, steps per second: 222, episode reward: -7784.000, mean reward: -27.312 [-50.000, 148.000], mean action: 1.333 [0.000, 3.000],  loss: 989.767761, mse: 43249.046875, mean_q: -194.875870\n",
      " 6927/10000: episode: 44, duration: 0.601s, episode steps: 136, steps per second: 226, episode reward: -2382.000, mean reward: -17.515 [-50.000, 64.000], mean action: 1.985 [0.000, 3.000],  loss: 1132.810059, mse: 43537.281250, mean_q: -195.316864\n",
      " 7027/10000: episode: 45, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -1066.000, mean reward: -10.660 [-50.000, 84.000], mean action: 1.810 [0.000, 3.000],  loss: 1120.968384, mse: 42563.726562, mean_q: -191.802673\n",
      " 7199/10000: episode: 46, duration: 0.765s, episode steps: 172, steps per second: 225, episode reward: -2242.000, mean reward: -13.035 [-50.000, 148.000], mean action: 1.663 [0.000, 3.000],  loss: 1103.243286, mse: 43760.335938, mean_q: -194.535889\n",
      " 7295/10000: episode: 47, duration: 0.500s, episode steps:  96, steps per second: 192, episode reward: -448.000, mean reward: -4.667 [-50.000, 64.000], mean action: 1.656 [0.000, 3.000],  loss: 1289.342529, mse: 46439.328125, mean_q: -201.911880\n",
      " 7582/10000: episode: 48, duration: 1.301s, episode steps: 287, steps per second: 221, episode reward: -6656.000, mean reward: -23.192 [-50.000, 136.000], mean action: 1.843 [0.000, 3.000],  loss: 1192.733032, mse: 47391.371094, mean_q: -205.071335\n",
      " 7680/10000: episode: 49, duration: 0.452s, episode steps:  98, steps per second: 217, episode reward: -158.000, mean reward: -1.612 [-50.000, 80.000], mean action: 1.449 [0.000, 3.000],  loss: 1258.514282, mse: 50016.328125, mean_q: -211.330063\n",
      " 7934/10000: episode: 50, duration: 1.133s, episode steps: 254, steps per second: 224, episode reward: -4816.000, mean reward: -18.961 [-50.000, 128.000], mean action: 1.524 [0.000, 3.000],  loss: 1222.266479, mse: 51782.980469, mean_q: -216.802094\n",
      " 8025/10000: episode: 51, duration: 0.408s, episode steps:  91, steps per second: 223, episode reward: 378.000, mean reward:  4.154 [-50.000, 92.000], mean action: 1.659 [0.000, 3.000],  loss: 1291.514038, mse: 55857.863281, mean_q: -228.252762\n",
      " 8122/10000: episode: 52, duration: 0.435s, episode steps:  97, steps per second: 223, episode reward: -394.000, mean reward: -4.062 [-50.000, 72.000], mean action: 1.010 [0.000, 3.000],  loss: 1280.894043, mse: 54890.839844, mean_q: -224.652527\n",
      " 8189/10000: episode: 53, duration: 0.314s, episode steps:  67, steps per second: 213, episode reward: 404.000, mean reward:  6.030 [ 0.000, 40.000], mean action: 1.507 [0.000, 3.000],  loss: 1363.069824, mse: 54594.968750, mean_q: -222.969681\n",
      " 8319/10000: episode: 54, duration: 0.584s, episode steps: 130, steps per second: 223, episode reward: -1096.000, mean reward: -8.431 [-50.000, 80.000], mean action: 1.608 [0.000, 3.000],  loss: 1197.148804, mse: 55646.484375, mean_q: -224.649475\n",
      " 8407/10000: episode: 55, duration: 0.396s, episode steps:  88, steps per second: 222, episode reward: 610.000, mean reward:  6.932 [-50.000, 80.000], mean action: 1.273 [0.000, 3.000],  loss: 1246.338379, mse: 53279.515625, mean_q: -217.098724\n",
      " 8610/10000: episode: 56, duration: 0.919s, episode steps: 203, steps per second: 221, episode reward: -3042.000, mean reward: -14.985 [-50.000, 144.000], mean action: 1.601 [0.000, 3.000],  loss: 1406.062622, mse: 53317.433594, mean_q: -217.375961\n",
      " 8905/10000: episode: 57, duration: 1.317s, episode steps: 295, steps per second: 224, episode reward: -5014.000, mean reward: -16.997 [-50.000, 288.000], mean action: 1.624 [0.000, 3.000],  loss: 1374.055664, mse: 55942.808594, mean_q: -223.911911\n",
      " 8951/10000: episode: 58, duration: 0.209s, episode steps:  46, steps per second: 220, episode reward: 134.000, mean reward:  2.913 [-50.000, 24.000], mean action: 1.087 [0.000, 3.000],  loss: 1589.970459, mse: 57404.558594, mean_q: -225.907516\n",
      " 9078/10000: episode: 59, duration: 0.581s, episode steps: 127, steps per second: 219, episode reward: -1318.000, mean reward: -10.378 [-50.000, 72.000], mean action: 1.709 [0.000, 3.000],  loss: 1370.476929, mse: 59405.937500, mean_q: -232.198120\n",
      " 9304/10000: episode: 60, duration: 1.013s, episode steps: 226, steps per second: 223, episode reward: -4092.000, mean reward: -18.106 [-50.000, 76.000], mean action: 0.894 [0.000, 3.000],  loss: 1346.556641, mse: 60658.796875, mean_q: -233.920929\n",
      " 9691/10000: episode: 61, duration: 1.734s, episode steps: 387, steps per second: 223, episode reward: -12744.000, mean reward: -32.930 [-50.000, 64.000], mean action: 1.385 [0.000, 3.000],  loss: 1491.864990, mse: 67541.789062, mean_q: -251.367294\n",
      " 9752/10000: episode: 62, duration: 0.297s, episode steps:  61, steps per second: 205, episode reward: 332.000, mean reward:  5.443 [ 0.000, 44.000], mean action: 1.689 [0.000, 3.000],  loss: 1581.469727, mse: 75993.179688, mean_q: -271.005493\n",
      " 9956/10000: episode: 63, duration: 0.969s, episode steps: 204, steps per second: 211, episode reward: -5384.000, mean reward: -26.392 [-50.000, 84.000], mean action: 1.750 [0.000, 3.000],  loss: 1340.667847, mse: 74147.601562, mean_q: -263.984863\n",
      "done, took 42.629 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4944e2d60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.compile(optimizer=RMSprop(learning_rate=0.01, epsilon=0.0000001), metrics=[\"mse\"])\n",
    "dqn.fit(env=env, nb_steps=2000, visualize=False, verbose=2)\n",
    "\n",
    "dqn.compile(optimizer=RMSprop(learning_rate=0.001, epsilon=0.0000001), metrics=[\"mse\"])\n",
    "dqn.fit(env=env, nb_steps=10000, visualize=False, verbose=2)\n",
    "\n",
    "dqn.compile(optimizer=RMSprop(learning_rate=0.0001, epsilon=0.0000001), metrics=[\"mse\"])\n",
    "dqn.fit(env=env, nb_steps=10000, visualize=False, verbose=2)\n",
    "\n",
    "# dqn.compile(optimizer=RMSprop(learning_rate=0.00001, epsilon=0.0000001), metrics=[\"mse\"])\n",
    "# dqn.fit(env=env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10 is over.\n",
      "Game 20 is over.\n",
      "Game 30 is over.\n",
      "Game 40 is over.\n",
      "Game 50 is over.\n",
      "Game 60 is over.\n",
      "Game 70 is over.\n",
      "Game 80 is over.\n",
      "Game 90 is over.\n",
      "Game 100 is over.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>higherTile</th>\n",
       "      <th>invalidMove (%)</th>\n",
       "      <th>steps</th>\n",
       "      <th>upMoves</th>\n",
       "      <th>downMoves</th>\n",
       "      <th>leftMoves</th>\n",
       "      <th>rightMoves</th>\n",
       "      <th>upSeqAvg</th>\n",
       "      <th>downSeqAvg</th>\n",
       "      <th>leftSeqAvg</th>\n",
       "      <th>rightSeqAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>803.960000</td>\n",
       "      <td>83.200000</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>157.330000</td>\n",
       "      <td>40.790000</td>\n",
       "      <td>44.57000</td>\n",
       "      <td>27.41000</td>\n",
       "      <td>44.560000</td>\n",
       "      <td>1.371752</td>\n",
       "      <td>1.533670</td>\n",
       "      <td>1.377408</td>\n",
       "      <td>1.74171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>511.688824</td>\n",
       "      <td>55.843938</td>\n",
       "      <td>0.209667</td>\n",
       "      <td>113.790882</td>\n",
       "      <td>41.797321</td>\n",
       "      <td>47.30457</td>\n",
       "      <td>26.16154</td>\n",
       "      <td>44.314377</td>\n",
       "      <td>0.749537</td>\n",
       "      <td>0.887672</td>\n",
       "      <td>1.032266</td>\n",
       "      <td>1.03559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>512.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.086990</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.044091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.03125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>650.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.279085</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>1.045549</td>\n",
       "      <td>1.111046</td>\n",
       "      <td>1.040833</td>\n",
       "      <td>1.17617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1026.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.446970</td>\n",
       "      <td>185.750000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>48.25000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>1.235577</td>\n",
       "      <td>1.683333</td>\n",
       "      <td>1.123575</td>\n",
       "      <td>2.07000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2876.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.720060</td>\n",
       "      <td>668.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>297.00000</td>\n",
       "      <td>166.00000</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>4.433962</td>\n",
       "      <td>5.033898</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>5.25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score  higherTile  invalidMove (%)       steps     upMoves  \\\n",
       "count   100.000000  100.000000       100.000000  100.000000  100.000000   \n",
       "mean    803.960000   83.200000         0.284232  157.330000   40.790000   \n",
       "std     511.688824   55.843938         0.209667  113.790882   41.797321   \n",
       "min     140.000000   16.000000         0.000000   45.000000   10.000000   \n",
       "25%     512.000000   64.000000         0.086990   88.250000   20.000000   \n",
       "50%     650.000000   64.000000         0.279085  122.000000   27.000000   \n",
       "75%    1026.000000  128.000000         0.446970  185.750000   42.500000   \n",
       "max    2876.000000  256.000000         0.720060  668.000000  264.000000   \n",
       "\n",
       "       downMoves  leftMoves  rightMoves    upSeqAvg  downSeqAvg  leftSeqAvg  \\\n",
       "count  100.00000  100.00000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean    44.57000   27.41000   44.560000    1.371752    1.533670    1.377408   \n",
       "std     47.30457   26.16154   44.314377    0.749537    0.887672    1.032266   \n",
       "min     11.00000    2.00000    4.000000    1.000000    1.000000    1.000000   \n",
       "25%     21.00000   14.00000   19.750000    1.000000    1.044091    1.000000   \n",
       "50%     28.00000   20.00000   27.500000    1.045549    1.111046    1.040833   \n",
       "75%     48.25000   29.00000   51.750000    1.235577    1.683333    1.123575   \n",
       "max    297.00000  166.00000  273.000000    4.433962    5.033898    6.636364   \n",
       "\n",
       "       rightSeqAvg  \n",
       "count    100.00000  \n",
       "mean       1.74171  \n",
       "std        1.03559  \n",
       "min        1.00000  \n",
       "25%        1.03125  \n",
       "50%        1.17617  \n",
       "75%        2.07000  \n",
       "max        5.25000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "def log2_transform(grid):\n",
    "    transformed = np.log2(np.where(grid > 0, grid, 1))\n",
    "    return transformed\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(100)]\n",
    "metrics = {\n",
    "    \"gameId\": [],\n",
    "    \"score\": [],\n",
    "    \"higherTile\": [],\n",
    "    \"invalidMove (%)\": [],\n",
    "    \"steps\": [],\n",
    "    \"upMoves\": [],\n",
    "    \"downMoves\": [],\n",
    "    \"leftMoves\": [],\n",
    "    \"rightMoves\": [],\n",
    "    \"upSeqAvg\": [],\n",
    "    \"downSeqAvg\": [],\n",
    "    \"leftSeqAvg\": [],\n",
    "    \"rightSeqAvg\": [],\n",
    "}\n",
    "for i, game in enumerate(games):\n",
    "    # Metrics\n",
    "    steps = 0\n",
    "    invalidMove = 0\n",
    "    upMoves = 0\n",
    "    downMoves = 0\n",
    "    leftMoves = 0\n",
    "    rightMoves = 0\n",
    "    upSeqAvg = 0\n",
    "    downSeqAvg = 0\n",
    "    leftSeqAvg = 0\n",
    "    rightSeqAvg = 0\n",
    "\n",
    "    sequences = {\n",
    "        \"up\": [],\n",
    "        \"down\": [],\n",
    "        \"left\": [],\n",
    "        \"right\": []\n",
    "    }\n",
    "    lastMove = None\n",
    "\n",
    "    # print(f\"Game {i + 1} initial state:\")\n",
    "    # game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        observation = log2_transform(observation)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        # print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        if not game.move(actionMapping[predictedAction]):\n",
    "            invalidMove += 1\n",
    "        \n",
    "        # print(f\"Game {i + 1} currrent state:\")\n",
    "        # game.render()\n",
    "\n",
    "        # input(\"\\nPress any key to continue...\")\n",
    "\n",
    "        steps += 1\n",
    "        if actionMapping[predictedAction] == \"up\":\n",
    "            upMoves += 1\n",
    "            if lastMove == \"up\":\n",
    "                sequences[\"up\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"up\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"down\":\n",
    "            downMoves += 1\n",
    "            if lastMove == \"down\":\n",
    "                sequences[\"down\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"down\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"left\":\n",
    "            leftMoves += 1\n",
    "            if lastMove == \"left\":\n",
    "                sequences[\"left\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"left\"].append(1)\n",
    "        elif actionMapping[predictedAction] == \"right\":\n",
    "            rightMoves += 1\n",
    "            if lastMove == \"right\":\n",
    "                sequences[\"right\"][-1] += 1\n",
    "            else:\n",
    "                sequences[\"right\"].append(1)\n",
    "        \n",
    "        lastMove = actionMapping[predictedAction]\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "    metrics[\"gameId\"].append(i)\n",
    "    metrics[\"score\"].append(game.score)\n",
    "    metrics[\"higherTile\"].append(np.max(game.grid))\n",
    "    metrics[\"invalidMove (%)\"].append(invalidMove/steps)\n",
    "    metrics[\"steps\"].append(steps)\n",
    "    metrics[\"upMoves\"].append(upMoves)\n",
    "    metrics[\"downMoves\"].append(downMoves)\n",
    "    metrics[\"leftMoves\"].append(leftMoves)\n",
    "    metrics[\"rightMoves\"].append(rightMoves)\n",
    "    metrics[\"upSeqAvg\"].append(np.mean(np.array(sequences[\"up\"])))\n",
    "    metrics[\"downSeqAvg\"].append(np.mean(np.array(sequences[\"down\"])))\n",
    "    metrics[\"leftSeqAvg\"].append(np.mean(np.array(sequences[\"left\"])))\n",
    "    metrics[\"rightSeqAvg\"].append(np.mean(np.array(sequences[\"right\"])))\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics).set_index(\"gameId\")\n",
    "# display(metrics)\n",
    "display(metrics.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [\"up\", \"left\", \"right\", \"down\"]\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
