{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 13:52:58.100856: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-09 13:52:58.100893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "  456/5000: episode: 1, duration: 2.026s, episode steps: 456, steps per second: 225, episode reward: 1476.000, mean reward:  3.237 [ 0.000, 152.000], mean action: 1.596 [0.000, 3.000],  loss: 73.984195, mae: 32.208742, mean_q: 61.612010\n",
      "  813/5000: episode: 2, duration: 1.669s, episode steps: 357, steps per second: 214, episode reward: 1828.000, mean reward:  5.120 [ 0.000, 152.000], mean action: 1.429 [0.000, 3.000],  loss: 149.498032, mae: 122.091827, mean_q: 176.068237\n",
      " 1034/5000: episode: 3, duration: 1.016s, episode steps: 221, steps per second: 217, episode reward: 612.000, mean reward:  2.769 [ 0.000, 72.000], mean action: 1.534 [0.000, 3.000],  loss: 155.434280, mae: 88.493912, mean_q: 142.728912\n",
      " 1358/5000: episode: 4, duration: 1.544s, episode steps: 324, steps per second: 210, episode reward: 1640.000, mean reward:  5.062 [ 0.000, 132.000], mean action: 1.694 [0.000, 3.000],  loss: 129.850006, mae: 145.126968, mean_q: 225.303650\n",
      " 1527/5000: episode: 5, duration: 0.777s, episode steps: 169, steps per second: 218, episode reward: 700.000, mean reward:  4.142 [ 0.000, 104.000], mean action: 1.195 [0.000, 3.000],  loss: 370.141083, mae: 130.155136, mean_q: 191.203339\n",
      " 1779/5000: episode: 6, duration: 1.187s, episode steps: 252, steps per second: 212, episode reward: 1344.000, mean reward:  5.333 [ 0.000, 136.000], mean action: 1.718 [0.000, 3.000],  loss: 159.502792, mae: 153.492722, mean_q: 217.441422\n",
      " 1953/5000: episode: 7, duration: 0.818s, episode steps: 174, steps per second: 213, episode reward: 596.000, mean reward:  3.425 [ 0.000, 88.000], mean action: 2.063 [0.000, 3.000],  loss: 500.850708, mae: 143.638992, mean_q: 202.159622\n",
      " 2433/5000: episode: 8, duration: 2.203s, episode steps: 480, steps per second: 218, episode reward: 1252.000, mean reward:  2.608 [ 0.000, 140.000], mean action: 1.131 [0.000, 3.000],  loss: 133.224686, mae: 216.950409, mean_q: 318.980957\n",
      " 2620/5000: episode: 9, duration: 0.869s, episode steps: 187, steps per second: 215, episode reward: 768.000, mean reward:  4.107 [ 0.000, 72.000], mean action: 1.321 [0.000, 3.000],  loss: 365.350067, mae: 199.478561, mean_q: 298.743591\n",
      " 2891/5000: episode: 10, duration: 1.238s, episode steps: 271, steps per second: 219, episode reward: 816.000, mean reward:  3.011 [ 0.000, 64.000], mean action: 1.129 [0.000, 3.000],  loss: 363.656250, mae: 216.783936, mean_q: 306.212860\n",
      " 3063/5000: episode: 11, duration: 0.783s, episode steps: 172, steps per second: 220, episode reward: 932.000, mean reward:  5.419 [ 0.000, 96.000], mean action: 1.302 [0.000, 3.000],  loss: 609.796509, mae: 208.236664, mean_q: 298.269409\n",
      " 3224/5000: episode: 12, duration: 0.734s, episode steps: 161, steps per second: 219, episode reward: 264.000, mean reward:  1.640 [ 0.000, 36.000], mean action: 0.292 [0.000, 3.000],  loss: 619.866394, mae: 151.081146, mean_q: 225.621307\n",
      " 3680/5000: episode: 13, duration: 2.127s, episode steps: 456, steps per second: 214, episode reward: 2228.000, mean reward:  4.886 [ 0.000, 260.000], mean action: 1.783 [0.000, 3.000],  loss: 302.777649, mae: 321.058807, mean_q: 473.804474\n",
      " 3883/5000: episode: 14, duration: 0.943s, episode steps: 203, steps per second: 215, episode reward: 1100.000, mean reward:  5.419 [ 0.000, 128.000], mean action: 1.665 [0.000, 3.000],  loss: 2216.046143, mae: 335.290192, mean_q: 468.456940\n",
      " 4045/5000: episode: 15, duration: 0.761s, episode steps: 162, steps per second: 213, episode reward: 716.000, mean reward:  4.420 [ 0.000, 76.000], mean action: 1.463 [0.000, 3.000],  loss: 693.497681, mae: 264.145660, mean_q: 372.318420\n",
      " 4488/5000: episode: 16, duration: 2.049s, episode steps: 443, steps per second: 216, episode reward: 1228.000, mean reward:  2.772 [ 0.000, 144.000], mean action: 1.795 [0.000, 3.000],  loss: 511.813202, mae: 320.014221, mean_q: 446.040070\n",
      " 4674/5000: episode: 17, duration: 0.844s, episode steps: 186, steps per second: 220, episode reward: 568.000, mean reward:  3.054 [ 0.000, 64.000], mean action: 0.677 [0.000, 3.000],  loss: 1442.816528, mae: 293.450470, mean_q: 412.426758\n",
      " 4962/5000: episode: 18, duration: 1.325s, episode steps: 288, steps per second: 217, episode reward: 624.000, mean reward:  2.167 [ 0.000, 76.000], mean action: 1.497 [0.000, 3.000],  loss: 374.071350, mae: 306.465240, mean_q: 471.051208\n",
      "done, took 23.094 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0342dddf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "temporalWindow = 5\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "replayMemory = SequentialMemory(limit=100, window_length=temporalWindow)\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=100,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=5000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the game with predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = ['up', 'left', 'right', 'down']\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
