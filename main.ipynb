{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 14:07:20.110974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-15 14:07:20.111015: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2025-03-15 14:07:26.017082: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-03-15 14:07:26.017118: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-15 14:07:26.017135: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (623738dcea4e): /proc/driver/nvidia/version does not exist\n",
      "2025-03-15 14:07:26.017360: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-15 14:07:26.023707: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  391/5000: episode: 1, duration: 1.628s, episode steps: 391, steps per second: 240, episode reward: 1500.000, mean reward:  3.836 [ 0.000, 128.000], mean action: 1.143 [0.000, 3.000],  loss: 73.696651, mae: 31.797598, mean_q: 54.681725\n",
      "  806/5000: episode: 2, duration: 1.841s, episode steps: 415, steps per second: 225, episode reward: 1392.000, mean reward:  3.354 [ 0.000, 132.000], mean action: 1.482 [0.000, 3.000],  loss: 48.198509, mae: 60.740032, mean_q: 89.795158\n",
      "  874/5000: episode: 3, duration: 0.307s, episode steps:  68, steps per second: 221, episode reward: 264.000, mean reward:  3.882 [ 0.000, 44.000], mean action: 1.971 [0.000, 3.000],  loss: 117.348938, mae: 71.557304, mean_q: 100.110153\n",
      " 1049/5000: episode: 4, duration: 0.781s, episode steps: 175, steps per second: 224, episode reward: 568.000, mean reward:  3.246 [ 0.000, 68.000], mean action: 0.926 [0.000, 3.000],  loss: 49.945045, mae: 37.616653, mean_q: 59.433437\n",
      " 1216/5000: episode: 5, duration: 0.741s, episode steps: 167, steps per second: 226, episode reward: 1020.000, mean reward:  6.108 [ 0.000, 132.000], mean action: 1.275 [0.000, 3.000],  loss: 110.766350, mae: 58.122017, mean_q: 96.313789\n",
      " 1678/5000: episode: 6, duration: 2.030s, episode steps: 462, steps per second: 228, episode reward: 1408.000, mean reward:  3.048 [ 0.000, 148.000], mean action: 1.472 [0.000, 3.000],  loss: 75.736374, mae: 111.211487, mean_q: 164.160446\n",
      " 1817/5000: episode: 7, duration: 0.621s, episode steps: 139, steps per second: 224, episode reward: 740.000, mean reward:  5.324 [ 0.000, 112.000], mean action: 1.741 [0.000, 3.000],  loss: 257.030060, mae: 103.652214, mean_q: 148.917694\n",
      " 2042/5000: episode: 8, duration: 1.029s, episode steps: 225, steps per second: 219, episode reward: 1144.000, mean reward:  5.084 [ 0.000, 128.000], mean action: 1.236 [0.000, 3.000],  loss: 259.463287, mae: 153.749207, mean_q: 223.070831\n",
      " 2402/5000: episode: 9, duration: 1.583s, episode steps: 360, steps per second: 227, episode reward: 1620.000, mean reward:  4.500 [ 0.000, 136.000], mean action: 2.136 [0.000, 3.000],  loss: 462.293365, mae: 235.518494, mean_q: 344.143921\n",
      " 2765/5000: episode: 10, duration: 1.600s, episode steps: 363, steps per second: 227, episode reward: 1048.000, mean reward:  2.887 [ 0.000, 132.000], mean action: 1.504 [0.000, 3.000],  loss: 250.473480, mae: 175.847305, mean_q: 258.051910\n",
      " 2954/5000: episode: 11, duration: 0.841s, episode steps: 189, steps per second: 225, episode reward: 888.000, mean reward:  4.698 [ 0.000, 72.000], mean action: 1.402 [0.000, 3.000],  loss: 170.624054, mae: 184.908447, mean_q: 280.755676\n",
      " 3109/5000: episode: 12, duration: 0.681s, episode steps: 155, steps per second: 228, episode reward: 488.000, mean reward:  3.148 [ 0.000, 68.000], mean action: 2.342 [0.000, 3.000],  loss: 770.576965, mae: 169.391281, mean_q: 264.423645\n",
      " 3291/5000: episode: 13, duration: 0.801s, episode steps: 182, steps per second: 227, episode reward: 644.000, mean reward:  3.538 [ 0.000, 64.000], mean action: 1.929 [0.000, 3.000],  loss: 228.920349, mae: 167.565826, mean_q: 259.176514\n",
      " 3740/5000: episode: 14, duration: 1.962s, episode steps: 449, steps per second: 229, episode reward: 1776.000, mean reward:  3.955 [ 0.000, 208.000], mean action: 1.931 [0.000, 3.000],  loss: 273.549255, mae: 345.308563, mean_q: 528.408020\n",
      " 3902/5000: episode: 15, duration: 0.719s, episode steps: 162, steps per second: 225, episode reward: 524.000, mean reward:  3.235 [ 0.000, 52.000], mean action: 1.086 [0.000, 3.000],  loss: 2886.077637, mae: 316.916473, mean_q: 453.796631\n",
      " 4085/5000: episode: 16, duration: 0.806s, episode steps: 183, steps per second: 227, episode reward: 276.000, mean reward:  1.508 [ 0.000, 52.000], mean action: 1.678 [0.000, 3.000],  loss: 443.148651, mae: 193.327469, mean_q: 288.433075\n",
      " 4190/5000: episode: 17, duration: 0.480s, episode steps: 105, steps per second: 219, episode reward: 376.000, mean reward:  3.581 [ 0.000, 52.000], mean action: 2.362 [0.000, 3.000],  loss: 644.802368, mae: 168.312195, mean_q: 247.120132\n",
      " 4392/5000: episode: 18, duration: 0.915s, episode steps: 202, steps per second: 221, episode reward: 516.000, mean reward:  2.554 [ 0.000, 72.000], mean action: 1.634 [0.000, 3.000],  loss: 365.204559, mae: 190.443146, mean_q: 278.900421\n",
      " 4508/5000: episode: 19, duration: 0.514s, episode steps: 116, steps per second: 226, episode reward: 592.000, mean reward:  5.103 [ 0.000, 96.000], mean action: 1.750 [0.000, 3.000],  loss: 610.028137, mae: 208.159119, mean_q: 299.515961\n",
      " 4694/5000: episode: 20, duration: 0.824s, episode steps: 186, steps per second: 226, episode reward: 1140.000, mean reward:  6.129 [ 0.000, 132.000], mean action: 1.419 [0.000, 3.000],  loss: 718.475281, mae: 298.800079, mean_q: 432.957458\n",
      " 4975/5000: episode: 21, duration: 1.242s, episode steps: 281, steps per second: 226, episode reward: 548.000, mean reward:  1.950 [ 0.000, 72.000], mean action: 1.858 [0.000, 3.000],  loss: 1677.255005, mae: 324.739655, mean_q: 453.558716\n",
      "done, took 22.067 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46f7c366d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "temporalWindow = 5\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "replayMemory = SequentialMemory(limit=100, window_length=temporalWindow)\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=100,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=5000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "actionMapping = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)[0]\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        \n",
    "        print(f\"\\nPredicted Action: move {actionMapping[predictedAction]}\")\n",
    "        game.move(actionMapping[predictedAction])\n",
    "        \n",
    "        print(f\"Game {i + 1} currrent state:\")\n",
    "        game.render()\n",
    "\n",
    "        input(\"\\nPress any key to continue...\")\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = ['up', 'left', 'right', 'down']\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
