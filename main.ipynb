{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game2048\n",
    "from game_env import Game2048Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo de NN por reforço\n",
    "    # Métodos\n",
    "        # Q-Learning\n",
    "        # Monte Carlo\n",
    "        # Gradiente de políticas\n",
    "        # Aprendizado por diferença temporal\n",
    "        # Trust Region Policy Optimization (TRPO)\n",
    "# Definir parâmetros\n",
    "    # Grid do jogo\n",
    "# Definir penalidades\n",
    "    # Tentar mover para uma direção sem movimentos válidos\n",
    "    # Valores iguais/próximos muito dispersos? (viés?)\n",
    "        # Aplicar penalidade apenas quando saiu do range (até 2 de distância?)\n",
    "        # Verificar valores disponíveis no grid e contabilizar como valor próximo se estiver a até (3?) valores de distância\n",
    "        # Não penalizar se valor for 2 ou 4.\n",
    "# Definir recompensas\n",
    "    # Juntar tiles\n",
    "    # Manter valores da sequência em fileira ou próximos? (viés?)\n",
    "    # \n",
    "# Definir estratégia de treino\n",
    "    # Utilizar jogo já iniciado com mais tiles no grid? (aumentar aprendizado inicial sobre a regra do jogo)\n",
    "        # Treinar o mesmo jogo com rotações diferentes\n",
    "\n",
    "# Material:\n",
    "# https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "# https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago\n",
    "# https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "# https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 14:58:42.135204: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-12 14:58:42.201763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-12 14:58:42.201784: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow==2.9.0 keras-rl2\n",
    "# pip uninstall numpy -y\n",
    "# pip install numpy==1.24.3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2025-03-12 15:04:45.737905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-03-12 15:04:45.737971: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-12 15:04:45.737990: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (923b5dc5ef02): /proc/driver/nvidia/version does not exist\n",
      "2025-03-12 15:04:45.738867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-12 15:04:45.752653: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  146/5000: episode: 1, duration: 0.764s, episode steps: 146, steps per second: 191, episode reward: 288.000, mean reward:  1.973 [ 0.000, 32.000], mean action: 2.726 [0.000, 3.000],  loss: 17.230431, mae: 5.464291, mean_q: 12.271279\n",
      "  445/5000: episode: 2, duration: 1.824s, episode steps: 299, steps per second: 164, episode reward: 1244.000, mean reward:  4.161 [ 0.000, 132.000], mean action: 1.656 [0.000, 3.000],  loss: 57.038853, mae: 20.151827, mean_q: 32.551838\n",
      "  823/5000: episode: 3, duration: 2.299s, episode steps: 378, steps per second: 164, episode reward: 928.000, mean reward:  2.455 [ 0.000, 72.000], mean action: 1.548 [0.000, 3.000],  loss: 42.011642, mae: 47.876934, mean_q: 73.160934\n",
      " 1077/5000: episode: 4, duration: 1.523s, episode steps: 254, steps per second: 167, episode reward: 964.000, mean reward:  3.795 [ 0.000, 76.000], mean action: 1.035 [0.000, 3.000],  loss: 51.596786, mae: 52.684246, mean_q: 78.994682\n",
      " 1309/5000: episode: 5, duration: 1.377s, episode steps: 232, steps per second: 168, episode reward: 684.000, mean reward:  2.948 [ 0.000, 76.000], mean action: 1.379 [0.000, 3.000],  loss: 81.902435, mae: 67.350288, mean_q: 104.032730\n",
      " 1594/5000: episode: 6, duration: 1.707s, episode steps: 285, steps per second: 167, episode reward: 1204.000, mean reward:  4.225 [ 0.000, 168.000], mean action: 0.888 [0.000, 3.000],  loss: 140.456696, mae: 108.645065, mean_q: 156.696533\n",
      " 1845/5000: episode: 7, duration: 1.431s, episode steps: 251, steps per second: 175, episode reward: 1180.000, mean reward:  4.701 [ 0.000, 128.000], mean action: 1.614 [0.000, 3.000],  loss: 220.327530, mae: 127.752083, mean_q: 187.316696\n",
      " 2144/5000: episode: 8, duration: 1.659s, episode steps: 299, steps per second: 180, episode reward: 1004.000, mean reward:  3.358 [ 0.000, 72.000], mean action: 1.124 [0.000, 3.000],  loss: 152.546646, mae: 172.952469, mean_q: 246.397049\n",
      " 2255/5000: episode: 9, duration: 0.613s, episode steps: 111, steps per second: 181, episode reward: 616.000, mean reward:  5.550 [ 0.000, 80.000], mean action: 1.414 [0.000, 3.000],  loss: 1061.737915, mae: 185.202698, mean_q: 254.814865\n",
      " 2659/5000: episode: 10, duration: 2.281s, episode steps: 404, steps per second: 177, episode reward: 1616.000, mean reward:  4.000 [ 0.000, 164.000], mean action: 1.480 [0.000, 3.000],  loss: 151.467606, mae: 197.466965, mean_q: 284.361542\n",
      " 2929/5000: episode: 11, duration: 1.759s, episode steps: 270, steps per second: 154, episode reward: 1300.000, mean reward:  4.815 [ 0.000, 128.000], mean action: 1.152 [0.000, 3.000],  loss: 530.467102, mae: 224.952667, mean_q: 330.727112\n",
      " 3108/5000: episode: 12, duration: 1.257s, episode steps: 179, steps per second: 142, episode reward: 636.000, mean reward:  3.553 [ 0.000, 124.000], mean action: 1.553 [0.000, 3.000],  loss: 397.528503, mae: 177.988983, mean_q: 283.267365\n",
      " 3284/5000: episode: 13, duration: 1.063s, episode steps: 176, steps per second: 166, episode reward: 1296.000, mean reward:  7.364 [ 0.000, 152.000], mean action: 1.841 [0.000, 3.000],  loss: 671.762512, mae: 195.465378, mean_q: 288.620331\n",
      " 3490/5000: episode: 14, duration: 1.290s, episode steps: 206, steps per second: 160, episode reward: 704.000, mean reward:  3.417 [ 0.000, 76.000], mean action: 2.204 [0.000, 3.000],  loss: 557.424683, mae: 242.893936, mean_q: 358.200531\n",
      " 3787/5000: episode: 15, duration: 1.780s, episode steps: 297, steps per second: 167, episode reward: 1244.000, mean reward:  4.189 [ 0.000, 156.000], mean action: 1.535 [0.000, 3.000],  loss: 353.056061, mae: 294.552307, mean_q: 434.093079\n",
      " 3887/5000: episode: 16, duration: 0.612s, episode steps: 100, steps per second: 163, episode reward: 220.000, mean reward:  2.200 [ 0.000, 52.000], mean action: 1.070 [0.000, 3.000],  loss: 2840.278320, mae: 302.559204, mean_q: 429.210419\n",
      " 4208/5000: episode: 17, duration: 1.914s, episode steps: 321, steps per second: 168, episode reward: 1428.000, mean reward:  4.449 [ 0.000, 144.000], mean action: 1.620 [0.000, 3.000],  loss: 369.823608, mae: 451.294403, mean_q: 639.544434\n",
      " 4318/5000: episode: 18, duration: 0.657s, episode steps: 110, steps per second: 168, episode reward: 348.000, mean reward:  3.164 [ 0.000, 36.000], mean action: 1.791 [0.000, 3.000],  loss: 3944.067383, mae: 395.169861, mean_q: 600.546814\n",
      " 4501/5000: episode: 19, duration: 1.065s, episode steps: 183, steps per second: 172, episode reward: 516.000, mean reward:  2.820 [ 0.000, 40.000], mean action: 1.765 [0.000, 3.000],  loss: 778.769043, mae: 214.937500, mean_q: 323.810822\n",
      " 4859/5000: episode: 20, duration: 2.151s, episode steps: 358, steps per second: 166, episode reward: 1092.000, mean reward:  3.050 [ 0.000, 136.000], mean action: 1.578 [0.000, 3.000],  loss: 439.491272, mae: 285.030426, mean_q: 406.559967\n",
      "done, took 29.853 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdff076fb50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nActions = 4\n",
    "tableSize = 4\n",
    "hiddenFeatures = 16\n",
    "temporalWindow = 5\n",
    "env = Game2048Env()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(temporalWindow, tableSize, tableSize)))\n",
    "\n",
    "model.add(Dense(hiddenFeatures))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "# model.add(Activation(activation=\"leaky_relu\")) # Tune alpha\n",
    "# model.add(Activation(activation=\"elu\")) # Tune alpha\n",
    "\n",
    "model.add(Dense(nActions))\n",
    "model.add(Activation(activation=\"linear\"))\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "replayMemory = SequentialMemory(limit=100, window_length=temporalWindow)\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nActions,\n",
    "    memory=replayMemory,\n",
    "    nb_steps_warmup=100,\n",
    "    target_model_update=0.01,\n",
    "    policy=policy\n",
    ")\n",
    "dqn.compile(optimizer=Adam(lr=0.001), metrics=[\"mae\"])\n",
    "dqn.fit(env=env, nb_steps=5000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 initial state:\n",
      "Score: 0\n",
      "[[0 0 0 2]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_input to have 4 dimensions, but got array with shape (1, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m game\u001b[38;5;241m.\u001b[39mgame_over):\n\u001b[1;32m      8\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(game\u001b[38;5;241m.\u001b[39mgrid, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     predictedAction \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictedAction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predictedAction)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/rl/agents/dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Select an action.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_recent_state(observation)\n\u001b[0;32m--> 224\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    226\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mselect_action(q_values\u001b[38;5;241m=\u001b[39mq_values)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/rl/agents/dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions,)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/rl/agents/dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_batch_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_batch):\n\u001b[1;32m     62\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[0;32m---> 63\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(state_batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:1186\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1183\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`predict_on_batch` is not supported for models distributed with\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1184\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tf.distribute.Strategy.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;66;03m# Validate and standardize user data.\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m inputs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_tensors_from_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# at this point.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_eagerly \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2323\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m run_eagerly \u001b[38;5;129;01mand\u001b[39;00m is_build_called \u001b[38;5;129;01mand\u001b[39;00m is_compile_called \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   2320\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_dataset  \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)):\n\u001b[1;32m   2321\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_v1.py:2351\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[1;32m   2350\u001b[0m   \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[0;32m-> 2351\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[1;32m   2356\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the standardization\u001b[39;00m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;66;03m# code.\u001b[39;00m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training_utils_v1.py:633\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    631\u001b[0m shape \u001b[38;5;241m=\u001b[39m shapes[i]\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape):\n\u001b[0;32m--> 633\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError when checking \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m exception_prefix \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    634\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m names[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    635\u001b[0m                    \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dimensions, but got array \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    636\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith shape \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_shape))\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_batch_axis:\n\u001b[1;32m    638\u001b[0m   data_shape \u001b[38;5;241m=\u001b[39m data_shape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_input to have 4 dimensions, but got array with shape (1, 5)"
     ]
    }
   ],
   "source": [
    "# Play the game with predicts\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        observation = np.expand_dims(game.grid, axis=0)\n",
    "        predictedAction = dqn.forward(observation)\n",
    "        print(\"predictedAction\")\n",
    "        print(predictedAction)\n",
    "        # currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "\n",
    "        # game.move(currentMove)\n",
    "        # print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        # game.render()\n",
    "        break\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = ['up', 'left', 'right', 'down']\n",
    "directions = [\"w\", \"a\", \"d\", \"s\"]\n",
    "\n",
    "games = [Game2048() for _ in range(1)]\n",
    "for i, game in enumerate(games):\n",
    "    print(f\"Game {i + 1} initial state:\")\n",
    "    game.render()\n",
    "    while (not game.game_over):\n",
    "        try: \n",
    "            currentMove = moves[directions.index(input(\"Next move (w, a, s , d): \"))]\n",
    "        except ValueError:\n",
    "            print(\"Invalid move, try again.\")\n",
    "            continue\n",
    "\n",
    "        game.move(currentMove)\n",
    "        print(f\"Game {i + 1} after move '{currentMove}':\")\n",
    "        game.render()\n",
    "\n",
    "    print(f\"Game {i + 1} is over.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
